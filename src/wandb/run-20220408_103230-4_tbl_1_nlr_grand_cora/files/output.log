[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1, 'max_iters': 100}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1, 'max_iters': 100}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
Epoch: 001/100, Runtime 1.859009, Loss 1.947705, forward nfe 242, backward nfe 0, Train: 0.4357, Val: 0.4463, Test: 0.4457, Best time: 4.9837
Epoch: 002/100, Runtime 1.719964, Loss 2.048487, forward nfe 1092, backward nfe 0, Train: 0.4357, Val: 0.4463, Test: 0.4457, Best time: 128.0000
Epoch: 003/100, Runtime 1.751929, Loss 1.991901, forward nfe 1948, backward nfe 0, Train: 0.4357, Val: 0.4463, Test: 0.4457, Best time: 128.0000
Epoch: 004/100, Runtime 1.740705, Loss 1.935419, forward nfe 2804, backward nfe 0, Train: 0.4357, Val: 0.4463, Test: 0.4457, Best time: 128.0000
Epoch: 005/100, Runtime 2.201092, Loss 1.907858, forward nfe 3780, backward nfe 0, Train: 0.5429, Val: 0.4618, Test: 0.4660, Best time: 31.1110
Epoch: 006/100, Runtime 5.274038, Loss 1.854585, forward nfe 5668, backward nfe 0, Train: 0.5857, Val: 0.5728, Test: 0.5726, Best time: 31.5398
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 263, in forward
    prods = torch.sum(src * dst_k, dim=1) / np.sqrt(self.d_k)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.81 GiB already allocated; 8.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.572794 with test accuracy 0.572589 at epoch 6 and best time 31.539763
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #1...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.755656, Loss 1.948844, forward nfe 242, backward nfe 0, Train: 0.2286, Val: 0.4125, Test: 0.4142, Best time: 1.1235
Epoch: 002/100, Runtime 1.782070, Loss 2.063234, forward nfe 1098, backward nfe 0, Train: 0.2286, Val: 0.4125, Test: 0.4142, Best time: 128.0000
Epoch: 003/100, Runtime 2.044074, Loss 1.996744, forward nfe 1960, backward nfe 0, Train: 0.2286, Val: 0.4125, Test: 0.4142, Best time: 128.0000
Epoch: 004/100, Runtime 3.705095, Loss 1.936774, forward nfe 3488, backward nfe 0, Train: 0.2286, Val: 0.4125, Test: 0.4142, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 258, in forward
    dst_k = k[edge[1, :], :, :]
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.80 GiB already allocated; 10.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.412500 with test accuracy 0.414213 at epoch 1 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #2...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.758334, Loss 1.948647, forward nfe 242, backward nfe 0, Train: 0.0929, Val: 0.2287, Test: 0.2041, Best time: 128.0000
Epoch: 002/100, Runtime 1.806390, Loss 2.048965, forward nfe 1104, backward nfe 0, Train: 0.0929, Val: 0.2287, Test: 0.2041, Best time: 128.0000
Epoch: 003/100, Runtime 1.770355, Loss 1.985329, forward nfe 1966, backward nfe 0, Train: 0.0929, Val: 0.2287, Test: 0.2041, Best time: 128.0000
Epoch: 004/100, Runtime 1.777041, Loss 1.976900, forward nfe 2828, backward nfe 0, Train: 0.0929, Val: 0.2287, Test: 0.2041, Best time: 128.0000
Epoch: 005/100, Runtime 1.765850, Loss 1.922384, forward nfe 3684, backward nfe 0, Train: 0.3643, Val: 0.2971, Test: 0.2802, Best time: 7.7703
Epoch: 006/100, Runtime 2.134808, Loss 1.906019, forward nfe 4684, backward nfe 0, Train: 0.3643, Val: 0.2971, Test: 0.2802, Best time: 128.0000
Epoch: 007/100, Runtime 2.101802, Loss 1.899749, forward nfe 5672, backward nfe 0, Train: 0.4500, Val: 0.4044, Test: 0.3990, Best time: 0.3255
Epoch: 008/100, Runtime 2.284565, Loss 1.860689, forward nfe 6636, backward nfe 0, Train: 0.5000, Val: 0.4728, Test: 0.4680, Best time: 3.2492
Epoch: 009/100, Runtime 1.797425, Loss 1.802822, forward nfe 7522, backward nfe 0, Train: 0.5000, Val: 0.4728, Test: 0.4680, Best time: 128.0000
Epoch: 010/100, Runtime 1.751782, Loss 1.753433, forward nfe 8378, backward nfe 0, Train: 0.4643, Val: 0.4860, Test: 0.4964, Best time: 2.1589
Epoch: 011/100, Runtime 1.969226, Loss 1.673495, forward nfe 9312, backward nfe 0, Train: 0.5857, Val: 0.6779, Test: 0.6741, Best time: 66.6506
Epoch: 012/100, Runtime 3.331676, Loss 1.546458, forward nfe 10780, backward nfe 0, Train: 0.5857, Val: 0.6779, Test: 0.6741, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 107, in forward
    raise MaxNFEException
utils.MaxNFEException
best val accuracy 0.677941 with test accuracy 0.674112 at epoch 11 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #3...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.761271, Loss 1.948572, forward nfe 242, backward nfe 0, Train: 0.2071, Val: 0.2184, Test: 0.2203, Best time: 0.0924
Epoch: 002/100, Runtime 1.752329, Loss 2.037072, forward nfe 1098, backward nfe 0, Train: 0.2500, Val: 0.2353, Test: 0.2447, Best time: 0.9332
Epoch: 003/100, Runtime 1.771080, Loss 1.974250, forward nfe 1960, backward nfe 0, Train: 0.2500, Val: 0.2353, Test: 0.2447, Best time: 128.0000
Epoch: 004/100, Runtime 1.782213, Loss 1.999837, forward nfe 2822, backward nfe 0, Train: 0.2357, Val: 0.2375, Test: 0.2467, Best time: 0.9112
Epoch: 005/100, Runtime 1.784945, Loss 1.922080, forward nfe 3690, backward nfe 0, Train: 0.3857, Val: 0.3868, Test: 0.3787, Best time: 1.0047
Epoch: 006/100, Runtime 1.765679, Loss 1.918441, forward nfe 4552, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 6.6919
Epoch: 007/100, Runtime 1.751992, Loss 1.912601, forward nfe 5414, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 008/100, Runtime 1.742742, Loss 1.890841, forward nfe 6264, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 009/100, Runtime 1.831002, Loss 1.863678, forward nfe 7114, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 010/100, Runtime 1.717650, Loss 1.818562, forward nfe 7964, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 011/100, Runtime 1.740873, Loss 1.740373, forward nfe 8814, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 012/100, Runtime 1.729472, Loss 1.657129, forward nfe 9664, backward nfe 0, Train: 0.5571, Val: 0.5757, Test: 0.5452, Best time: 128.0000
Epoch: 013/100, Runtime 1.795230, Loss 1.560680, forward nfe 10526, backward nfe 0, Train: 0.7000, Val: 0.6882, Test: 0.6650, Best time: 20.8363
Epoch: 014/100, Runtime 1.845606, Loss 1.395416, forward nfe 11400, backward nfe 0, Train: 0.7000, Val: 0.6882, Test: 0.6650, Best time: 128.0000
Epoch: 015/100, Runtime 2.132427, Loss 1.266517, forward nfe 12286, backward nfe 0, Train: 0.6929, Val: 0.7647, Test: 0.7523, Best time: 39.3806
Epoch: 016/100, Runtime 1.989951, Loss 1.134654, forward nfe 13202, backward nfe 0, Train: 0.6929, Val: 0.7647, Test: 0.7523, Best time: 128.0000
Epoch: 017/100, Runtime 2.130561, Loss 1.007070, forward nfe 14172, backward nfe 0, Train: 0.6929, Val: 0.7647, Test: 0.7523, Best time: 128.0000
Epoch: 018/100, Runtime 2.440211, Loss 0.942554, forward nfe 15244, backward nfe 0, Train: 0.6929, Val: 0.7647, Test: 0.7523, Best time: 128.0000
Epoch: 019/100, Runtime 2.715034, Loss 0.862357, forward nfe 16394, backward nfe 0, Train: 0.7786, Val: 0.7949, Test: 0.7777, Best time: 55.0618
Epoch: 020/100, Runtime 2.241798, Loss 0.813526, forward nfe 17394, backward nfe 0, Train: 0.7786, Val: 0.7949, Test: 0.7777, Best time: 128.0000
Epoch: 021/100, Runtime 2.043905, Loss 0.789142, forward nfe 18328, backward nfe 0, Train: 0.7786, Val: 0.7949, Test: 0.7777, Best time: 128.0000
Epoch: 022/100, Runtime 1.974193, Loss 0.749747, forward nfe 19238, backward nfe 0, Train: 0.7786, Val: 0.7949, Test: 0.7777, Best time: 128.0000
Epoch: 023/100, Runtime 1.980527, Loss 0.716474, forward nfe 20154, backward nfe 0, Train: 0.7786, Val: 0.7985, Test: 0.7787, Best time: 34.3935
Epoch: 024/100, Runtime 1.992417, Loss 0.689851, forward nfe 21070, backward nfe 0, Train: 0.8000, Val: 0.8081, Test: 0.7797, Best time: 160.5158
Epoch: 025/100, Runtime 2.242311, Loss 0.667442, forward nfe 21986, backward nfe 0, Train: 0.8000, Val: 0.8081, Test: 0.7797, Best time: 128.0000
Epoch: 026/100, Runtime 1.963197, Loss 0.681076, forward nfe 22896, backward nfe 0, Train: 0.8000, Val: 0.8081, Test: 0.7797, Best time: 128.0000
Epoch: 027/100, Runtime 1.982090, Loss 0.673971, forward nfe 23806, backward nfe 0, Train: 0.8429, Val: 0.8287, Test: 0.8041, Best time: 76.9054
Epoch: 028/100, Runtime 1.968138, Loss 0.613716, forward nfe 24710, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 52.3531
Epoch: 029/100, Runtime 2.001725, Loss 0.617631, forward nfe 25632, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 030/100, Runtime 2.002290, Loss 0.638997, forward nfe 26542, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 031/100, Runtime 1.932006, Loss 0.601480, forward nfe 27434, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 032/100, Runtime 1.965733, Loss 0.592284, forward nfe 28338, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 033/100, Runtime 1.971234, Loss 0.702023, forward nfe 29248, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 034/100, Runtime 1.956163, Loss 0.588638, forward nfe 30152, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 035/100, Runtime 1.961448, Loss 0.626395, forward nfe 31056, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 036/100, Runtime 1.968141, Loss 0.561179, forward nfe 31966, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 037/100, Runtime 1.966635, Loss 0.538760, forward nfe 32882, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 038/100, Runtime 1.935322, Loss 0.573484, forward nfe 33780, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 039/100, Runtime 1.913074, Loss 0.535832, forward nfe 34672, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 040/100, Runtime 1.965278, Loss 0.641896, forward nfe 35558, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 041/100, Runtime 1.909550, Loss 0.542508, forward nfe 36450, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 042/100, Runtime 1.919331, Loss 0.560696, forward nfe 37342, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 043/100, Runtime 1.921064, Loss 0.556303, forward nfe 38228, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 044/100, Runtime 2.093636, Loss 0.594851, forward nfe 39096, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 045/100, Runtime 1.832140, Loss 0.550369, forward nfe 39964, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 046/100, Runtime 1.865620, Loss 0.593829, forward nfe 40844, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 047/100, Runtime 1.844346, Loss 0.502477, forward nfe 41712, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 048/100, Runtime 1.833994, Loss 0.506960, forward nfe 42574, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 049/100, Runtime 1.857098, Loss 0.505494, forward nfe 43448, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 050/100, Runtime 1.859642, Loss 0.420318, forward nfe 44328, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 051/100, Runtime 1.863849, Loss 0.467541, forward nfe 45208, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 052/100, Runtime 1.833287, Loss 0.489937, forward nfe 46076, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 053/100, Runtime 1.829237, Loss 0.492964, forward nfe 46938, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 054/100, Runtime 1.854477, Loss 0.619619, forward nfe 47812, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 055/100, Runtime 1.845094, Loss 0.483773, forward nfe 48686, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 056/100, Runtime 1.848727, Loss 0.576700, forward nfe 49554, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 057/100, Runtime 2.154173, Loss 0.499764, forward nfe 50422, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 058/100, Runtime 1.816718, Loss 0.511919, forward nfe 51290, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 059/100, Runtime 1.854427, Loss 0.474478, forward nfe 52164, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 060/100, Runtime 1.838415, Loss 0.543128, forward nfe 53032, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 061/100, Runtime 1.809158, Loss 0.427177, forward nfe 53888, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 062/100, Runtime 1.830644, Loss 0.509941, forward nfe 54756, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 063/100, Runtime 1.790997, Loss 0.452435, forward nfe 55606, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 064/100, Runtime 1.794260, Loss 0.505075, forward nfe 56462, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 065/100, Runtime 1.789265, Loss 0.413755, forward nfe 57318, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 066/100, Runtime 1.887475, Loss 0.486703, forward nfe 58174, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 067/100, Runtime 1.787876, Loss 0.488657, forward nfe 59030, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 068/100, Runtime 1.794147, Loss 0.406722, forward nfe 59886, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 069/100, Runtime 1.792687, Loss 0.448695, forward nfe 60742, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 070/100, Runtime 1.766684, Loss 0.423443, forward nfe 61592, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 071/100, Runtime 1.771905, Loss 0.436287, forward nfe 62442, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 072/100, Runtime 1.743121, Loss 0.444376, forward nfe 63286, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 073/100, Runtime 1.760351, Loss 0.428884, forward nfe 64130, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 074/100, Runtime 1.726259, Loss 0.458415, forward nfe 64962, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 075/100, Runtime 1.751568, Loss 0.376137, forward nfe 65806, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 076/100, Runtime 1.701469, Loss 0.454933, forward nfe 66644, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 077/100, Runtime 1.960093, Loss 0.371752, forward nfe 67476, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 078/100, Runtime 1.716613, Loss 0.457756, forward nfe 68308, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 079/100, Runtime 1.671183, Loss 0.377457, forward nfe 69128, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 080/100, Runtime 1.713995, Loss 0.420615, forward nfe 69960, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 081/100, Runtime 1.698227, Loss 0.434748, forward nfe 70786, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 082/100, Runtime 1.708731, Loss 0.435939, forward nfe 71618, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 083/100, Runtime 1.694794, Loss 0.347138, forward nfe 72444, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 084/100, Runtime 1.694349, Loss 0.550488, forward nfe 73270, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 085/100, Runtime 1.672337, Loss 0.406469, forward nfe 74096, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 086/100, Runtime 1.670716, Loss 0.577599, forward nfe 74916, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 087/100, Runtime 1.690683, Loss 0.339604, forward nfe 75748, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 088/100, Runtime 1.701038, Loss 0.558504, forward nfe 76586, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 089/100, Runtime 1.690122, Loss 0.372056, forward nfe 77418, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 090/100, Runtime 1.604434, Loss 0.545536, forward nfe 78226, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 091/100, Runtime 1.932441, Loss 0.417866, forward nfe 79058, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 092/100, Runtime 1.627951, Loss 0.368355, forward nfe 79872, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 093/100, Runtime 1.647000, Loss 0.551645, forward nfe 80686, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 094/100, Runtime 1.653367, Loss 0.362153, forward nfe 81500, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 095/100, Runtime 1.635184, Loss 0.375312, forward nfe 82308, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 096/100, Runtime 1.671287, Loss 0.426325, forward nfe 83128, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 097/100, Runtime 1.676268, Loss 0.445794, forward nfe 83954, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 098/100, Runtime 1.632593, Loss 0.333784, forward nfe 84762, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
Epoch: 099/100, Runtime 1.636996, Loss 0.334052, forward nfe 85570, backward nfe 0, Train: 0.8500, Val: 0.8507, Test: 0.8294, Best time: 128.0000
best val accuracy 0.850735 with test accuracy 0.829442 at epoch 28 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #4...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.744029, Loss 1.948051, forward nfe 242, backward nfe 0, Train: 0.2929, Val: 0.3544, Test: 0.3462, Best time: 0.9277
Epoch: 002/100, Runtime 1.736151, Loss 2.042582, forward nfe 1098, backward nfe 0, Train: 0.2929, Val: 0.3544, Test: 0.3462, Best time: 128.0000
Epoch: 003/100, Runtime 1.735078, Loss 1.990196, forward nfe 1948, backward nfe 0, Train: 0.2929, Val: 0.3544, Test: 0.3462, Best time: 128.0000
Epoch: 004/100, Runtime 1.740534, Loss 1.940791, forward nfe 2798, backward nfe 0, Train: 0.2929, Val: 0.3544, Test: 0.3462, Best time: 128.0000
Epoch: 005/100, Runtime 1.856796, Loss 1.903791, forward nfe 3690, backward nfe 0, Train: 0.4500, Val: 0.5228, Test: 0.4883, Best time: 1.7489
Epoch: 006/100, Runtime 1.896934, Loss 1.870365, forward nfe 4588, backward nfe 0, Train: 0.5286, Val: 0.5846, Test: 0.5360, Best time: 42.8532
Epoch: 007/100, Runtime 1.934727, Loss 1.789209, forward nfe 5492, backward nfe 0, Train: 0.6000, Val: 0.7118, Test: 0.6569, Best time: 17.6704
Epoch: 008/100, Runtime 2.004674, Loss 1.657713, forward nfe 6420, backward nfe 0, Train: 0.6357, Val: 0.7154, Test: 0.6721, Best time: 8.1528
Epoch: 009/100, Runtime 2.168262, Loss 1.505350, forward nfe 7360, backward nfe 0, Train: 0.7643, Val: 0.7574, Test: 0.7350, Best time: 17.9870
Epoch: 010/100, Runtime 2.008899, Loss 1.331213, forward nfe 8288, backward nfe 0, Train: 0.7643, Val: 0.7574, Test: 0.7350, Best time: 128.0000
Epoch: 011/100, Runtime 2.034280, Loss 1.159345, forward nfe 9228, backward nfe 0, Train: 0.8071, Val: 0.7735, Test: 0.7503, Best time: 13.8127
Epoch: 012/100, Runtime 2.410280, Loss 0.998437, forward nfe 10192, backward nfe 0, Train: 0.8071, Val: 0.7978, Test: 0.7959, Best time: 20.7564
Epoch: 013/100, Runtime 2.187149, Loss 0.850628, forward nfe 11162, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 33.4460
Epoch: 014/100, Runtime 2.122433, Loss 0.758934, forward nfe 12114, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 015/100, Runtime 2.035640, Loss 0.658441, forward nfe 13042, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 016/100, Runtime 2.045530, Loss 0.659827, forward nfe 13964, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 017/100, Runtime 2.084993, Loss 0.537721, forward nfe 14904, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 018/100, Runtime 2.391423, Loss 0.694401, forward nfe 15988, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 019/100, Runtime 2.133693, Loss 0.464651, forward nfe 16946, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 020/100, Runtime 2.181448, Loss 0.533436, forward nfe 17916, backward nfe 0, Train: 0.8357, Val: 0.8184, Test: 0.8051, Best time: 128.0000
Epoch: 021/100, Runtime 2.257848, Loss 0.461671, forward nfe 18910, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 54.2452
Epoch: 022/100, Runtime 2.235028, Loss 0.582216, forward nfe 19898, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 023/100, Runtime 2.520778, Loss 0.602845, forward nfe 20898, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 024/100, Runtime 2.404589, Loss 0.483110, forward nfe 21904, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 025/100, Runtime 2.473741, Loss 0.628360, forward nfe 22964, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 026/100, Runtime 2.332768, Loss 0.501572, forward nfe 23976, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 027/100, Runtime 2.354851, Loss 0.519602, forward nfe 25006, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 028/100, Runtime 2.370480, Loss 0.458618, forward nfe 26036, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 029/100, Runtime 2.641826, Loss 0.441150, forward nfe 27174, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 030/100, Runtime 2.339001, Loss 0.413322, forward nfe 28192, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 031/100, Runtime 2.313221, Loss 0.433739, forward nfe 29204, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 032/100, Runtime 2.242492, Loss 0.411773, forward nfe 30198, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 033/100, Runtime 2.369294, Loss 0.483622, forward nfe 31234, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 034/100, Runtime 2.339956, Loss 0.396519, forward nfe 32252, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 035/100, Runtime 2.505379, Loss 0.530824, forward nfe 33300, backward nfe 0, Train: 0.8714, Val: 0.8235, Test: 0.8142, Best time: 128.0000
Epoch: 036/100, Runtime 2.221238, Loss 0.348406, forward nfe 34288, backward nfe 0, Train: 0.9143, Val: 0.8243, Test: 0.8244, Best time: 33.7560
Epoch: 037/100, Runtime 2.198899, Loss 0.521803, forward nfe 35264, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 159.6799
Epoch: 038/100, Runtime 2.578341, Loss 0.383850, forward nfe 36282, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 039/100, Runtime 2.362920, Loss 0.395916, forward nfe 37318, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 040/100, Runtime 2.493999, Loss 0.405616, forward nfe 38390, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 041/100, Runtime 2.445541, Loss 0.389031, forward nfe 39438, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 042/100, Runtime 2.161491, Loss 0.420923, forward nfe 40402, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 043/100, Runtime 2.268576, Loss 0.351919, forward nfe 41408, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 044/100, Runtime 2.201607, Loss 0.399734, forward nfe 42384, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 045/100, Runtime 2.187167, Loss 0.456350, forward nfe 43360, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 046/100, Runtime 2.301730, Loss 0.383991, forward nfe 44372, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 047/100, Runtime 2.313833, Loss 0.466137, forward nfe 45390, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 048/100, Runtime 2.709559, Loss 0.422366, forward nfe 46414, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 049/100, Runtime 2.161253, Loss 0.326451, forward nfe 47372, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 050/100, Runtime 2.093094, Loss 0.383418, forward nfe 48318, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 051/100, Runtime 2.126951, Loss 0.391177, forward nfe 49276, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 052/100, Runtime 2.142925, Loss 0.412805, forward nfe 50240, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 053/100, Runtime 2.165580, Loss 0.346854, forward nfe 51210, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 054/100, Runtime 2.061082, Loss 0.353914, forward nfe 52144, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 055/100, Runtime 2.228414, Loss 0.348998, forward nfe 53108, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 056/100, Runtime 2.175723, Loss 0.352209, forward nfe 54090, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 057/100, Runtime 2.063049, Loss 0.324682, forward nfe 55024, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 058/100, Runtime 2.039365, Loss 0.389875, forward nfe 55952, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 059/100, Runtime 2.057609, Loss 0.375047, forward nfe 56892, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 060/100, Runtime 2.208361, Loss 0.347106, forward nfe 57880, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 061/100, Runtime 2.273603, Loss 0.427306, forward nfe 58892, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 062/100, Runtime 2.155789, Loss 0.449988, forward nfe 59856, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 063/100, Runtime 2.200155, Loss 0.385398, forward nfe 60838, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 064/100, Runtime 2.236311, Loss 0.397999, forward nfe 61838, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 065/100, Runtime 2.572154, Loss 0.339200, forward nfe 62868, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 066/100, Runtime 2.188233, Loss 0.389257, forward nfe 63844, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 067/100, Runtime 2.141600, Loss 0.256287, forward nfe 64808, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 068/100, Runtime 2.280504, Loss 0.336138, forward nfe 65820, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 069/100, Runtime 2.233835, Loss 0.337708, forward nfe 66820, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 070/100, Runtime 2.115608, Loss 0.283798, forward nfe 67778, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 071/100, Runtime 2.114640, Loss 0.331796, forward nfe 68736, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 072/100, Runtime 2.087943, Loss 0.239825, forward nfe 69688, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 073/100, Runtime 2.263583, Loss 0.301381, forward nfe 70676, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 074/100, Runtime 2.073039, Loss 0.293832, forward nfe 71616, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 075/100, Runtime 2.212424, Loss 0.255678, forward nfe 72598, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 076/100, Runtime 2.339903, Loss 0.264578, forward nfe 73550, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 077/100, Runtime 2.050381, Loss 0.263118, forward nfe 74478, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 078/100, Runtime 2.061837, Loss 0.338132, forward nfe 75418, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 079/100, Runtime 2.141424, Loss 0.328295, forward nfe 76382, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 080/100, Runtime 2.038079, Loss 0.357226, forward nfe 77316, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 081/100, Runtime 2.034643, Loss 0.275613, forward nfe 78244, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 082/100, Runtime 2.200282, Loss 0.313374, forward nfe 79184, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 083/100, Runtime 2.169228, Loss 0.236406, forward nfe 80106, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 084/100, Runtime 2.180001, Loss 0.302545, forward nfe 81040, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 085/100, Runtime 2.304381, Loss 0.278175, forward nfe 82016, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 086/100, Runtime 2.200718, Loss 0.276680, forward nfe 82956, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 087/100, Runtime 2.247479, Loss 0.270930, forward nfe 83908, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 088/100, Runtime 2.165895, Loss 0.314995, forward nfe 84836, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 089/100, Runtime 2.285429, Loss 0.248269, forward nfe 85764, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 090/100, Runtime 2.224437, Loss 0.312763, forward nfe 86704, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 091/100, Runtime 2.156579, Loss 0.309142, forward nfe 87626, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 092/100, Runtime 2.381495, Loss 0.282975, forward nfe 88542, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 093/100, Runtime 2.188670, Loss 0.267771, forward nfe 89452, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 094/100, Runtime 2.072676, Loss 0.295123, forward nfe 90356, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 095/100, Runtime 2.074135, Loss 0.271893, forward nfe 91254, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 096/100, Runtime 2.035914, Loss 0.389044, forward nfe 92146, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 097/100, Runtime 2.096874, Loss 0.271418, forward nfe 93050, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 098/100, Runtime 2.145419, Loss 0.376214, forward nfe 93972, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
Epoch: 099/100, Runtime 2.055118, Loss 0.299169, forward nfe 94876, backward nfe 0, Train: 0.9000, Val: 0.8353, Test: 0.8264, Best time: 128.0000
best val accuracy 0.835294 with test accuracy 0.826396 at epoch 37 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #5...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.868153, Loss 1.945939, forward nfe 242, backward nfe 0, Train: 0.3214, Val: 0.4213, Test: 0.4213, Best time: 1.1314
Epoch: 002/100, Runtime 1.831753, Loss 1.906493, forward nfe 1086, backward nfe 0, Train: 0.5143, Val: 0.4419, Test: 0.4284, Best time: 3.7640
Epoch: 003/100, Runtime 1.860326, Loss 1.807760, forward nfe 1942, backward nfe 0, Train: 0.6071, Val: 0.5309, Test: 0.5147, Best time: 6.7121
Epoch: 004/100, Runtime 1.841902, Loss 1.680879, forward nfe 2792, backward nfe 0, Train: 0.6714, Val: 0.6191, Test: 0.5949, Best time: 37.9028
Epoch: 005/100, Runtime 2.577327, Loss 1.479054, forward nfe 3786, backward nfe 0, Train: 0.6857, Val: 0.7228, Test: 0.6985, Best time: 17.9950
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 263, in forward
    prods = torch.sum(src * dst_k, dim=1) / np.sqrt(self.d_k)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.83 GiB already allocated; 4.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.722794 with test accuracy 0.698477 at epoch 5 and best time 17.995015
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #6...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.930858, Loss 1.948372, forward nfe 242, backward nfe 0, Train: 0.2357, Val: 0.2228, Test: 0.2386, Best time: 0.0957
Epoch: 002/100, Runtime 1.870606, Loss 2.012648, forward nfe 1098, backward nfe 0, Train: 0.2357, Val: 0.2228, Test: 0.2386, Best time: 128.0000
Epoch: 003/100, Runtime 1.897341, Loss 1.979711, forward nfe 1960, backward nfe 0, Train: 0.2357, Val: 0.2228, Test: 0.2386, Best time: 128.0000
Epoch: 004/100, Runtime 1.865879, Loss 1.926552, forward nfe 2816, backward nfe 0, Train: 0.2357, Val: 0.2228, Test: 0.2386, Best time: 128.0000
Epoch: 005/100, Runtime 1.893823, Loss 1.902709, forward nfe 3672, backward nfe 0, Train: 0.3643, Val: 0.3103, Test: 0.3391, Best time: 47.1759
Epoch: 006/100, Runtime 2.394785, Loss 1.858860, forward nfe 4696, backward nfe 0, Train: 0.5643, Val: 0.5074, Test: 0.5492, Best time: 112.1933
Epoch: 007/100, Runtime 3.402403, Loss 1.783478, forward nfe 6092, backward nfe 0, Train: 0.5643, Val: 0.5074, Test: 0.5492, Best time: 128.0000
Epoch: 008/100, Runtime 3.901647, Loss 1.675538, forward nfe 7656, backward nfe 0, Train: 0.5643, Val: 0.5074, Test: 0.5492, Best time: 128.0000
Epoch: 009/100, Runtime 3.151000, Loss 1.572522, forward nfe 8968, backward nfe 0, Train: 0.6643, Val: 0.7147, Test: 0.7198, Best time: 31.5673
Epoch: 010/100, Runtime 3.133756, Loss 1.442418, forward nfe 10244, backward nfe 0, Train: 0.7429, Val: 0.7566, Test: 0.7736, Best time: 66.2197
Epoch: 011/100, Runtime 2.320203, Loss 1.346298, forward nfe 11238, backward nfe 0, Train: 0.7429, Val: 0.7566, Test: 0.7736, Best time: 128.0000
Epoch: 012/100, Runtime 1.872000, Loss 1.224903, forward nfe 12100, backward nfe 0, Train: 0.7429, Val: 0.7566, Test: 0.7736, Best time: 128.0000
Epoch: 013/100, Runtime 2.159909, Loss 1.067837, forward nfe 12986, backward nfe 0, Train: 0.7429, Val: 0.7566, Test: 0.7736, Best time: 128.0000
Epoch: 014/100, Runtime 2.388484, Loss 0.958835, forward nfe 13902, backward nfe 0, Train: 0.8000, Val: 0.8007, Test: 0.8264, Best time: 87.1463
Epoch: 015/100, Runtime 2.277241, Loss 0.844066, forward nfe 14848, backward nfe 0, Train: 0.8000, Val: 0.8007, Test: 0.8264, Best time: 128.0000
Epoch: 016/100, Runtime 2.374403, Loss 0.872943, forward nfe 15824, backward nfe 0, Train: 0.8000, Val: 0.8007, Test: 0.8264, Best time: 128.0000
Epoch: 017/100, Runtime 2.456401, Loss 0.769984, forward nfe 16824, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 113.4738
Epoch: 018/100, Runtime 3.397050, Loss 0.783531, forward nfe 18136, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 019/100, Runtime 2.713351, Loss 0.636661, forward nfe 19220, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 020/100, Runtime 2.410046, Loss 0.801776, forward nfe 20208, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 021/100, Runtime 2.268610, Loss 0.699895, forward nfe 21154, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 022/100, Runtime 2.366865, Loss 0.738064, forward nfe 22088, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 023/100, Runtime 2.233134, Loss 0.597576, forward nfe 23022, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 024/100, Runtime 2.451343, Loss 0.833513, forward nfe 23956, backward nfe 0, Train: 0.8071, Val: 0.8110, Test: 0.8132, Best time: 128.0000
Epoch: 025/100, Runtime 2.226498, Loss 0.531313, forward nfe 24890, backward nfe 0, Train: 0.7857, Val: 0.8132, Test: 0.8203, Best time: 73.5619
Epoch: 026/100, Runtime 2.214157, Loss 0.747455, forward nfe 25818, backward nfe 0, Train: 0.7857, Val: 0.8132, Test: 0.8203, Best time: 128.0000
Epoch: 027/100, Runtime 2.241588, Loss 0.638903, forward nfe 26764, backward nfe 0, Train: 0.7857, Val: 0.8132, Test: 0.8203, Best time: 128.0000
Epoch: 028/100, Runtime 2.191201, Loss 0.581390, forward nfe 27692, backward nfe 0, Train: 0.7857, Val: 0.8132, Test: 0.8203, Best time: 128.0000
Epoch: 029/100, Runtime 2.282489, Loss 0.630637, forward nfe 28638, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 160.6103
Epoch: 030/100, Runtime 2.142112, Loss 0.553876, forward nfe 29560, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 031/100, Runtime 2.213924, Loss 0.619895, forward nfe 30494, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 032/100, Runtime 2.298163, Loss 0.570065, forward nfe 31452, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 033/100, Runtime 2.314718, Loss 0.555365, forward nfe 32440, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 034/100, Runtime 2.158951, Loss 0.584113, forward nfe 33362, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 035/100, Runtime 2.203497, Loss 0.492258, forward nfe 34296, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 036/100, Runtime 2.303031, Loss 0.571726, forward nfe 35254, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 037/100, Runtime 2.187745, Loss 0.563907, forward nfe 36182, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 038/100, Runtime 2.194797, Loss 0.531123, forward nfe 37110, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 039/100, Runtime 2.485922, Loss 0.480658, forward nfe 38062, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 040/100, Runtime 2.272546, Loss 0.510223, forward nfe 39020, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 041/100, Runtime 2.187487, Loss 0.528233, forward nfe 39942, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 042/100, Runtime 2.180352, Loss 0.524766, forward nfe 40870, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 043/100, Runtime 2.089935, Loss 0.514678, forward nfe 41780, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 044/100, Runtime 2.190720, Loss 0.605844, forward nfe 42714, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 045/100, Runtime 2.173180, Loss 0.564948, forward nfe 43642, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 046/100, Runtime 2.142435, Loss 0.507097, forward nfe 44558, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 047/100, Runtime 2.099583, Loss 0.493714, forward nfe 45462, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 048/100, Runtime 2.177505, Loss 0.459394, forward nfe 46390, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 049/100, Runtime 2.231317, Loss 0.447386, forward nfe 47342, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 050/100, Runtime 2.116755, Loss 0.437809, forward nfe 48252, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 051/100, Runtime 2.529669, Loss 0.454655, forward nfe 49174, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 052/100, Runtime 2.120024, Loss 0.422665, forward nfe 50084, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 053/100, Runtime 2.117711, Loss 0.451149, forward nfe 50994, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 054/100, Runtime 2.052369, Loss 0.444770, forward nfe 51886, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 055/100, Runtime 2.053007, Loss 0.466522, forward nfe 52778, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 056/100, Runtime 2.051377, Loss 0.406243, forward nfe 53670, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 057/100, Runtime 2.078088, Loss 0.461548, forward nfe 54568, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 058/100, Runtime 2.131208, Loss 0.472345, forward nfe 55484, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 059/100, Runtime 2.079292, Loss 0.504869, forward nfe 56382, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 060/100, Runtime 2.131256, Loss 0.448124, forward nfe 57298, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 061/100, Runtime 2.114791, Loss 0.624213, forward nfe 58214, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 062/100, Runtime 2.032650, Loss 0.518919, forward nfe 59100, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 063/100, Runtime 1.976346, Loss 0.489548, forward nfe 59974, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 064/100, Runtime 1.951783, Loss 0.559518, forward nfe 60842, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 065/100, Runtime 2.034026, Loss 0.371109, forward nfe 61710, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 066/100, Runtime 1.907366, Loss 0.415548, forward nfe 62566, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 067/100, Runtime 1.991851, Loss 0.396934, forward nfe 63446, backward nfe 0, Train: 0.8286, Val: 0.8338, Test: 0.8426, Best time: 128.0000
Epoch: 068/100, Runtime 2.236078, Loss 0.454527, forward nfe 64326, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 131.1490
Epoch: 069/100, Runtime 1.977552, Loss 0.375123, forward nfe 65212, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 070/100, Runtime 1.970938, Loss 0.461961, forward nfe 66080, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 071/100, Runtime 1.975828, Loss 0.480320, forward nfe 66948, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 072/100, Runtime 1.968087, Loss 0.386826, forward nfe 67816, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 073/100, Runtime 1.929172, Loss 0.461469, forward nfe 68672, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 074/100, Runtime 1.905431, Loss 0.436028, forward nfe 69534, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 075/100, Runtime 1.933905, Loss 0.438650, forward nfe 70396, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 076/100, Runtime 1.933815, Loss 0.357191, forward nfe 71264, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 077/100, Runtime 1.965225, Loss 0.408936, forward nfe 72132, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 078/100, Runtime 2.038558, Loss 0.482132, forward nfe 72994, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 079/100, Runtime 1.971385, Loss 0.432838, forward nfe 73862, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 080/100, Runtime 2.005204, Loss 0.398816, forward nfe 74742, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 081/100, Runtime 2.252785, Loss 0.479162, forward nfe 75622, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 082/100, Runtime 1.868029, Loss 0.312066, forward nfe 76472, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 083/100, Runtime 1.906233, Loss 0.454211, forward nfe 77322, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 084/100, Runtime 1.919363, Loss 0.411817, forward nfe 78178, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 085/100, Runtime 1.857989, Loss 0.355287, forward nfe 79016, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 086/100, Runtime 1.885486, Loss 0.357682, forward nfe 79860, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 087/100, Runtime 1.857755, Loss 0.316182, forward nfe 80710, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 088/100, Runtime 1.837715, Loss 0.363179, forward nfe 81554, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 089/100, Runtime 1.840903, Loss 0.321818, forward nfe 82392, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 090/100, Runtime 1.846584, Loss 0.368445, forward nfe 83230, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 091/100, Runtime 1.830894, Loss 0.326334, forward nfe 84062, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 092/100, Runtime 1.869704, Loss 0.353533, forward nfe 84906, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 093/100, Runtime 1.828616, Loss 0.361244, forward nfe 85738, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 094/100, Runtime 1.817405, Loss 0.250304, forward nfe 86582, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 095/100, Runtime 1.924078, Loss 0.391390, forward nfe 87432, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 096/100, Runtime 1.840126, Loss 0.293569, forward nfe 88276, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 097/100, Runtime 1.829668, Loss 0.365650, forward nfe 89126, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 098/100, Runtime 1.840548, Loss 0.378483, forward nfe 89964, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
Epoch: 099/100, Runtime 2.049054, Loss 0.459540, forward nfe 90802, backward nfe 0, Train: 0.8929, Val: 0.8493, Test: 0.8569, Best time: 128.0000
best val accuracy 0.849265 with test accuracy 0.856853 at epoch 68 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #7...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.842749, Loss 1.949776, forward nfe 242, backward nfe 0, Train: 0.3714, Val: 0.3676, Test: 0.3919, Best time: 7.4019
Epoch: 002/100, Runtime 1.830114, Loss 1.899970, forward nfe 1092, backward nfe 0, Train: 0.4857, Val: 0.4890, Test: 0.4731, Best time: 0.9915
Epoch: 003/100, Runtime 1.938838, Loss 1.841213, forward nfe 1978, backward nfe 0, Train: 0.6214, Val: 0.5228, Test: 0.5523, Best time: 0.3873
Epoch: 004/100, Runtime 2.176905, Loss 1.730416, forward nfe 2924, backward nfe 0, Train: 0.6214, Val: 0.5228, Test: 0.5523, Best time: 128.0000
Epoch: 005/100, Runtime 2.346453, Loss 1.600501, forward nfe 3930, backward nfe 0, Train: 0.7286, Val: 0.6463, Test: 0.6640, Best time: 17.5140
Epoch: 006/100, Runtime 2.083537, Loss 1.407952, forward nfe 4864, backward nfe 0, Train: 0.7214, Val: 0.6706, Test: 0.6701, Best time: 58.5035
Epoch: 007/100, Runtime 1.959624, Loss 1.176156, forward nfe 5738, backward nfe 0, Train: 0.7500, Val: 0.7265, Test: 0.7279, Best time: 71.5972
Epoch: 008/100, Runtime 2.158735, Loss 1.006773, forward nfe 6636, backward nfe 0, Train: 0.7500, Val: 0.7265, Test: 0.7279, Best time: 128.0000
Epoch: 009/100, Runtime 2.223309, Loss 0.833849, forward nfe 7576, backward nfe 0, Train: 0.7500, Val: 0.7265, Test: 0.7279, Best time: 128.0000
Epoch: 010/100, Runtime 2.422561, Loss 0.714671, forward nfe 8570, backward nfe 0, Train: 0.8500, Val: 0.7926, Test: 0.7929, Best time: 27.7277
Epoch: 011/100, Runtime 2.545362, Loss 0.665408, forward nfe 9600, backward nfe 0, Train: 0.8500, Val: 0.8051, Test: 0.8091, Best time: 128.4923
Epoch: 012/100, Runtime 2.717141, Loss 0.613159, forward nfe 10564, backward nfe 0, Train: 0.8500, Val: 0.8051, Test: 0.8091, Best time: 128.0000
Epoch: 013/100, Runtime 2.300885, Loss 0.588698, forward nfe 11558, backward nfe 0, Train: 0.8500, Val: 0.8051, Test: 0.8091, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 111, in forward
    ax = self.multiply_attention(x, attention, values)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 102, in multiply_attention
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 25, in spmm
    out = out * value.unsqueeze(-1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.80 GiB already allocated; 16.56 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.805147 with test accuracy 0.809137 at epoch 11 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #8...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.876097, Loss 1.948233, forward nfe 242, backward nfe 0, Train: 0.1857, Val: 0.1706, Test: 0.1726, Best time: 0.0926
Epoch: 002/100, Runtime 1.857523, Loss 2.028215, forward nfe 1098, backward nfe 0, Train: 0.3000, Val: 0.2140, Test: 0.2325, Best time: 121.7013
Epoch: 003/100, Runtime 1.860917, Loss 1.898097, forward nfe 1948, backward nfe 0, Train: 0.3500, Val: 0.2176, Test: 0.2497, Best time: 0.9949
Epoch: 004/100, Runtime 1.846307, Loss 1.892715, forward nfe 2798, backward nfe 0, Train: 0.4214, Val: 0.2684, Test: 0.3188, Best time: 2.3258
Epoch: 005/100, Runtime 1.846437, Loss 1.859310, forward nfe 3654, backward nfe 0, Train: 0.4929, Val: 0.3897, Test: 0.4335, Best time: 0.9942
Epoch: 006/100, Runtime 1.840690, Loss 1.780111, forward nfe 4504, backward nfe 0, Train: 0.6571, Val: 0.5346, Test: 0.5645, Best time: 63.3183
Epoch: 007/100, Runtime 1.850642, Loss 1.687494, forward nfe 5360, backward nfe 0, Train: 0.6571, Val: 0.5346, Test: 0.5645, Best time: 128.0000
Epoch: 008/100, Runtime 1.777711, Loss 1.559564, forward nfe 6204, backward nfe 0, Train: 0.6571, Val: 0.5949, Test: 0.6091, Best time: 6.2769
Epoch: 009/100, Runtime 2.219795, Loss 1.418828, forward nfe 7180, backward nfe 0, Train: 0.7571, Val: 0.6779, Test: 0.7107, Best time: 11.3688
Epoch: 010/100, Runtime 2.334308, Loss 1.281039, forward nfe 8144, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 15.3721
Epoch: 011/100, Runtime 1.775134, Loss 1.152395, forward nfe 8976, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 128.0000
Epoch: 012/100, Runtime 2.019125, Loss 1.048232, forward nfe 9886, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 128.0000
Epoch: 013/100, Runtime 2.037380, Loss 0.864084, forward nfe 10772, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 128.0000
Epoch: 014/100, Runtime 2.060241, Loss 0.818087, forward nfe 11664, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 128.0000
Epoch: 015/100, Runtime 2.508718, Loss 0.801558, forward nfe 12562, backward nfe 0, Train: 0.8071, Val: 0.7647, Test: 0.7848, Best time: 128.0000
Epoch: 016/100, Runtime 2.388222, Loss 0.726059, forward nfe 13478, backward nfe 0, Train: 0.8286, Val: 0.8029, Test: 0.8051, Best time: 185.9289
Epoch: 017/100, Runtime 2.404753, Loss 0.711476, forward nfe 14400, backward nfe 0, Train: 0.8286, Val: 0.8029, Test: 0.8051, Best time: 128.0000
Epoch: 018/100, Runtime 2.505836, Loss 0.643567, forward nfe 15352, backward nfe 0, Train: 0.8286, Val: 0.8029, Test: 0.8051, Best time: 128.0000
Epoch: 019/100, Runtime 2.604783, Loss 0.631753, forward nfe 16334, backward nfe 0, Train: 0.8286, Val: 0.8029, Test: 0.8051, Best time: 128.0000
Epoch: 020/100, Runtime 2.436067, Loss 0.576572, forward nfe 17268, backward nfe 0, Train: 0.8286, Val: 0.8029, Test: 0.8051, Best time: 128.0000
Epoch: 021/100, Runtime 2.289796, Loss 0.595461, forward nfe 18172, backward nfe 0, Train: 0.8714, Val: 0.8081, Test: 0.8071, Best time: 208.6260
Epoch: 022/100, Runtime 2.340100, Loss 0.494519, forward nfe 19076, backward nfe 0, Train: 0.8714, Val: 0.8081, Test: 0.8071, Best time: 128.0000
Epoch: 023/100, Runtime 2.385201, Loss 0.547038, forward nfe 19998, backward nfe 0, Train: 0.8714, Val: 0.8081, Test: 0.8071, Best time: 128.0000
Epoch: 024/100, Runtime 2.375165, Loss 0.582414, forward nfe 20902, backward nfe 0, Train: 0.8714, Val: 0.8081, Test: 0.8071, Best time: 128.0000
Epoch: 025/100, Runtime 2.391466, Loss 0.537105, forward nfe 21812, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 190.8804
Epoch: 026/100, Runtime 2.564946, Loss 0.592372, forward nfe 22710, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 027/100, Runtime 2.347484, Loss 0.521766, forward nfe 23614, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 028/100, Runtime 2.360308, Loss 0.532401, forward nfe 24524, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 029/100, Runtime 2.282576, Loss 0.489514, forward nfe 25416, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 030/100, Runtime 2.324555, Loss 0.542995, forward nfe 26314, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 031/100, Runtime 2.279130, Loss 0.572919, forward nfe 27206, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 032/100, Runtime 2.302675, Loss 0.568453, forward nfe 28104, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 033/100, Runtime 2.272597, Loss 0.437192, forward nfe 28990, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 034/100, Runtime 2.309078, Loss 0.543971, forward nfe 29888, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 035/100, Runtime 2.387417, Loss 0.519189, forward nfe 30780, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 036/100, Runtime 2.264956, Loss 0.455925, forward nfe 31666, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 037/100, Runtime 2.277393, Loss 0.474301, forward nfe 32546, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 038/100, Runtime 2.317376, Loss 0.490053, forward nfe 33444, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 039/100, Runtime 2.328935, Loss 0.482758, forward nfe 34348, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 040/100, Runtime 2.694445, Loss 0.377798, forward nfe 35288, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 041/100, Runtime 2.424275, Loss 0.528787, forward nfe 36210, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 042/100, Runtime 2.301837, Loss 0.387646, forward nfe 37108, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 043/100, Runtime 2.320802, Loss 0.410009, forward nfe 38012, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 044/100, Runtime 2.267863, Loss 0.379373, forward nfe 38898, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 045/100, Runtime 2.308430, Loss 0.417872, forward nfe 39796, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 046/100, Runtime 2.251732, Loss 0.414198, forward nfe 40676, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 047/100, Runtime 2.351559, Loss 0.362233, forward nfe 41592, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 048/100, Runtime 2.299840, Loss 0.434698, forward nfe 42490, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 049/100, Runtime 2.386339, Loss 0.386789, forward nfe 43412, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 050/100, Runtime 2.419190, Loss 0.398956, forward nfe 44340, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 051/100, Runtime 2.681158, Loss 0.367602, forward nfe 45280, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 052/100, Runtime 2.360592, Loss 0.420770, forward nfe 46196, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 053/100, Runtime 2.238214, Loss 0.391966, forward nfe 47070, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 054/100, Runtime 2.250129, Loss 0.416072, forward nfe 47956, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 055/100, Runtime 2.304415, Loss 0.381478, forward nfe 48854, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 056/100, Runtime 2.228788, Loss 0.363982, forward nfe 49728, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 057/100, Runtime 2.176305, Loss 0.409705, forward nfe 50590, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 058/100, Runtime 2.191740, Loss 0.337575, forward nfe 51458, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 059/100, Runtime 2.309987, Loss 0.359744, forward nfe 52332, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 060/100, Runtime 2.236558, Loss 0.333152, forward nfe 53212, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 061/100, Runtime 2.216063, Loss 0.339795, forward nfe 54080, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 062/100, Runtime 2.242658, Loss 0.327308, forward nfe 54966, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 063/100, Runtime 2.270568, Loss 0.324179, forward nfe 55852, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 064/100, Runtime 2.227251, Loss 0.287372, forward nfe 56732, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 065/100, Runtime 2.265226, Loss 0.353702, forward nfe 57618, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 066/100, Runtime 2.449801, Loss 0.389915, forward nfe 58486, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 067/100, Runtime 2.162235, Loss 0.313657, forward nfe 59342, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 068/100, Runtime 2.138231, Loss 0.345486, forward nfe 60192, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 069/100, Runtime 2.139016, Loss 0.314785, forward nfe 61048, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 070/100, Runtime 2.158291, Loss 0.361576, forward nfe 61904, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 071/100, Runtime 2.185739, Loss 0.304459, forward nfe 62772, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 072/100, Runtime 2.204479, Loss 0.408348, forward nfe 63652, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 073/100, Runtime 2.168829, Loss 0.291587, forward nfe 64514, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 074/100, Runtime 2.160662, Loss 0.307481, forward nfe 65376, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 075/100, Runtime 2.196485, Loss 0.341154, forward nfe 66250, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 076/100, Runtime 2.172113, Loss 0.321206, forward nfe 67112, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 077/100, Runtime 2.115466, Loss 0.302913, forward nfe 67956, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 078/100, Runtime 2.420687, Loss 0.371938, forward nfe 68818, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 079/100, Runtime 2.279329, Loss 0.279836, forward nfe 69686, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 080/100, Runtime 2.135926, Loss 0.339989, forward nfe 70542, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 081/100, Runtime 2.127544, Loss 0.288539, forward nfe 71392, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 082/100, Runtime 2.079894, Loss 0.301733, forward nfe 72230, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 083/100, Runtime 2.072034, Loss 0.283242, forward nfe 73062, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 084/100, Runtime 2.042477, Loss 0.289734, forward nfe 73888, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 085/100, Runtime 2.063766, Loss 0.259665, forward nfe 74720, backward nfe 0, Train: 0.8500, Val: 0.8235, Test: 0.8122, Best time: 128.0000
Epoch: 086/100, Runtime 2.058976, Loss 0.295150, forward nfe 75552, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 152.0596
Epoch: 087/100, Runtime 2.086654, Loss 0.287140, forward nfe 76396, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 088/100, Runtime 2.076619, Loss 0.301998, forward nfe 77234, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 089/100, Runtime 2.146383, Loss 0.410783, forward nfe 78096, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 090/100, Runtime 2.072243, Loss 0.259315, forward nfe 78934, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 091/100, Runtime 2.116223, Loss 0.317481, forward nfe 79784, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 092/100, Runtime 2.079665, Loss 0.281637, forward nfe 80622, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 093/100, Runtime 2.057907, Loss 0.329526, forward nfe 81454, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 094/100, Runtime 2.257717, Loss 0.262515, forward nfe 82268, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 095/100, Runtime 2.010635, Loss 0.251647, forward nfe 83082, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 096/100, Runtime 2.038110, Loss 0.266614, forward nfe 83908, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 097/100, Runtime 2.049101, Loss 0.305868, forward nfe 84740, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 098/100, Runtime 2.046182, Loss 0.223525, forward nfe 85572, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
Epoch: 099/100, Runtime 2.017787, Loss 0.278971, forward nfe 86392, backward nfe 0, Train: 0.9214, Val: 0.8331, Test: 0.8122, Best time: 128.0000
best val accuracy 0.833088 with test accuracy 0.812183 at epoch 86 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #9...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 2.096443, Loss 1.948649, forward nfe 242, backward nfe 0, Train: 0.2357, Val: 0.2132, Test: 0.2071, Best time: 0.0943
Epoch: 002/100, Runtime 2.109545, Loss 2.044769, forward nfe 1098, backward nfe 0, Train: 0.3000, Val: 0.2735, Test: 0.2426, Best time: 6.4038
Epoch: 003/100, Runtime 2.144067, Loss 1.988568, forward nfe 1960, backward nfe 0, Train: 0.3000, Val: 0.2735, Test: 0.2426, Best time: 128.0000
Epoch: 004/100, Runtime 2.132118, Loss 1.928841, forward nfe 2822, backward nfe 0, Train: 0.3000, Val: 0.2735, Test: 0.2426, Best time: 128.0000
Epoch: 005/100, Runtime 2.365189, Loss 1.880320, forward nfe 3750, backward nfe 0, Train: 0.4857, Val: 0.4684, Test: 0.4234, Best time: 1.8699
Epoch: 006/100, Runtime 4.467118, Loss 1.816939, forward nfe 5362, backward nfe 0, Train: 0.5786, Val: 0.5985, Test: 0.5330, Best time: 35.1167
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 258, in forward
    dst_k = k[edge[1, :], :, :]
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.81 GiB already allocated; 20.56 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.598529 with test accuracy 0.532995 at epoch 6 and best time 35.116658
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #10...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 2.134817, Loss 1.948679, forward nfe 242, backward nfe 0, Train: 0.1429, Val: 0.3154, Test: 0.2812, Best time: 49.2312
Epoch: 002/100, Runtime 2.122823, Loss 2.028496, forward nfe 1098, backward nfe 0, Train: 0.1429, Val: 0.3154, Test: 0.2812, Best time: 128.0000
Epoch: 003/100, Runtime 2.123723, Loss 1.977338, forward nfe 1954, backward nfe 0, Train: 0.1429, Val: 0.3154, Test: 0.2812, Best time: 128.0000
Epoch: 004/100, Runtime 2.143983, Loss 1.931412, forward nfe 2816, backward nfe 0, Train: 0.1429, Val: 0.3154, Test: 0.2812, Best time: 128.0000
Epoch: 005/100, Runtime 2.132268, Loss 1.892930, forward nfe 3672, backward nfe 0, Train: 0.1429, Val: 0.3154, Test: 0.2812, Best time: 128.0000
Epoch: 006/100, Runtime 5.195710, Loss 1.864471, forward nfe 5464, backward nfe 0, Train: 0.6286, Val: 0.5706, Test: 0.5645, Best time: 18.3425
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 111, in forward
    ax = self.multiply_attention(x, attention, values)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 102, in multiply_attention
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 25, in spmm
    out = out * value.unsqueeze(-1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.80 GiB already allocated; 12.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.570588 with test accuracy 0.564467 at epoch 6 and best time 18.342468
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #11...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 2.123543, Loss 1.947183, forward nfe 242, backward nfe 0, Train: 0.2500, Val: 0.2250, Test: 0.2254, Best time: 0.0947
Epoch: 002/100, Runtime 2.136625, Loss 2.015518, forward nfe 1098, backward nfe 0, Train: 0.2500, Val: 0.2250, Test: 0.2254, Best time: 128.0000
Epoch: 003/100, Runtime 2.120921, Loss 1.982361, forward nfe 1954, backward nfe 0, Train: 0.2500, Val: 0.2250, Test: 0.2254, Best time: 128.0000
Epoch: 004/100, Runtime 2.144285, Loss 1.934472, forward nfe 2816, backward nfe 0, Train: 0.2500, Val: 0.2250, Test: 0.2254, Best time: 128.0000
Epoch: 005/100, Runtime 2.917583, Loss 1.909274, forward nfe 3840, backward nfe 0, Train: 0.3571, Val: 0.4294, Test: 0.4294, Best time: 4.8426
Epoch: 006/100, Runtime 5.368402, Loss 1.866095, forward nfe 5674, backward nfe 0, Train: 0.4071, Val: 0.5029, Test: 0.4914, Best time: 41.2119
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 258, in forward
    dst_k = k[edge[1, :], :, :]
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.80 GiB already allocated; 10.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.502941 with test accuracy 0.491371 at epoch 6 and best time 41.211872
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #12...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 2.140167, Loss 1.945718, forward nfe 242, backward nfe 0, Train: 0.1929, Val: 0.1066, Test: 0.1157, Best time: 0.0912
Epoch: 002/100, Runtime 2.097912, Loss 2.214787, forward nfe 1092, backward nfe 0, Train: 0.3429, Val: 0.3309, Test: 0.3279, Best time: 45.7729
Epoch: 003/100, Runtime 2.181519, Loss 1.876071, forward nfe 1978, backward nfe 0, Train: 0.4786, Val: 0.4691, Test: 0.4975, Best time: 9.1369
Epoch: 004/100, Runtime 2.173930, Loss 1.884358, forward nfe 2840, backward nfe 0, Train: 0.4929, Val: 0.4809, Test: 0.5096, Best time: 5.1071
Epoch: 005/100, Runtime 2.145376, Loss 1.870380, forward nfe 3690, backward nfe 0, Train: 0.5286, Val: 0.5140, Test: 0.5137, Best time: 4.9476
Epoch: 006/100, Runtime 2.104606, Loss 1.838524, forward nfe 4540, backward nfe 0, Train: 0.5286, Val: 0.5140, Test: 0.5137, Best time: 128.0000
Epoch: 007/100, Runtime 2.180015, Loss 1.793411, forward nfe 5414, backward nfe 0, Train: 0.6071, Val: 0.5228, Test: 0.5391, Best time: 3.6686
Epoch: 008/100, Runtime 2.274583, Loss 1.713320, forward nfe 6336, backward nfe 0, Train: 0.6643, Val: 0.6478, Test: 0.6264, Best time: 5.0120
Epoch: 009/100, Runtime 2.424567, Loss 1.636195, forward nfe 7210, backward nfe 0, Train: 0.7000, Val: 0.7368, Test: 0.7259, Best time: 34.3987
Epoch: 010/100, Runtime 1.890210, Loss 1.532948, forward nfe 8090, backward nfe 0, Train: 0.7000, Val: 0.7368, Test: 0.7259, Best time: 128.0000
Epoch: 011/100, Runtime 1.755255, Loss 1.441383, forward nfe 8946, backward nfe 0, Train: 0.7000, Val: 0.7368, Test: 0.7259, Best time: 128.0000
Epoch: 012/100, Runtime 1.684836, Loss 1.332986, forward nfe 9778, backward nfe 0, Train: 0.7000, Val: 0.7368, Test: 0.7259, Best time: 128.0000
Epoch: 013/100, Runtime 1.753848, Loss 1.211409, forward nfe 10622, backward nfe 0, Train: 0.7000, Val: 0.7368, Test: 0.7259, Best time: 128.0000
Epoch: 014/100, Runtime 1.856974, Loss 1.086666, forward nfe 11490, backward nfe 0, Train: 0.7714, Val: 0.7588, Test: 0.7807, Best time: 56.1394
Epoch: 015/100, Runtime 1.885101, Loss 0.929612, forward nfe 12376, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 61.9001
Epoch: 016/100, Runtime 1.972403, Loss 0.887188, forward nfe 13280, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 017/100, Runtime 2.022432, Loss 0.836367, forward nfe 14208, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 018/100, Runtime 2.143404, Loss 0.780449, forward nfe 15178, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 019/100, Runtime 2.313905, Loss 0.733900, forward nfe 16208, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 020/100, Runtime 2.348763, Loss 0.716620, forward nfe 17226, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 021/100, Runtime 2.039203, Loss 0.666301, forward nfe 18154, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 022/100, Runtime 1.959977, Loss 0.610309, forward nfe 19046, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 023/100, Runtime 1.931410, Loss 0.606826, forward nfe 19944, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 024/100, Runtime 1.923052, Loss 0.636251, forward nfe 20836, backward nfe 0, Train: 0.8143, Val: 0.7860, Test: 0.8041, Best time: 128.0000
Epoch: 025/100, Runtime 2.121229, Loss 0.594850, forward nfe 21710, backward nfe 0, Train: 0.8714, Val: 0.8007, Test: 0.8030, Best time: 47.0449
Epoch: 026/100, Runtime 1.796345, Loss 0.543925, forward nfe 22584, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 46.7440
Epoch: 027/100, Runtime 1.778661, Loss 0.533452, forward nfe 23446, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 028/100, Runtime 1.806370, Loss 0.523650, forward nfe 24314, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 029/100, Runtime 1.793893, Loss 0.544210, forward nfe 25176, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 030/100, Runtime 1.777446, Loss 0.508159, forward nfe 26044, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 031/100, Runtime 1.766376, Loss 0.511030, forward nfe 26912, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 032/100, Runtime 1.780858, Loss 0.529249, forward nfe 27786, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 033/100, Runtime 1.833515, Loss 0.445170, forward nfe 28660, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 034/100, Runtime 1.819215, Loss 0.478619, forward nfe 29534, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 035/100, Runtime 1.754102, Loss 0.507569, forward nfe 30396, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 036/100, Runtime 1.851002, Loss 0.428990, forward nfe 31276, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 037/100, Runtime 1.803855, Loss 0.464236, forward nfe 32156, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 038/100, Runtime 1.801297, Loss 0.481096, forward nfe 33030, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 039/100, Runtime 1.784833, Loss 0.413467, forward nfe 33886, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 040/100, Runtime 1.790008, Loss 0.425311, forward nfe 34754, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 041/100, Runtime 2.233441, Loss 0.416217, forward nfe 35622, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 042/100, Runtime 1.752777, Loss 0.474937, forward nfe 36484, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 043/100, Runtime 1.781078, Loss 0.410262, forward nfe 37340, backward nfe 0, Train: 0.8714, Val: 0.8154, Test: 0.8447, Best time: 128.0000
Epoch: 044/100, Runtime 1.752335, Loss 0.433989, forward nfe 38202, backward nfe 0, Train: 0.9071, Val: 0.8184, Test: 0.8437, Best time: 51.9671
Epoch: 045/100, Runtime 1.739871, Loss 0.425973, forward nfe 39058, backward nfe 0, Train: 0.9071, Val: 0.8184, Test: 0.8437, Best time: 128.0000
Epoch: 046/100, Runtime 1.770468, Loss 0.379996, forward nfe 39914, backward nfe 0, Train: 0.9071, Val: 0.8184, Test: 0.8437, Best time: 128.0000
Epoch: 047/100, Runtime 1.797838, Loss 0.433246, forward nfe 40782, backward nfe 0, Train: 0.9071, Val: 0.8184, Test: 0.8437, Best time: 128.0000
Epoch: 048/100, Runtime 1.875293, Loss 0.393452, forward nfe 41650, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 76.7792
Epoch: 049/100, Runtime 1.740244, Loss 0.385779, forward nfe 42506, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 050/100, Runtime 1.748937, Loss 0.347777, forward nfe 43362, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 051/100, Runtime 1.754849, Loss 0.374778, forward nfe 44218, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 052/100, Runtime 1.770162, Loss 0.374030, forward nfe 45074, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 053/100, Runtime 1.772536, Loss 0.294733, forward nfe 45930, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 054/100, Runtime 1.712262, Loss 0.410503, forward nfe 46780, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 055/100, Runtime 1.746338, Loss 0.289922, forward nfe 47630, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 056/100, Runtime 1.738437, Loss 0.366057, forward nfe 48480, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 057/100, Runtime 1.729786, Loss 0.344237, forward nfe 49324, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 058/100, Runtime 1.725132, Loss 0.318744, forward nfe 50168, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 059/100, Runtime 1.924258, Loss 0.354106, forward nfe 51006, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 060/100, Runtime 1.667923, Loss 0.362877, forward nfe 51838, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 061/100, Runtime 1.640543, Loss 0.325341, forward nfe 52664, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 062/100, Runtime 1.726213, Loss 0.329185, forward nfe 53514, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 063/100, Runtime 1.705951, Loss 0.316089, forward nfe 54352, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 064/100, Runtime 1.679296, Loss 0.275854, forward nfe 55178, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 065/100, Runtime 1.666347, Loss 0.289414, forward nfe 56016, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 066/100, Runtime 1.695971, Loss 0.299612, forward nfe 56854, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 067/100, Runtime 1.676588, Loss 0.287529, forward nfe 57698, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 068/100, Runtime 1.679498, Loss 0.262457, forward nfe 58530, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 069/100, Runtime 1.696888, Loss 0.268887, forward nfe 59368, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 070/100, Runtime 1.677081, Loss 0.273314, forward nfe 60194, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 071/100, Runtime 1.659843, Loss 0.259930, forward nfe 61020, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 072/100, Runtime 1.700770, Loss 0.246631, forward nfe 61858, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 073/100, Runtime 1.672628, Loss 0.281565, forward nfe 62684, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 074/100, Runtime 1.648040, Loss 0.270274, forward nfe 63516, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 075/100, Runtime 1.673629, Loss 0.331697, forward nfe 64342, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 076/100, Runtime 1.846111, Loss 0.262128, forward nfe 65156, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 077/100, Runtime 1.605242, Loss 0.301364, forward nfe 65970, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 078/100, Runtime 1.605167, Loss 0.274174, forward nfe 66784, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 079/100, Runtime 1.632189, Loss 0.236012, forward nfe 67598, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 080/100, Runtime 1.604997, Loss 0.218891, forward nfe 68406, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 081/100, Runtime 1.632611, Loss 0.251731, forward nfe 69226, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 082/100, Runtime 1.603736, Loss 0.215335, forward nfe 70034, backward nfe 0, Train: 0.9143, Val: 0.8235, Test: 0.8426, Best time: 128.0000
Epoch: 083/100, Runtime 1.647691, Loss 0.284968, forward nfe 70854, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 150.7421
Epoch: 084/100, Runtime 1.618317, Loss 0.254121, forward nfe 71662, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 085/100, Runtime 1.589541, Loss 0.237262, forward nfe 72476, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 086/100, Runtime 1.585223, Loss 0.202689, forward nfe 73278, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 087/100, Runtime 1.640492, Loss 0.240623, forward nfe 74098, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 088/100, Runtime 1.629683, Loss 0.195301, forward nfe 74918, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 089/100, Runtime 1.622565, Loss 0.187686, forward nfe 75732, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 090/100, Runtime 1.595279, Loss 0.223293, forward nfe 76534, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 091/100, Runtime 1.577138, Loss 0.234620, forward nfe 77330, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 092/100, Runtime 1.547482, Loss 0.196768, forward nfe 78114, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 093/100, Runtime 1.553509, Loss 0.258305, forward nfe 78910, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 094/100, Runtime 1.547325, Loss 0.251604, forward nfe 79694, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 095/100, Runtime 1.876662, Loss 0.237941, forward nfe 80484, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 096/100, Runtime 1.512208, Loss 0.233016, forward nfe 81268, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 097/100, Runtime 1.522082, Loss 0.238623, forward nfe 82052, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 098/100, Runtime 1.553014, Loss 0.230415, forward nfe 82842, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
Epoch: 099/100, Runtime 1.498232, Loss 0.247649, forward nfe 83626, backward nfe 0, Train: 0.9214, Val: 0.8243, Test: 0.8294, Best time: 128.0000
best val accuracy 0.824265 with test accuracy 0.829442 at epoch 83 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #13...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.795636, Loss 1.948410, forward nfe 242, backward nfe 0, Train: 0.2643, Val: 0.2794, Test: 0.2426, Best time: 0.0949
Epoch: 002/100, Runtime 1.764058, Loss 2.017780, forward nfe 1098, backward nfe 0, Train: 0.2643, Val: 0.2794, Test: 0.2426, Best time: 128.0000
Epoch: 003/100, Runtime 1.746349, Loss 1.987936, forward nfe 1960, backward nfe 0, Train: 0.2643, Val: 0.2794, Test: 0.2426, Best time: 128.0000
Epoch: 004/100, Runtime 1.756823, Loss 1.936341, forward nfe 2828, backward nfe 0, Train: 0.2643, Val: 0.2794, Test: 0.2426, Best time: 128.0000
Epoch: 005/100, Runtime 1.812699, Loss 1.907695, forward nfe 3726, backward nfe 0, Train: 0.2643, Val: 0.2794, Test: 0.2426, Best time: 128.0000
Epoch: 006/100, Runtime 4.530487, Loss 1.859932, forward nfe 5608, backward nfe 0, Train: 0.5143, Val: 0.4618, Test: 0.4447, Best time: 64.0180
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 258, in forward
    dst_k = k[edge[1, :], :, :]
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.81 GiB already allocated; 2.56 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.461765 with test accuracy 0.444670 at epoch 6 and best time 64.018017
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #14...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.745925, Loss 1.945256, forward nfe 236, backward nfe 0, Train: 0.1429, Val: 0.1360, Test: 0.1411, Best time: 128.0000
Epoch: 002/100, Runtime 1.754702, Loss 2.709458, forward nfe 1086, backward nfe 0, Train: 0.1786, Val: 0.1735, Test: 0.1614, Best time: 0.0912
Epoch: 003/100, Runtime 1.746659, Loss 1.901994, forward nfe 1942, backward nfe 0, Train: 0.3857, Val: 0.4382, Test: 0.4528, Best time: 5.4856
Epoch: 004/100, Runtime 1.927811, Loss 1.933367, forward nfe 2786, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 3.8930
Epoch: 005/100, Runtime 1.766221, Loss 1.932481, forward nfe 3636, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 006/100, Runtime 1.702332, Loss 1.942961, forward nfe 4480, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 007/100, Runtime 1.705676, Loss 1.953537, forward nfe 5330, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 008/100, Runtime 1.706853, Loss 1.954232, forward nfe 6180, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 009/100, Runtime 1.697233, Loss 1.943317, forward nfe 7024, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 010/100, Runtime 1.688521, Loss 1.914391, forward nfe 7868, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 011/100, Runtime 1.680203, Loss 1.887710, forward nfe 8706, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 012/100, Runtime 1.670344, Loss 1.853218, forward nfe 9544, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 013/100, Runtime 1.677986, Loss 1.848657, forward nfe 10382, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 014/100, Runtime 1.676054, Loss 1.800648, forward nfe 11220, backward nfe 0, Train: 0.4571, Val: 0.4713, Test: 0.4701, Best time: 128.0000
Epoch: 015/100, Runtime 1.643723, Loss 1.762246, forward nfe 12052, backward nfe 0, Train: 0.6143, Val: 0.4809, Test: 0.4843, Best time: 3.5138
Epoch: 016/100, Runtime 1.645685, Loss 1.720333, forward nfe 12878, backward nfe 0, Train: 0.6571, Val: 0.5324, Test: 0.5563, Best time: 9.6671
Epoch: 017/100, Runtime 1.648243, Loss 1.672234, forward nfe 13710, backward nfe 0, Train: 0.6929, Val: 0.5544, Test: 0.5756, Best time: 7.6698
Epoch: 018/100, Runtime 1.651553, Loss 1.650185, forward nfe 14530, backward nfe 0, Train: 0.6929, Val: 0.6206, Test: 0.6365, Best time: 17.8880
Epoch: 019/100, Runtime 1.654093, Loss 1.622498, forward nfe 15362, backward nfe 0, Train: 0.6143, Val: 0.6507, Test: 0.6437, Best time: 14.7887
Epoch: 020/100, Runtime 1.628640, Loss 1.575784, forward nfe 16188, backward nfe 0, Train: 0.6214, Val: 0.6801, Test: 0.6772, Best time: 54.4130
Epoch: 021/100, Runtime 1.628695, Loss 1.531598, forward nfe 17014, backward nfe 0, Train: 0.6357, Val: 0.7066, Test: 0.6944, Best time: 41.6824
Epoch: 022/100, Runtime 1.938907, Loss 1.498264, forward nfe 17852, backward nfe 0, Train: 0.6429, Val: 0.7309, Test: 0.7239, Best time: 45.2138
Epoch: 023/100, Runtime 1.736189, Loss 1.395987, forward nfe 18714, backward nfe 0, Train: 0.6571, Val: 0.7493, Test: 0.7391, Best time: 50.7945
Epoch: 024/100, Runtime 1.805972, Loss 1.291590, forward nfe 19588, backward nfe 0, Train: 0.6643, Val: 0.7647, Test: 0.7665, Best time: 79.7918
Epoch: 025/100, Runtime 1.843170, Loss 1.214396, forward nfe 20474, backward nfe 0, Train: 0.7071, Val: 0.7853, Test: 0.7827, Best time: 87.1101
Epoch: 026/100, Runtime 1.868341, Loss 1.144582, forward nfe 21366, backward nfe 0, Train: 0.7071, Val: 0.7853, Test: 0.7827, Best time: 128.0000
Epoch: 027/100, Runtime 1.971876, Loss 1.041737, forward nfe 22294, backward nfe 0, Train: 0.7071, Val: 0.7853, Test: 0.7827, Best time: 128.0000
Epoch: 028/100, Runtime 2.055080, Loss 0.995685, forward nfe 23252, backward nfe 0, Train: 0.7071, Val: 0.7853, Test: 0.7827, Best time: 128.0000
Epoch: 029/100, Runtime 2.207853, Loss 0.930002, forward nfe 24264, backward nfe 0, Train: 0.7071, Val: 0.7853, Test: 0.7827, Best time: 128.0000
Epoch: 030/100, Runtime 2.015586, Loss 0.850172, forward nfe 25204, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 39.7271
Epoch: 031/100, Runtime 1.988304, Loss 0.810148, forward nfe 26132, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 032/100, Runtime 1.875751, Loss 0.744670, forward nfe 27030, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 033/100, Runtime 1.854504, Loss 0.707412, forward nfe 27928, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 034/100, Runtime 1.931686, Loss 0.679669, forward nfe 28826, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 035/100, Runtime 1.864424, Loss 0.635905, forward nfe 29706, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 036/100, Runtime 1.852364, Loss 0.589099, forward nfe 30586, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 037/100, Runtime 1.994562, Loss 0.545205, forward nfe 31484, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 038/100, Runtime 2.022432, Loss 0.579965, forward nfe 32352, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 039/100, Runtime 1.755344, Loss 0.556164, forward nfe 33208, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 040/100, Runtime 1.852430, Loss 0.550067, forward nfe 34070, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 041/100, Runtime 1.774078, Loss 0.526989, forward nfe 34926, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 042/100, Runtime 1.731773, Loss 0.473669, forward nfe 35776, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 043/100, Runtime 1.733045, Loss 0.502934, forward nfe 36620, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 044/100, Runtime 1.770298, Loss 0.456495, forward nfe 37476, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 045/100, Runtime 1.737815, Loss 0.464022, forward nfe 38320, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 046/100, Runtime 1.758868, Loss 0.434854, forward nfe 39170, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 047/100, Runtime 1.745324, Loss 0.452613, forward nfe 40020, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 048/100, Runtime 1.744598, Loss 0.450582, forward nfe 40870, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 049/100, Runtime 1.703426, Loss 0.442913, forward nfe 41702, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 050/100, Runtime 1.662836, Loss 0.446822, forward nfe 42528, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 051/100, Runtime 1.715193, Loss 0.412538, forward nfe 43360, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 052/100, Runtime 1.700287, Loss 0.433470, forward nfe 44192, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 053/100, Runtime 1.693009, Loss 0.430220, forward nfe 45024, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 054/100, Runtime 1.698659, Loss 0.419087, forward nfe 45856, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 055/100, Runtime 1.887747, Loss 0.447784, forward nfe 46676, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 056/100, Runtime 1.639475, Loss 0.388955, forward nfe 47502, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 057/100, Runtime 1.658122, Loss 0.414438, forward nfe 48322, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 058/100, Runtime 1.660376, Loss 0.328961, forward nfe 49142, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 059/100, Runtime 1.651104, Loss 0.401292, forward nfe 49974, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 060/100, Runtime 1.705819, Loss 0.373054, forward nfe 50806, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 061/100, Runtime 1.640779, Loss 0.316988, forward nfe 51638, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 062/100, Runtime 1.642452, Loss 0.379867, forward nfe 52452, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 063/100, Runtime 1.661418, Loss 0.290148, forward nfe 53272, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 064/100, Runtime 1.604001, Loss 0.313794, forward nfe 54086, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 065/100, Runtime 1.579524, Loss 0.375331, forward nfe 54894, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 066/100, Runtime 1.646451, Loss 0.254153, forward nfe 55708, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 067/100, Runtime 1.616116, Loss 0.341072, forward nfe 56528, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 068/100, Runtime 1.630922, Loss 0.350885, forward nfe 57336, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 069/100, Runtime 1.590128, Loss 0.287648, forward nfe 58144, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 070/100, Runtime 1.610370, Loss 0.313299, forward nfe 58952, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 071/100, Runtime 1.566688, Loss 0.266737, forward nfe 59760, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 072/100, Runtime 1.597704, Loss 0.277244, forward nfe 60562, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 073/100, Runtime 1.795597, Loss 0.269441, forward nfe 61364, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 074/100, Runtime 1.566485, Loss 0.275760, forward nfe 62160, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 075/100, Runtime 1.541121, Loss 0.264243, forward nfe 62956, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 076/100, Runtime 1.578810, Loss 0.244094, forward nfe 63758, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 077/100, Runtime 1.539892, Loss 0.249721, forward nfe 64554, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 078/100, Runtime 1.590517, Loss 0.286313, forward nfe 65356, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 079/100, Runtime 1.579505, Loss 0.277473, forward nfe 66158, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 080/100, Runtime 1.523702, Loss 0.262310, forward nfe 66948, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 081/100, Runtime 1.522134, Loss 0.279982, forward nfe 67738, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 082/100, Runtime 1.565287, Loss 0.301554, forward nfe 68522, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 083/100, Runtime 1.548194, Loss 0.208589, forward nfe 69306, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 084/100, Runtime 1.493450, Loss 0.311657, forward nfe 70090, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 085/100, Runtime 1.518830, Loss 0.225647, forward nfe 70868, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 086/100, Runtime 1.507381, Loss 0.259683, forward nfe 71652, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 087/100, Runtime 1.494066, Loss 0.251163, forward nfe 72436, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 088/100, Runtime 1.525964, Loss 0.207374, forward nfe 73220, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 089/100, Runtime 1.556750, Loss 0.204022, forward nfe 73986, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 090/100, Runtime 1.431107, Loss 0.259125, forward nfe 74758, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 091/100, Runtime 1.486120, Loss 0.198389, forward nfe 75506, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 092/100, Runtime 1.443769, Loss 0.169521, forward nfe 76284, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 093/100, Runtime 1.642200, Loss 0.207364, forward nfe 77020, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 094/100, Runtime 1.387117, Loss 0.225512, forward nfe 77732, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 095/100, Runtime 1.433355, Loss 0.221372, forward nfe 78456, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 096/100, Runtime 1.429177, Loss 0.174887, forward nfe 79216, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 097/100, Runtime 1.376987, Loss 0.282931, forward nfe 79934, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 098/100, Runtime 1.384002, Loss 0.157121, forward nfe 80664, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
Epoch: 099/100, Runtime 1.330257, Loss 0.151108, forward nfe 81388, backward nfe 0, Train: 0.8429, Val: 0.8000, Test: 0.8183, Best time: 128.0000
best val accuracy 0.800000 with test accuracy 0.818274 at epoch 30 and best time 128.000000
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #15...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.692116, Loss 1.948673, forward nfe 242, backward nfe 0, Train: 0.4071, Val: 0.3250, Test: 0.2944, Best time: 3.9818
Epoch: 002/100, Runtime 1.735630, Loss 2.006105, forward nfe 1098, backward nfe 0, Train: 0.4714, Val: 0.3919, Test: 0.3645, Best time: 7.2011
Epoch: 003/100, Runtime 1.734148, Loss 1.984278, forward nfe 1960, backward nfe 0, Train: 0.4714, Val: 0.3919, Test: 0.3645, Best time: 128.0000
Epoch: 004/100, Runtime 1.745677, Loss 1.935223, forward nfe 2822, backward nfe 0, Train: 0.4500, Val: 0.4243, Test: 0.4071, Best time: 6.6245
Epoch: 005/100, Runtime 1.734964, Loss 1.912232, forward nfe 3684, backward nfe 0, Train: 0.5214, Val: 0.4779, Test: 0.4711, Best time: 9.6994
Epoch: 006/100, Runtime 4.115334, Loss 1.865573, forward nfe 5404, backward nfe 0, Train: 0.6286, Val: 0.6162, Test: 0.6112, Best time: 35.0611
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 110, in forward
    attention, values = self.multihead_att_layer(x, self.edge_index)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 263, in forward
    prods = torch.sum(src * dst_k, dim=1) / np.sqrt(self.d_k)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.81 GiB already allocated; 6.56 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.616176 with test accuracy 0.611168 at epoch 6 and best time 35.061119
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #16...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.741637, Loss 1.946385, forward nfe 242, backward nfe 0, Train: 0.1429, Val: 0.1390, Test: 0.1462, Best time: 0.0936
Epoch: 002/100, Runtime 1.740502, Loss 2.072400, forward nfe 1098, backward nfe 0, Train: 0.4143, Val: 0.5676, Test: 0.5442, Best time: 2.4583
Epoch: 003/100, Runtime 1.832675, Loss 1.896972, forward nfe 2002, backward nfe 0, Train: 0.4143, Val: 0.5676, Test: 0.5442, Best time: 128.0000
Epoch: 004/100, Runtime 1.996017, Loss 1.889819, forward nfe 2864, backward nfe 0, Train: 0.4143, Val: 0.5676, Test: 0.5442, Best time: 128.0000
Epoch: 005/100, Runtime 1.726027, Loss 1.860721, forward nfe 3732, backward nfe 0, Train: 0.4143, Val: 0.5676, Test: 0.5442, Best time: 128.0000
Epoch: 006/100, Runtime 1.788412, Loss 1.783427, forward nfe 4612, backward nfe 0, Train: 0.5643, Val: 0.5934, Test: 0.5838, Best time: 4.0907
Epoch: 007/100, Runtime 2.062024, Loss 1.672032, forward nfe 5594, backward nfe 0, Train: 0.6071, Val: 0.6654, Test: 0.6518, Best time: 8.1529
Epoch: 008/100, Runtime 3.658198, Loss 1.540116, forward nfe 7164, backward nfe 0, Train: 0.6214, Val: 0.7390, Test: 0.7299, Best time: 16.9198
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 111, in forward
    ax = self.multiply_attention(x, attention, values)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 102, in multiply_attention
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 25, in spmm
    out = out * value.unsqueeze(-1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.81 GiB already allocated; 14.56 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.738971 with test accuracy 0.729949 at epoch 8 and best time 16.919792
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #17...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.709795, Loss 1.945434, forward nfe 242, backward nfe 0, Train: 0.3143, Val: 0.2184, Test: 0.1939, Best time: 0.0944
Epoch: 002/100, Runtime 2.021594, Loss 1.905707, forward nfe 1206, backward nfe 0, Train: 0.4357, Val: 0.4897, Test: 0.4782, Best time: 3.4054
Epoch: 003/100, Runtime 2.218658, Loss 1.910103, forward nfe 2260, backward nfe 0, Train: 0.5643, Val: 0.5037, Test: 0.4751, Best time: 2.6085
Epoch: 004/100, Runtime 1.717360, Loss 1.875231, forward nfe 3122, backward nfe 0, Train: 0.5643, Val: 0.5037, Test: 0.4751, Best time: 128.0000
Epoch: 005/100, Runtime 1.892722, Loss 1.776237, forward nfe 4044, backward nfe 0, Train: 0.5643, Val: 0.5037, Test: 0.4751, Best time: 128.0000
Epoch: 006/100, Runtime 2.068184, Loss 1.647853, forward nfe 5020, backward nfe 0, Train: 0.6929, Val: 0.6551, Test: 0.6396, Best time: 10.6154
Epoch: 007/100, Runtime 2.618940, Loss 1.446637, forward nfe 6230, backward nfe 0, Train: 0.6357, Val: 0.6912, Test: 0.6812, Best time: 8.1277
Epoch: 008/100, Runtime 4.365039, Loss 1.371592, forward nfe 7998, backward nfe 0, Train: 0.7214, Val: 0.7485, Test: 0.7239, Best time: 53.0141
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 111, in forward
    ax = self.multiply_attention(x, attention, values)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 102, in multiply_attention
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 25, in spmm
    out = out * value.unsqueeze(-1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.80 GiB already allocated; 16.56 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.748529 with test accuracy 0.723858 at epoch 8 and best time 53.014137
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #18...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.743056, Loss 1.948364, forward nfe 242, backward nfe 0, Train: 0.2286, Val: 0.2007, Test: 0.2051, Best time: 0.0952
Epoch: 002/100, Runtime 1.718845, Loss 2.030603, forward nfe 1098, backward nfe 0, Train: 0.2286, Val: 0.2007, Test: 0.2051, Best time: 128.0000
Epoch: 003/100, Runtime 1.765668, Loss 1.983645, forward nfe 1966, backward nfe 0, Train: 0.2286, Val: 0.2007, Test: 0.2051, Best time: 128.0000
Epoch: 004/100, Runtime 1.744358, Loss 1.935378, forward nfe 2828, backward nfe 0, Train: 0.2286, Val: 0.2007, Test: 0.2051, Best time: 128.0000
Epoch: 005/100, Runtime 2.037603, Loss 1.900797, forward nfe 3804, backward nfe 0, Train: 0.2929, Val: 0.3007, Test: 0.3086, Best time: 68.4827
Epoch: 006/100, Runtime 4.634318, Loss 1.847480, forward nfe 5674, backward nfe 0, Train: 0.5571, Val: 0.4574, Test: 0.4528, Best time: 57.9015
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 111, in forward
    ax = self.multiply_attention(x, attention, values)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 102, in multiply_attention
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 25, in spmm
    out = out * value.unsqueeze(-1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 17.79 GiB already allocated; 20.56 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.457353 with test accuracy 0.452792 at epoch 6 and best time 57.901510
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #19...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  transformer
[INFO] Block type :  constant
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 1433])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([7, 80])
m2.module.bias
torch.Size([7])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Q.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.V.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.weight
torch.Size([128, 80])
odeblock.reg_odefunc.odefunc.multihead_att_layer.K.bias
torch.Size([128])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.weight
torch.Size([80, 16])
odeblock.reg_odefunc.odefunc.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001/100, Runtime 1.853426, Loss 1.948188, forward nfe 242, backward nfe 0, Train: 0.3500, Val: 0.2853, Test: 0.2924, Best time: 0.3827
Epoch: 002/100, Runtime 1.740193, Loss 2.021410, forward nfe 1098, backward nfe 0, Train: 0.3500, Val: 0.2853, Test: 0.2924, Best time: 128.0000
Epoch: 003/100, Runtime 1.758564, Loss 1.976505, forward nfe 1966, backward nfe 0, Train: 0.3500, Val: 0.2853, Test: 0.2924, Best time: 128.0000
Epoch: 004/100, Runtime 1.740193, Loss 1.931449, forward nfe 2828, backward nfe 0, Train: 0.3500, Val: 0.2853, Test: 0.2924, Best time: 128.0000
Epoch: 005/100, Runtime 2.045434, Loss 1.894628, forward nfe 3798, backward nfe 0, Train: 0.4286, Val: 0.4831, Test: 0.4629, Best time: 80.9387
Epoch: 006/100, Runtime 4.194556, Loss 1.823531, forward nfe 5500, backward nfe 0, Train: 0.5571, Val: 0.5441, Test: 0.5513, Best time: 34.0079
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_constant.py", line 56, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_transformer_attention.py", line 107, in forward
    raise MaxNFEException
utils.MaxNFEException
best val accuracy 0.544118 with test accuracy 0.551269 at epoch 6 and best time 34.007923
[INFO] Loggin into 4_tbl_1_nlr_grand_cora.json for seed #20...