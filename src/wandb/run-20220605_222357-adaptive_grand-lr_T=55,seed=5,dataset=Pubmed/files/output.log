****************** Adaptive GRAND laplacian function ******************
****************** Adaptive GRAND laplacian function ******************
GNNEarly
m1.module.weight
torch.Size([128, 500])
m1.module.bias
torch.Size([128])
m2.module.weight
torch.Size([3, 128])
m2.module.bias
torch.Size([3])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([128, 128])
odeblock.odefunc.d
torch.Size([128])
odeblock.odefunc.k_d
torch.Size([128])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([128, 128])
odeblock.reg_odefunc.odefunc.d
torch.Size([128])
odeblock.reg_odefunc.odefunc.k_d
torch.Size([128])
odeblock.multihead_att_layer.Q.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.Q.bias
torch.Size([16])
odeblock.multihead_att_layer.V.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.V.bias
torch.Size([16])
odeblock.multihead_att_layer.K.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.K.bias
torch.Size([16])
odeblock.multihead_att_layer.Wout.weight
torch.Size([128, 16])
odeblock.multihead_att_layer.Wout.bias
torch.Size([128])
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: AdaptiveHeunSolver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
Epoch: 001/100, Runtime 2140.671358, Loss 1.099457, forward nfe 116, backward nfe 249891, Train: 0.5833, Val: 0.5188, Test: 0.5283, Best time: 0.2392
Epoch: 002/100, Runtime 1956.475749, Loss 1.098934, forward nfe 762, backward nfe 477730, Train: 0.7500, Val: 0.6472, Test: 0.6377, Best time: 2.7179
Epoch: 003/100, Runtime 1406.175807, Loss 1.093819, forward nfe 1402, backward nfe 641992, Train: 0.7833, Val: 0.6722, Test: 0.6851, Best time: 2.6882
Epoch: 004/100, Runtime 2074.403628, Loss 1.082874, forward nfe 2048, backward nfe 887123, Train: 0.7833, Val: 0.6819, Test: 0.6877, Best time: 2.6188
Epoch: 005/100, Runtime 1829.912796, Loss 1.063849, forward nfe 2694, backward nfe 1102565, Train: 0.7833, Val: 0.6889, Test: 0.6960, Best time: 2.5512
Epoch: 006/100, Runtime 1998.038907, Loss 1.034321, forward nfe 3340, backward nfe 1337991, Train: 0.7833, Val: 0.6965, Test: 0.7034, Best time: 2.4930
Epoch: 007/100, Runtime 2316.103713, Loss 0.998866, forward nfe 3986, backward nfe 1612611, Train: 0.8167, Val: 0.6986, Test: 0.7118, Best time: 2.4416
Epoch: 008/100, Runtime 2091.625873, Loss 0.955206, forward nfe 4644, backward nfe 1859955, Train: 0.8333, Val: 0.7049, Test: 0.7181, Best time: 2.3951
Epoch: 009/100, Runtime 1758.725691, Loss 0.900447, forward nfe 5308, backward nfe 2069738, Train: 0.8333, Val: 0.7111, Test: 0.7251, Best time: 2.3533
Epoch: 010/100, Runtime 1580.950563, Loss 0.841647, forward nfe 5978, backward nfe 2255590, Train: 0.8333, Val: 0.7139, Test: 0.7290, Best time: 2.3143
Epoch: 011/100, Runtime 1578.796648, Loss 0.769358, forward nfe 6648, backward nfe 2441924, Train: 0.8333, Val: 0.7153, Test: 0.7284, Best time: 2.2777
Epoch: 012/100, Runtime 1768.799006, Loss 0.707803, forward nfe 7330, backward nfe 2652085, Train: 0.8167, Val: 0.7167, Test: 0.7299, Best time: 2.2445
Epoch: 013/100, Runtime 2110.990243, Loss 0.641665, forward nfe 8012, backward nfe 2903476, Train: 0.8167, Val: 0.7201, Test: 0.7324, Best time: 2.2137
Epoch: 014/100, Runtime 1343.287608, Loss 0.574177, forward nfe 8676, backward nfe 3063712, Train: 0.8333, Val: 0.7312, Test: 0.7379, Best time: 2.1859
Epoch: 015/100, Runtime 1584.676132, Loss 0.541660, forward nfe 9346, backward nfe 3252160, Train: 0.8167, Val: 0.7326, Test: 0.7417, Best time: 2.1626
Epoch: 016/100, Runtime 1713.628365, Loss 0.479298, forward nfe 10016, backward nfe 3457743, Train: 0.8167, Val: 0.7354, Test: 0.7434, Best time: 2.1421
Epoch: 017/100, Runtime 1578.077843, Loss 0.451468, forward nfe 10686, backward nfe 3645350, Train: 0.8167, Val: 0.7354, Test: 0.7434, Best time: 55.0000
Epoch: 018/100, Runtime 2537.934376, Loss 0.409107, forward nfe 11356, backward nfe 3950388, Train: 0.8167, Val: 0.7354, Test: 0.7434, Best time: 55.0000
Epoch: 019/100, Runtime 1875.364285, Loss 0.367266, forward nfe 12026, backward nfe 4175386, Train: 0.9000, Val: 0.7472, Test: 0.7516, Best time: 5.3709
Epoch: 020/100, Runtime 1539.495147, Loss 0.353200, forward nfe 12696, backward nfe 4356705, Train: 0.9333, Val: 0.7660, Test: 0.7652, Best time: 14.8430
Epoch: 021/100, Runtime 1029.354024, Loss 0.336518, forward nfe 13366, backward nfe 4476851, Train: 0.9333, Val: 0.7660, Test: 0.7652, Best time: 55.0000
Traceback (most recent call last):
  File "run_GNN.py", line 259, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "run_GNN.py", line 89, in train
    loss.backward()
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 95, in augmented_dynamics
    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 275, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.20 GiB total capacity; 1.60 GiB already allocated; 43.06 MiB free; 1.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.765972 with test accuracy 0.765219 at epoch 20 and best time 55.000000