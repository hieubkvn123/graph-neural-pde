****************** Adaptive GRAND laplacian function ******************
****************** Adaptive GRAND laplacian function ******************
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: AdaptiveHeunSolver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
GNNEarly
m1.module.weight
torch.Size([128, 500])
m1.module.bias
torch.Size([128])
m2.module.weight
torch.Size([3, 128])
m2.module.bias
torch.Size([3])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([128, 128])
odeblock.odefunc.d
torch.Size([128])
odeblock.odefunc.k_d
torch.Size([128])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([128, 128])
odeblock.reg_odefunc.odefunc.d
torch.Size([128])
odeblock.reg_odefunc.odefunc.k_d
torch.Size([128])
odeblock.multihead_att_layer.Q.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.Q.bias
torch.Size([16])
odeblock.multihead_att_layer.V.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.V.bias
torch.Size([16])
odeblock.multihead_att_layer.K.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.K.bias
torch.Size([16])
odeblock.multihead_att_layer.Wout.weight
torch.Size([128, 16])
odeblock.multihead_att_layer.Wout.bias
torch.Size([128])
Epoch: 001/100, Runtime 0.296858, Loss 1.099777, forward nfe 20, backward nfe 8, Train: 0.9333, Val: 0.6562, Test: 0.6283, Best time: 2.5737
Epoch: 002/100, Runtime 0.313620, Loss 1.085781, forward nfe 96, backward nfe 17, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 24.4626
Epoch: 003/100, Runtime 0.327436, Loss 1.066618, forward nfe 172, backward nfe 27, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 004/100, Runtime 0.305317, Loss 1.043038, forward nfe 254, backward nfe 39, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 005/100, Runtime 0.344366, Loss 1.015532, forward nfe 324, backward nfe 51, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 006/100, Runtime 0.332001, Loss 0.982219, forward nfe 406, backward nfe 64, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 007/100, Runtime 0.329731, Loss 0.944944, forward nfe 482, backward nfe 76, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 008/100, Runtime 0.356733, Loss 0.895952, forward nfe 558, backward nfe 90, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 009/100, Runtime 0.341916, Loss 0.833802, forward nfe 634, backward nfe 103, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 010/100, Runtime 0.329318, Loss 0.777281, forward nfe 710, backward nfe 117, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 011/100, Runtime 0.326658, Loss 0.711126, forward nfe 780, backward nfe 130, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 012/100, Runtime 0.325376, Loss 0.656685, forward nfe 850, backward nfe 143, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 013/100, Runtime 0.339663, Loss 0.584900, forward nfe 920, backward nfe 157, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 014/100, Runtime 0.345595, Loss 0.528106, forward nfe 990, backward nfe 171, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 015/100, Runtime 0.322903, Loss 0.484160, forward nfe 1060, backward nfe 184, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 016/100, Runtime 0.326734, Loss 0.415401, forward nfe 1130, backward nfe 197, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 017/100, Runtime 0.353848, Loss 0.344483, forward nfe 1200, backward nfe 212, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 018/100, Runtime 0.334646, Loss 0.312731, forward nfe 1270, backward nfe 226, Train: 0.8000, Val: 0.7194, Test: 0.7095, Best time: 5.0000
Epoch: 019/100, Runtime 0.311315, Loss 0.296583, forward nfe 1340, backward nfe 238, Train: 1.0000, Val: 0.7243, Test: 0.7087, Best time: 5.9039
Epoch: 020/100, Runtime 0.296990, Loss 0.278139, forward nfe 1410, backward nfe 251, Train: 0.9833, Val: 0.7306, Test: 0.7109, Best time: 1.8200
Epoch: 021/100, Runtime 0.286876, Loss 0.201448, forward nfe 1468, backward nfe 263, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 6.1350
Epoch: 022/100, Runtime 0.278583, Loss 0.199992, forward nfe 1526, backward nfe 274, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 5.0000
Epoch: 023/100, Runtime 0.284358, Loss 0.140009, forward nfe 1584, backward nfe 286, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 5.0000
Epoch: 024/100, Runtime 0.293390, Loss 0.167708, forward nfe 1642, backward nfe 299, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 5.0000
Epoch: 025/100, Runtime 0.260327, Loss 0.148399, forward nfe 1700, backward nfe 309, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 5.0000
Epoch: 026/100, Runtime 0.261621, Loss 0.148712, forward nfe 1758, backward nfe 319, Train: 1.0000, Val: 0.7354, Test: 0.7175, Best time: 5.0000
Epoch: 027/100, Runtime 0.295756, Loss 0.160685, forward nfe 1816, backward nfe 331, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 30.4340
Epoch: 028/100, Runtime 0.284815, Loss 0.157664, forward nfe 1874, backward nfe 342, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 029/100, Runtime 0.270295, Loss 0.113526, forward nfe 1938, backward nfe 351, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 030/100, Runtime 0.252011, Loss 0.113010, forward nfe 2002, backward nfe 358, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 031/100, Runtime 0.302202, Loss 0.131263, forward nfe 2066, backward nfe 370, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 032/100, Runtime 0.292555, Loss 0.149197, forward nfe 2130, backward nfe 381, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 033/100, Runtime 0.274158, Loss 0.134576, forward nfe 2194, backward nfe 390, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 034/100, Runtime 0.286264, Loss 0.160063, forward nfe 2258, backward nfe 400, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 035/100, Runtime 0.238212, Loss 0.141260, forward nfe 2322, backward nfe 409, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 036/100, Runtime 0.227106, Loss 0.120338, forward nfe 2374, backward nfe 417, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 037/100, Runtime 0.254274, Loss 0.146325, forward nfe 2426, backward nfe 427, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 038/100, Runtime 0.283188, Loss 0.162022, forward nfe 2478, backward nfe 440, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 039/100, Runtime 0.221699, Loss 0.079190, forward nfe 2530, backward nfe 447, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 040/100, Runtime 0.260401, Loss 0.127503, forward nfe 2582, backward nfe 457, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 041/100, Runtime 0.254161, Loss 0.104543, forward nfe 2640, backward nfe 466, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 042/100, Runtime 0.266997, Loss 0.121613, forward nfe 2698, backward nfe 476, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 043/100, Runtime 0.257641, Loss 0.132133, forward nfe 2756, backward nfe 486, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 044/100, Runtime 0.229465, Loss 0.118095, forward nfe 2814, backward nfe 494, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 045/100, Runtime 0.254102, Loss 0.139469, forward nfe 2866, backward nfe 504, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 046/100, Runtime 0.272454, Loss 0.155340, forward nfe 2918, backward nfe 516, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 047/100, Runtime 0.218609, Loss 0.108450, forward nfe 2970, backward nfe 523, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 048/100, Runtime 0.251514, Loss 0.102088, forward nfe 3022, backward nfe 533, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 049/100, Runtime 0.229797, Loss 0.081571, forward nfe 3074, backward nfe 541, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 050/100, Runtime 0.253821, Loss 0.129764, forward nfe 3126, backward nfe 551, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 051/100, Runtime 0.250363, Loss 0.073304, forward nfe 3178, backward nfe 561, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 052/100, Runtime 0.244262, Loss 0.089855, forward nfe 3230, backward nfe 570, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 053/100, Runtime 0.282425, Loss 0.113527, forward nfe 3282, backward nfe 583, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 054/100, Runtime 0.251773, Loss 0.112351, forward nfe 3334, backward nfe 593, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 055/100, Runtime 0.244158, Loss 0.099004, forward nfe 3386, backward nfe 602, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 056/100, Runtime 0.241741, Loss 0.092085, forward nfe 3438, backward nfe 611, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 057/100, Runtime 0.233226, Loss 0.117915, forward nfe 3490, backward nfe 620, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 058/100, Runtime 0.240137, Loss 0.078579, forward nfe 3542, backward nfe 629, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 059/100, Runtime 0.258599, Loss 0.114141, forward nfe 3594, backward nfe 639, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 060/100, Runtime 0.224203, Loss 0.121205, forward nfe 3646, backward nfe 647, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 061/100, Runtime 0.219726, Loss 0.083032, forward nfe 3698, backward nfe 654, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 062/100, Runtime 0.215417, Loss 0.077056, forward nfe 3750, backward nfe 661, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 063/100, Runtime 0.227596, Loss 0.070631, forward nfe 3802, backward nfe 669, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 064/100, Runtime 0.226124, Loss 0.060987, forward nfe 3854, backward nfe 677, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 065/100, Runtime 0.231416, Loss 0.080591, forward nfe 3906, backward nfe 685, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 066/100, Runtime 0.234298, Loss 0.087133, forward nfe 3958, backward nfe 693, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 067/100, Runtime 0.245483, Loss 0.082886, forward nfe 4010, backward nfe 702, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 068/100, Runtime 0.240124, Loss 0.085477, forward nfe 4062, backward nfe 711, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 069/100, Runtime 0.255046, Loss 0.095050, forward nfe 4114, backward nfe 721, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 070/100, Runtime 0.251832, Loss 0.084897, forward nfe 4166, backward nfe 731, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 071/100, Runtime 0.223366, Loss 0.088595, forward nfe 4218, backward nfe 738, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 072/100, Runtime 0.234979, Loss 0.079192, forward nfe 4270, backward nfe 747, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 073/100, Runtime 0.231144, Loss 0.073931, forward nfe 4322, backward nfe 756, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 074/100, Runtime 0.244695, Loss 0.072334, forward nfe 4374, backward nfe 765, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 075/100, Runtime 0.239890, Loss 0.113016, forward nfe 4426, backward nfe 774, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 076/100, Runtime 0.243813, Loss 0.092066, forward nfe 4478, backward nfe 783, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 077/100, Runtime 0.230267, Loss 0.070431, forward nfe 4530, backward nfe 791, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 078/100, Runtime 0.223297, Loss 0.073117, forward nfe 4582, backward nfe 798, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 079/100, Runtime 0.218022, Loss 0.059824, forward nfe 4634, backward nfe 805, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 080/100, Runtime 0.219813, Loss 0.073330, forward nfe 4686, backward nfe 812, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 081/100, Runtime 0.228918, Loss 0.094565, forward nfe 4738, backward nfe 820, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 082/100, Runtime 0.239537, Loss 0.047781, forward nfe 4790, backward nfe 829, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 083/100, Runtime 0.230638, Loss 0.060520, forward nfe 4842, backward nfe 837, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 084/100, Runtime 0.238966, Loss 0.053298, forward nfe 4894, backward nfe 846, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 085/100, Runtime 0.240475, Loss 0.076293, forward nfe 4946, backward nfe 855, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 086/100, Runtime 0.239751, Loss 0.077485, forward nfe 4998, backward nfe 864, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 087/100, Runtime 0.241616, Loss 0.109644, forward nfe 5050, backward nfe 873, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 088/100, Runtime 0.242743, Loss 0.108967, forward nfe 5102, backward nfe 882, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 089/100, Runtime 0.244482, Loss 0.061774, forward nfe 5154, backward nfe 891, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 090/100, Runtime 0.226206, Loss 0.074770, forward nfe 5206, backward nfe 899, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 091/100, Runtime 0.239198, Loss 0.110425, forward nfe 5258, backward nfe 908, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 092/100, Runtime 0.218740, Loss 0.058229, forward nfe 5310, backward nfe 915, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 093/100, Runtime 0.238240, Loss 0.078689, forward nfe 5362, backward nfe 923, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Epoch: 094/100, Runtime 0.230236, Loss 0.078581, forward nfe 5414, backward nfe 931, Train: 1.0000, Val: 0.7479, Test: 0.7265, Best time: 5.0000
Traceback (most recent call last):
  File "run_GNN.py", line 259, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "run_GNN.py", line 89, in train
    loss.backward()
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 95, in augmented_dynamics
    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 275, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.20 GiB total capacity; 2.92 GiB already allocated; 10.06 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.747917 with test accuracy 0.726464 at epoch 27 and best time 5.000000