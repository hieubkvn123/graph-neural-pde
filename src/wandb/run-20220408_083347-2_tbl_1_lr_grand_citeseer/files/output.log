[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
Epoch: 001, Runtime 1.193620, Loss 1.793406, forward nfe 356, backward nfe 0, Train: 0.4167, Val: 0.4870, Test: 0.5048, Best time: 68.0162
Epoch: 002, Runtime 1.073939, Loss 1.769198, forward nfe 1296, backward nfe 0, Train: 0.4167, Val: 0.4870, Test: 0.5048, Best time: 128.0000
Epoch: 003, Runtime 1.133258, Loss 1.736296, forward nfe 2290, backward nfe 0, Train: 0.4167, Val: 0.4870, Test: 0.5048, Best time: 128.0000
Epoch: 004, Runtime 1.391699, Loss 1.683727, forward nfe 3308, backward nfe 0, Train: 0.5917, Val: 0.5000, Test: 0.5548, Best time: 134.4515
Epoch: 005, Runtime 1.212229, Loss 1.569401, forward nfe 4362, backward nfe 0, Train: 0.7083, Val: 0.6543, Test: 0.6774, Best time: 88.4015
Epoch: 006, Runtime 1.401936, Loss 1.538644, forward nfe 5470, backward nfe 0, Train: 0.6833, Val: 0.6826, Test: 0.6952, Best time: 26.6793
Epoch: 007, Runtime 1.282639, Loss 1.501975, forward nfe 6578, backward nfe 0, Train: 0.6917, Val: 0.7065, Test: 0.7210, Best time: 22.5081
Epoch: 008, Runtime 1.293299, Loss 1.475615, forward nfe 7698, backward nfe 0, Train: 0.7250, Val: 0.7152, Test: 0.7403, Best time: 69.4998
Epoch: 009, Runtime 1.215084, Loss 1.485452, forward nfe 8752, backward nfe 0, Train: 0.7333, Val: 0.7246, Test: 0.7419, Best time: 125.8409
Epoch: 010, Runtime 1.211449, Loss 1.455567, forward nfe 9806, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 96.8714
Epoch: 011, Runtime 1.267672, Loss 1.431259, forward nfe 10842, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 012, Runtime 1.247437, Loss 1.407843, forward nfe 11914, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 013, Runtime 1.310750, Loss 1.373431, forward nfe 13028, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 014, Runtime 1.591122, Loss 1.416164, forward nfe 14310, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 015, Runtime 1.322674, Loss 1.276891, forward nfe 15436, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 016, Runtime 1.666845, Loss 1.426292, forward nfe 16790, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 017, Runtime 1.275133, Loss 1.286915, forward nfe 17880, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 018, Runtime 1.340106, Loss 1.255020, forward nfe 19012, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 019, Runtime 1.646804, Loss 1.348702, forward nfe 20336, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 020, Runtime 1.584659, Loss 1.312805, forward nfe 21630, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 021, Runtime 1.801050, Loss 1.271789, forward nfe 23050, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 022, Runtime 1.542529, Loss 1.344871, forward nfe 24314, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 023, Runtime 1.592265, Loss 1.341251, forward nfe 25620, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 024, Runtime 1.449291, Loss 1.425082, forward nfe 26830, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 025, Runtime 1.648831, Loss 1.440346, forward nfe 28178, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 026, Runtime 1.718836, Loss 1.320563, forward nfe 29568, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 027, Runtime 1.969891, Loss 1.382741, forward nfe 31054, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 028, Runtime 1.851403, Loss 1.428160, forward nfe 32522, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 029, Runtime 1.612581, Loss 1.379641, forward nfe 33864, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 030, Runtime 2.270898, Loss 1.395904, forward nfe 35542, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 031, Runtime 1.489037, Loss 1.290766, forward nfe 36848, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 032, Runtime 2.026845, Loss 1.439400, forward nfe 38538, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 033, Runtime 1.529176, Loss 1.321703, forward nfe 39874, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 034, Runtime 1.758379, Loss 1.341272, forward nfe 41372, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 035, Runtime 1.668273, Loss 1.365246, forward nfe 42804, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 036, Runtime 1.783312, Loss 1.524320, forward nfe 44308, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 037, Runtime 1.842993, Loss 1.407831, forward nfe 45860, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 038, Runtime 1.879768, Loss 1.429542, forward nfe 47358, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 039, Runtime 1.767474, Loss 1.375887, forward nfe 48850, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 040, Runtime 2.133830, Loss 1.223264, forward nfe 50612, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 041, Runtime 2.357933, Loss 1.346013, forward nfe 52416, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 042, Runtime 1.770931, Loss 1.427415, forward nfe 53914, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 043, Runtime 1.869885, Loss 1.354520, forward nfe 55478, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 044, Runtime 2.126508, Loss 1.384335, forward nfe 57240, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 045, Runtime 2.155670, Loss 1.344205, forward nfe 59038, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 046, Runtime 1.673610, Loss 1.383837, forward nfe 60488, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 047, Runtime 2.069970, Loss 1.376220, forward nfe 62136, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 048, Runtime 1.686157, Loss 1.398464, forward nfe 63592, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 049, Runtime 1.591306, Loss 1.350819, forward nfe 64946, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 050, Runtime 1.570965, Loss 1.437476, forward nfe 66312, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 051, Runtime 1.842501, Loss 1.319590, forward nfe 67858, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 052, Runtime 1.638001, Loss 1.376854, forward nfe 69278, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 053, Runtime 2.072161, Loss 1.457397, forward nfe 70998, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 054, Runtime 1.630041, Loss 1.588402, forward nfe 72406, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 055, Runtime 1.768216, Loss 1.398317, forward nfe 73898, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 056, Runtime 2.168802, Loss 1.276884, forward nfe 75708, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 057, Runtime 2.173831, Loss 1.384279, forward nfe 77476, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 058, Runtime 2.279881, Loss 1.433972, forward nfe 79376, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 059, Runtime 2.760060, Loss 1.305936, forward nfe 81612, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 060, Runtime 2.399658, Loss 1.287645, forward nfe 83620, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 061, Runtime 2.478122, Loss 1.382589, forward nfe 85544, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 062, Runtime 2.671741, Loss 1.321115, forward nfe 87666, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 063, Runtime 2.662541, Loss 1.334406, forward nfe 89800, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 064, Runtime 2.005486, Loss 1.423353, forward nfe 91490, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 065, Runtime 2.369706, Loss 1.431797, forward nfe 93474, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 066, Runtime 2.195453, Loss 1.310716, forward nfe 95320, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 067, Runtime 1.996660, Loss 1.338528, forward nfe 97004, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 068, Runtime 1.706431, Loss 1.379736, forward nfe 98490, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 069, Runtime 1.753617, Loss 1.494594, forward nfe 100000, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 070, Runtime 2.356149, Loss 1.400799, forward nfe 101912, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 071, Runtime 2.145102, Loss 1.447089, forward nfe 103602, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 072, Runtime 1.568404, Loss 1.416007, forward nfe 104980, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 073, Runtime 2.127266, Loss 1.362486, forward nfe 106772, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 074, Runtime 1.700361, Loss 1.362819, forward nfe 108246, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 075, Runtime 2.037898, Loss 1.385813, forward nfe 109972, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 076, Runtime 1.728566, Loss 1.169553, forward nfe 111470, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 077, Runtime 1.982729, Loss 1.351956, forward nfe 113154, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 078, Runtime 1.584851, Loss 1.386179, forward nfe 114538, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 079, Runtime 1.626575, Loss 1.281358, forward nfe 115952, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 080, Runtime 1.632555, Loss 1.360854, forward nfe 117366, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 081, Runtime 1.901030, Loss 1.429329, forward nfe 118996, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 082, Runtime 2.056224, Loss 1.303045, forward nfe 120668, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 083, Runtime 2.439815, Loss 1.480318, forward nfe 122700, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 084, Runtime 2.278816, Loss 1.263641, forward nfe 124612, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 085, Runtime 1.735288, Loss 1.247404, forward nfe 126110, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 086, Runtime 2.115999, Loss 1.604084, forward nfe 127896, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 087, Runtime 1.856561, Loss 1.405842, forward nfe 129490, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 088, Runtime 2.264332, Loss 1.446684, forward nfe 131384, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 089, Runtime 1.821602, Loss 1.387389, forward nfe 132900, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 090, Runtime 2.642827, Loss 1.296053, forward nfe 135064, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 091, Runtime 2.153875, Loss 1.253729, forward nfe 136754, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 092, Runtime 2.256024, Loss 1.451134, forward nfe 138636, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 093, Runtime 2.074087, Loss 1.368146, forward nfe 140386, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 094, Runtime 2.046144, Loss 1.429633, forward nfe 142118, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 095, Runtime 1.939285, Loss 1.422832, forward nfe 143766, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 096, Runtime 1.855104, Loss 1.572924, forward nfe 145354, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 097, Runtime 2.077143, Loss 1.375077, forward nfe 147038, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 098, Runtime 1.829354, Loss 1.306470, forward nfe 148602, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 099, Runtime 2.283943, Loss 1.197912, forward nfe 150514, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 100, Runtime 2.917292, Loss 1.365902, forward nfe 152768, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 101, Runtime 2.260513, Loss 1.398287, forward nfe 154668, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 102, Runtime 2.405128, Loss 1.423362, forward nfe 156676, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 103, Runtime 2.164643, Loss 1.335241, forward nfe 158366, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 104, Runtime 2.525297, Loss 1.460930, forward nfe 160464, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 105, Runtime 1.845129, Loss 1.438251, forward nfe 162040, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 106, Runtime 2.112200, Loss 1.406288, forward nfe 163814, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 107, Runtime 2.066185, Loss 1.438269, forward nfe 165552, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 108, Runtime 2.748028, Loss 1.504082, forward nfe 167818, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 109, Runtime 2.597642, Loss 1.563993, forward nfe 169970, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 110, Runtime 2.178750, Loss 1.488567, forward nfe 171732, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 111, Runtime 2.636307, Loss 1.407225, forward nfe 173902, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 112, Runtime 2.157250, Loss 1.449713, forward nfe 175706, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 113, Runtime 2.195601, Loss 1.274594, forward nfe 177552, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 114, Runtime 2.313666, Loss 1.381412, forward nfe 179482, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 115, Runtime 2.327399, Loss 1.299047, forward nfe 181424, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 116, Runtime 2.037030, Loss 1.325657, forward nfe 183144, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 117, Runtime 2.264609, Loss 1.279325, forward nfe 184966, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 118, Runtime 2.112313, Loss 1.341389, forward nfe 186602, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 119, Runtime 1.943239, Loss 1.201635, forward nfe 188256, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 120, Runtime 2.724210, Loss 1.408171, forward nfe 190504, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 121, Runtime 2.234360, Loss 1.446963, forward nfe 192368, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 122, Runtime 3.027755, Loss 1.483852, forward nfe 194820, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 123, Runtime 2.229903, Loss 1.468973, forward nfe 196642, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 124, Runtime 1.931483, Loss 1.501693, forward nfe 198284, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 125, Runtime 2.286020, Loss 1.425473, forward nfe 200196, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 126, Runtime 2.603256, Loss 1.469550, forward nfe 202204, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 127, Runtime 2.354889, Loss 1.461653, forward nfe 204158, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 128, Runtime 2.088799, Loss 1.510803, forward nfe 205914, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 129, Runtime 2.430832, Loss 1.463316, forward nfe 207934, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 130, Runtime 2.075346, Loss 1.393742, forward nfe 209636, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 131, Runtime 1.784312, Loss 1.339667, forward nfe 211170, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 132, Runtime 2.199995, Loss 1.591973, forward nfe 213010, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 133, Runtime 1.597955, Loss 1.522365, forward nfe 214388, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 134, Runtime 2.520814, Loss 1.512526, forward nfe 216474, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 135, Runtime 2.279742, Loss 1.398137, forward nfe 218368, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 136, Runtime 2.350310, Loss 1.441053, forward nfe 220322, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 137, Runtime 2.819332, Loss 1.463292, forward nfe 222558, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 138, Runtime 2.216688, Loss 1.331886, forward nfe 224404, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 139, Runtime 2.484270, Loss 1.465214, forward nfe 226466, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 140, Runtime 2.364143, Loss 1.523219, forward nfe 228438, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 141, Runtime 2.487196, Loss 1.400962, forward nfe 230500, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 142, Runtime 2.360631, Loss 1.457518, forward nfe 232370, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 143, Runtime 2.319021, Loss 1.540470, forward nfe 234288, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 144, Runtime 2.145712, Loss 1.383029, forward nfe 235888, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 145, Runtime 2.211516, Loss 1.357233, forward nfe 237734, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 146, Runtime 2.285791, Loss 1.394880, forward nfe 239646, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 147, Runtime 2.305657, Loss 1.386609, forward nfe 241564, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 148, Runtime 2.203851, Loss 1.361725, forward nfe 243410, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 149, Runtime 2.024705, Loss 1.315981, forward nfe 245124, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 150, Runtime 2.357434, Loss 1.399537, forward nfe 247090, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 151, Runtime 2.263428, Loss 1.393424, forward nfe 248936, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 152, Runtime 1.984066, Loss 1.473817, forward nfe 250614, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 153, Runtime 2.938891, Loss 1.424412, forward nfe 252862, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 154, Runtime 2.323317, Loss 1.514765, forward nfe 254798, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 155, Runtime 2.367097, Loss 1.359555, forward nfe 256764, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 156, Runtime 2.718706, Loss 1.331462, forward nfe 259000, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 157, Runtime 2.141312, Loss 1.367157, forward nfe 260798, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 158, Runtime 2.165692, Loss 1.400985, forward nfe 262572, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 159, Runtime 1.990704, Loss 1.335447, forward nfe 264262, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 160, Runtime 2.086480, Loss 1.358405, forward nfe 266030, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 161, Runtime 2.590714, Loss 1.585910, forward nfe 268170, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 162, Runtime 2.362590, Loss 1.604625, forward nfe 270142, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 163, Runtime 2.767707, Loss 1.421723, forward nfe 272408, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 164, Runtime 2.688199, Loss 1.409077, forward nfe 274554, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 165, Runtime 2.554959, Loss 1.490303, forward nfe 276664, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 166, Runtime 3.767617, Loss 1.416522, forward nfe 279608, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 167, Runtime 2.338936, Loss 1.489242, forward nfe 281556, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 168, Runtime 2.590023, Loss 1.407170, forward nfe 283666, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 169, Runtime 3.594325, Loss 1.439125, forward nfe 286328, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 170, Runtime 2.624147, Loss 1.519688, forward nfe 288456, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 171, Runtime 2.776850, Loss 1.326537, forward nfe 290704, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 172, Runtime 2.321098, Loss 1.442130, forward nfe 292598, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 173, Runtime 2.968039, Loss 1.493446, forward nfe 294990, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 174, Runtime 2.462744, Loss 1.485676, forward nfe 296992, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 175, Runtime 2.619907, Loss 1.560497, forward nfe 299072, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 176, Runtime 2.716285, Loss 1.394104, forward nfe 301074, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 177, Runtime 2.219033, Loss 1.492757, forward nfe 302896, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 178, Runtime 2.393763, Loss 1.430564, forward nfe 304856, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 179, Runtime 1.786027, Loss 1.360676, forward nfe 306378, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 180, Runtime 1.709005, Loss 1.320450, forward nfe 307840, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 181, Runtime 1.917755, Loss 1.197350, forward nfe 309434, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 182, Runtime 1.881784, Loss 1.425236, forward nfe 310950, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 183, Runtime 1.961027, Loss 1.310950, forward nfe 312574, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 184, Runtime 2.335685, Loss 1.352791, forward nfe 314492, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 185, Runtime 2.147074, Loss 1.361234, forward nfe 316260, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 186, Runtime 2.090569, Loss 1.423015, forward nfe 317986, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 187, Runtime 2.252021, Loss 1.415460, forward nfe 319826, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 188, Runtime 2.038810, Loss 1.486565, forward nfe 321504, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 189, Runtime 2.585960, Loss 1.460593, forward nfe 323596, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 190, Runtime 2.596789, Loss 1.468097, forward nfe 325646, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 191, Runtime 2.702889, Loss 1.470255, forward nfe 327822, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 192, Runtime 3.541912, Loss 1.368660, forward nfe 330628, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 193, Runtime 2.522498, Loss 1.336430, forward nfe 332648, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 194, Runtime 2.937321, Loss 1.461359, forward nfe 334902, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 195, Runtime 2.705392, Loss 1.370424, forward nfe 337006, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 196, Runtime 2.414170, Loss 1.435045, forward nfe 338972, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 197, Runtime 2.322928, Loss 1.335040, forward nfe 340866, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 198, Runtime 2.266157, Loss 1.347626, forward nfe 342730, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 199, Runtime 2.090844, Loss 1.316670, forward nfe 344450, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 200, Runtime 1.802513, Loss 1.383224, forward nfe 345984, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 201, Runtime 2.121851, Loss 1.456459, forward nfe 347740, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 202, Runtime 2.548258, Loss 1.319833, forward nfe 349676, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 203, Runtime 2.208278, Loss 1.371231, forward nfe 351510, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 204, Runtime 2.774750, Loss 1.388523, forward nfe 353752, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 205, Runtime 2.137470, Loss 1.434313, forward nfe 355520, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 206, Runtime 2.458460, Loss 1.503243, forward nfe 357528, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 207, Runtime 2.828874, Loss 1.343403, forward nfe 359806, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 208, Runtime 2.850419, Loss 1.508614, forward nfe 362042, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 209, Runtime 2.457651, Loss 1.342610, forward nfe 364038, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 210, Runtime 2.784035, Loss 1.520768, forward nfe 366274, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 211, Runtime 2.438092, Loss 1.426044, forward nfe 368186, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 212, Runtime 2.581503, Loss 1.378043, forward nfe 370278, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 213, Runtime 2.592879, Loss 1.611454, forward nfe 372376, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 214, Runtime 2.162433, Loss 1.423839, forward nfe 374096, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 215, Runtime 2.358549, Loss 1.494240, forward nfe 376032, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 216, Runtime 2.535114, Loss 1.305653, forward nfe 378088, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 217, Runtime 2.316761, Loss 1.415897, forward nfe 379988, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 218, Runtime 2.575695, Loss 1.265085, forward nfe 381984, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 219, Runtime 2.100439, Loss 1.463276, forward nfe 383656, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 220, Runtime 2.409068, Loss 1.443011, forward nfe 385622, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 221, Runtime 2.825774, Loss 1.361512, forward nfe 387834, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 222, Runtime 2.360138, Loss 1.426550, forward nfe 389770, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 223, Runtime 2.254789, Loss 1.308510, forward nfe 391616, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 224, Runtime 2.148367, Loss 1.423117, forward nfe 393378, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 225, Runtime 2.098881, Loss 1.469512, forward nfe 395116, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 226, Runtime 2.462480, Loss 1.512956, forward nfe 397052, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 227, Runtime 3.265815, Loss 1.621825, forward nfe 399564, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Epoch: 228, Runtime 3.467526, Loss 1.771273, forward nfe 402322, backward nfe 0, Train: 0.7250, Val: 0.7275, Test: 0.7500, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 40, in forward
    raise MaxNFEException
utils.MaxNFEException
best val accuracy 0.727536 with test accuracy 0.750000 at epoch 10 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #1...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.092292, Loss 1.794971, forward nfe 356, backward nfe 0, Train: 0.2667, Val: 0.2703, Test: 0.2855, Best time: 287.6607
Epoch: 002, Runtime 1.037299, Loss 1.774477, forward nfe 1314, backward nfe 0, Train: 0.4250, Val: 0.3558, Test: 0.3806, Best time: 229.9930
Epoch: 003, Runtime 1.093138, Loss 1.729766, forward nfe 2314, backward nfe 0, Train: 0.5333, Val: 0.5203, Test: 0.5113, Best time: 96.4665
Epoch: 004, Runtime 1.232626, Loss 1.690687, forward nfe 3392, backward nfe 0, Train: 0.6417, Val: 0.6906, Test: 0.6355, Best time: 179.0331
Epoch: 005, Runtime 1.183707, Loss 1.595057, forward nfe 4464, backward nfe 0, Train: 0.6417, Val: 0.6906, Test: 0.6355, Best time: 128.0000
Epoch: 006, Runtime 1.185378, Loss 1.560427, forward nfe 5530, backward nfe 0, Train: 0.6500, Val: 0.7145, Test: 0.6677, Best time: 112.8221
Epoch: 007, Runtime 1.362953, Loss 1.551747, forward nfe 6650, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 57.7120
Epoch: 008, Runtime 1.188855, Loss 1.558661, forward nfe 7722, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 128.0000
Epoch: 009, Runtime 1.179732, Loss 1.513653, forward nfe 8788, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 128.0000
Epoch: 010, Runtime 1.195347, Loss 1.522704, forward nfe 9866, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 128.0000
Epoch: 011, Runtime 1.187324, Loss 1.549171, forward nfe 10938, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 128.0000
Epoch: 012, Runtime 1.192830, Loss 1.576103, forward nfe 12010, backward nfe 0, Train: 0.7167, Val: 0.7304, Test: 0.6871, Best time: 128.0000
Epoch: 013, Runtime 1.187824, Loss 1.550497, forward nfe 13082, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 95.4999
Epoch: 014, Runtime 1.166360, Loss 1.515712, forward nfe 14136, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 015, Runtime 1.271322, Loss 1.527364, forward nfe 15268, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 016, Runtime 1.749865, Loss 1.434140, forward nfe 16742, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 017, Runtime 1.241175, Loss 1.411728, forward nfe 17856, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 018, Runtime 1.277705, Loss 1.442623, forward nfe 18994, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 019, Runtime 1.460488, Loss 1.355770, forward nfe 20246, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 020, Runtime 1.647789, Loss 1.399595, forward nfe 21660, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 021, Runtime 1.265902, Loss 1.489498, forward nfe 22792, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 022, Runtime 1.583805, Loss 1.455182, forward nfe 24140, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 023, Runtime 1.492879, Loss 1.518885, forward nfe 25440, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 024, Runtime 1.684797, Loss 1.548398, forward nfe 26860, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 025, Runtime 1.769810, Loss 1.395218, forward nfe 28190, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 026, Runtime 1.542297, Loss 1.371790, forward nfe 29466, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 027, Runtime 1.561947, Loss 1.426166, forward nfe 30790, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 028, Runtime 1.567923, Loss 1.618082, forward nfe 32144, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 029, Runtime 2.199017, Loss 1.446130, forward nfe 33942, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 030, Runtime 1.866320, Loss 1.410989, forward nfe 35536, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 031, Runtime 1.561463, Loss 1.579801, forward nfe 36908, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 032, Runtime 1.690166, Loss 1.455507, forward nfe 38394, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 033, Runtime 2.199797, Loss 1.543151, forward nfe 40228, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 034, Runtime 2.221254, Loss 1.480282, forward nfe 42092, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 035, Runtime 2.161313, Loss 1.560260, forward nfe 43740, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 036, Runtime 2.147416, Loss 1.450001, forward nfe 45508, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 037, Runtime 1.573254, Loss 1.453934, forward nfe 46904, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 038, Runtime 1.453541, Loss 1.419894, forward nfe 48198, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 039, Runtime 1.614675, Loss 1.476452, forward nfe 49618, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 040, Runtime 1.461249, Loss 1.502942, forward nfe 50924, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 041, Runtime 1.835166, Loss 1.503180, forward nfe 52488, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 042, Runtime 1.621330, Loss 1.537575, forward nfe 53914, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 043, Runtime 1.521084, Loss 1.514044, forward nfe 55262, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 044, Runtime 2.266860, Loss 1.569658, forward nfe 57156, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 045, Runtime 1.593578, Loss 1.464280, forward nfe 58564, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 046, Runtime 1.698300, Loss 1.522951, forward nfe 60020, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 047, Runtime 1.699645, Loss 1.529886, forward nfe 61434, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 048, Runtime 1.697088, Loss 1.493150, forward nfe 62878, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 049, Runtime 1.667942, Loss 1.553282, forward nfe 64316, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 050, Runtime 1.544035, Loss 1.505908, forward nfe 65682, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 051, Runtime 1.604318, Loss 1.488690, forward nfe 67096, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 052, Runtime 1.447966, Loss 1.520929, forward nfe 68384, backward nfe 0, Train: 0.7000, Val: 0.7464, Test: 0.7048, Best time: 128.0000
Epoch: 053, Runtime 1.447882, Loss 1.454829, forward nfe 69678, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 90.0497
Epoch: 054, Runtime 1.457364, Loss 1.453648, forward nfe 70984, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 055, Runtime 1.367876, Loss 1.342486, forward nfe 72218, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 056, Runtime 1.389176, Loss 1.363797, forward nfe 73464, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 057, Runtime 1.772251, Loss 1.475438, forward nfe 74992, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 058, Runtime 1.627073, Loss 1.506018, forward nfe 76388, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 059, Runtime 1.438066, Loss 1.467545, forward nfe 77658, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 060, Runtime 1.530559, Loss 1.546992, forward nfe 78940, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 061, Runtime 1.979830, Loss 1.473050, forward nfe 80438, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 062, Runtime 2.597887, Loss 1.529884, forward nfe 82596, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 063, Runtime 2.135325, Loss 1.467348, forward nfe 84388, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 064, Runtime 1.931069, Loss 1.459529, forward nfe 86042, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 065, Runtime 2.072658, Loss 1.474881, forward nfe 87786, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 066, Runtime 2.169931, Loss 1.578795, forward nfe 89614, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 067, Runtime 1.986758, Loss 1.471241, forward nfe 91292, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 068, Runtime 1.931734, Loss 1.478336, forward nfe 92880, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 069, Runtime 1.831754, Loss 1.538690, forward nfe 94444, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 070, Runtime 2.051356, Loss 1.519165, forward nfe 96008, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 071, Runtime 1.865070, Loss 1.436850, forward nfe 97590, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 072, Runtime 1.862032, Loss 1.457798, forward nfe 99190, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 073, Runtime 1.663490, Loss 1.602341, forward nfe 100652, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 074, Runtime 1.631055, Loss 1.535167, forward nfe 102084, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 075, Runtime 1.753274, Loss 1.563740, forward nfe 103612, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 076, Runtime 1.904331, Loss 1.480244, forward nfe 105230, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 077, Runtime 1.989943, Loss 1.480508, forward nfe 106806, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 078, Runtime 1.726418, Loss 1.575456, forward nfe 108322, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 079, Runtime 1.852779, Loss 1.548641, forward nfe 109898, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 080, Runtime 1.983836, Loss 1.498774, forward nfe 111570, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 081, Runtime 1.901897, Loss 1.511950, forward nfe 113188, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 082, Runtime 2.139587, Loss 1.522219, forward nfe 114992, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 083, Runtime 1.857671, Loss 1.488799, forward nfe 116580, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 084, Runtime 1.929066, Loss 1.456297, forward nfe 118216, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 085, Runtime 1.813654, Loss 1.566500, forward nfe 119792, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 086, Runtime 1.894643, Loss 1.602809, forward nfe 121362, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 087, Runtime 2.289671, Loss 1.627369, forward nfe 123274, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 088, Runtime 2.113972, Loss 1.546033, forward nfe 125066, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 089, Runtime 2.709014, Loss 1.535232, forward nfe 127314, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 090, Runtime 2.968542, Loss 1.556574, forward nfe 129748, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 091, Runtime 2.370290, Loss 1.510645, forward nfe 131528, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 092, Runtime 2.109911, Loss 1.447237, forward nfe 133302, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 093, Runtime 1.848965, Loss 1.615884, forward nfe 134842, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 094, Runtime 1.805820, Loss 1.575322, forward nfe 136412, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 095, Runtime 1.585311, Loss 1.614974, forward nfe 137814, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 096, Runtime 1.467641, Loss 1.588824, forward nfe 139126, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 097, Runtime 1.981171, Loss 1.504884, forward nfe 140804, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 098, Runtime 1.606308, Loss 1.485630, forward nfe 142218, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 099, Runtime 2.276675, Loss 1.555506, forward nfe 144124, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 100, Runtime 2.291963, Loss 1.518662, forward nfe 145868, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 101, Runtime 2.595864, Loss 1.576290, forward nfe 148026, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 102, Runtime 1.935847, Loss 1.634681, forward nfe 149620, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 103, Runtime 1.946399, Loss 1.607228, forward nfe 151280, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 104, Runtime 1.747351, Loss 1.526732, forward nfe 152814, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 105, Runtime 2.074858, Loss 1.554832, forward nfe 154600, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 106, Runtime 2.134764, Loss 1.637878, forward nfe 156410, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 107, Runtime 2.243194, Loss 1.542506, forward nfe 158316, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 108, Runtime 1.821788, Loss 1.526287, forward nfe 159916, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 109, Runtime 1.819111, Loss 1.477124, forward nfe 161498, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 110, Runtime 2.136138, Loss 1.469043, forward nfe 163242, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 111, Runtime 1.803128, Loss 1.547538, forward nfe 164782, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 112, Runtime 2.077615, Loss 1.519220, forward nfe 166550, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 113, Runtime 1.722981, Loss 1.512100, forward nfe 168072, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 114, Runtime 2.107309, Loss 1.467016, forward nfe 169864, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 115, Runtime 1.881296, Loss 1.534240, forward nfe 171506, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 116, Runtime 2.420429, Loss 1.477283, forward nfe 173538, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 117, Runtime 1.764467, Loss 1.456867, forward nfe 175084, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 118, Runtime 1.898668, Loss 1.533156, forward nfe 176708, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 119, Runtime 2.026016, Loss 1.488297, forward nfe 178332, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 120, Runtime 1.692194, Loss 1.513896, forward nfe 179830, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 121, Runtime 1.623083, Loss 1.491033, forward nfe 181262, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 122, Runtime 1.942882, Loss 1.558061, forward nfe 182772, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 123, Runtime 1.821788, Loss 1.613216, forward nfe 184324, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 124, Runtime 1.632512, Loss 1.565436, forward nfe 185762, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 125, Runtime 2.197876, Loss 1.552991, forward nfe 187614, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 126, Runtime 1.789728, Loss 1.524249, forward nfe 189178, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 127, Runtime 1.924654, Loss 1.377243, forward nfe 190820, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 128, Runtime 2.719617, Loss 1.511377, forward nfe 193014, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 129, Runtime 1.767408, Loss 1.372268, forward nfe 194554, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 130, Runtime 2.267046, Loss 1.451569, forward nfe 196268, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 131, Runtime 1.642375, Loss 1.541500, forward nfe 197724, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 132, Runtime 1.859042, Loss 1.536465, forward nfe 199318, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 133, Runtime 1.860148, Loss 1.510343, forward nfe 200906, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 134, Runtime 1.716672, Loss 1.529177, forward nfe 202416, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 135, Runtime 1.638041, Loss 1.581667, forward nfe 203866, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 136, Runtime 1.873042, Loss 1.566289, forward nfe 205478, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 137, Runtime 1.944505, Loss 1.584065, forward nfe 207126, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 138, Runtime 2.080315, Loss 1.447484, forward nfe 208834, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 139, Runtime 1.736505, Loss 1.440413, forward nfe 210332, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 140, Runtime 1.956453, Loss 1.488158, forward nfe 211998, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 141, Runtime 1.902676, Loss 1.619003, forward nfe 213628, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 142, Runtime 1.999627, Loss 1.535665, forward nfe 215336, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 143, Runtime 1.853601, Loss 1.697476, forward nfe 216966, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 144, Runtime 2.057051, Loss 1.506028, forward nfe 218710, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 145, Runtime 2.931175, Loss 1.551551, forward nfe 221132, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 146, Runtime 2.458392, Loss 1.486282, forward nfe 223128, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 147, Runtime 2.183536, Loss 1.612098, forward nfe 224968, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 148, Runtime 2.460760, Loss 1.556648, forward nfe 227036, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 149, Runtime 2.187931, Loss 1.639493, forward nfe 228876, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 150, Runtime 2.288983, Loss 1.534189, forward nfe 230812, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 151, Runtime 2.380990, Loss 1.554806, forward nfe 232634, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 152, Runtime 2.727609, Loss 1.654646, forward nfe 234930, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 153, Runtime 2.817227, Loss 1.518139, forward nfe 237220, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 154, Runtime 2.380806, Loss 1.579608, forward nfe 239162, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 155, Runtime 2.685245, Loss 1.600203, forward nfe 241350, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 156, Runtime 2.796880, Loss 1.562083, forward nfe 243604, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 157, Runtime 2.220940, Loss 1.594959, forward nfe 245426, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 158, Runtime 2.439654, Loss 1.469325, forward nfe 247278, backward nfe 0, Train: 0.6917, Val: 0.7514, Test: 0.6839, Best time: 128.0000
Epoch: 159, Runtime 2.535182, Loss 1.528664, forward nfe 249250, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 78.1074
Epoch: 160, Runtime 1.972749, Loss 1.460851, forward nfe 250892, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 161, Runtime 1.720374, Loss 1.469467, forward nfe 252360, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 162, Runtime 1.660601, Loss 1.535602, forward nfe 253792, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 163, Runtime 2.013671, Loss 1.586657, forward nfe 255464, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 164, Runtime 2.073554, Loss 1.581640, forward nfe 257196, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 165, Runtime 2.015572, Loss 1.560398, forward nfe 258880, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 166, Runtime 2.270458, Loss 1.581899, forward nfe 260750, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 167, Runtime 2.467021, Loss 1.558466, forward nfe 262764, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 168, Runtime 1.781085, Loss 1.688205, forward nfe 264280, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 169, Runtime 2.119873, Loss 1.478851, forward nfe 265970, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 170, Runtime 1.957816, Loss 1.505708, forward nfe 267630, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 171, Runtime 2.006073, Loss 1.595206, forward nfe 269296, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 172, Runtime 1.855145, Loss 1.540638, forward nfe 270860, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 173, Runtime 2.116327, Loss 1.617848, forward nfe 272616, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 174, Runtime 2.251511, Loss 1.584656, forward nfe 274474, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 175, Runtime 2.370349, Loss 1.506247, forward nfe 276428, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 176, Runtime 2.397696, Loss 1.565813, forward nfe 278352, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 177, Runtime 2.786643, Loss 1.514600, forward nfe 280624, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 178, Runtime 2.020897, Loss 1.568773, forward nfe 282128, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 179, Runtime 1.685241, Loss 1.555059, forward nfe 283578, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 180, Runtime 2.232422, Loss 1.554461, forward nfe 285406, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 181, Runtime 1.833386, Loss 1.619229, forward nfe 286916, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 182, Runtime 1.584632, Loss 1.568574, forward nfe 288288, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 183, Runtime 1.784257, Loss 1.560086, forward nfe 289792, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 184, Runtime 1.746299, Loss 1.504842, forward nfe 291260, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 185, Runtime 2.414402, Loss 1.550684, forward nfe 293226, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 186, Runtime 2.158859, Loss 1.490876, forward nfe 294952, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 187, Runtime 2.485480, Loss 1.596711, forward nfe 296846, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 188, Runtime 2.161489, Loss 1.557899, forward nfe 298638, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 189, Runtime 2.479079, Loss 1.521287, forward nfe 300670, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 190, Runtime 2.119792, Loss 1.557268, forward nfe 302426, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 191, Runtime 2.397588, Loss 1.551526, forward nfe 304374, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 192, Runtime 1.948847, Loss 1.649468, forward nfe 305986, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 193, Runtime 1.936045, Loss 1.501837, forward nfe 307556, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 194, Runtime 2.219492, Loss 1.545263, forward nfe 309384, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 195, Runtime 2.487788, Loss 1.533633, forward nfe 311404, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 196, Runtime 2.426893, Loss 1.575235, forward nfe 313388, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 197, Runtime 2.065374, Loss 1.590696, forward nfe 315126, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 198, Runtime 2.297004, Loss 1.525566, forward nfe 317008, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 199, Runtime 2.149975, Loss 1.625604, forward nfe 318782, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 200, Runtime 2.249943, Loss 1.480026, forward nfe 320514, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 201, Runtime 2.335302, Loss 1.503510, forward nfe 322438, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 202, Runtime 2.155574, Loss 1.976646, forward nfe 324230, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 203, Runtime 2.035400, Loss 1.789032, forward nfe 325932, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 204, Runtime 2.226948, Loss 1.705709, forward nfe 327766, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 205, Runtime 3.523747, Loss 1.659164, forward nfe 330578, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 206, Runtime 3.721484, Loss 1.609217, forward nfe 333360, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 207, Runtime 2.597973, Loss 1.530476, forward nfe 335488, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 208, Runtime 3.013767, Loss 1.583882, forward nfe 337922, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 209, Runtime 2.376603, Loss 1.580692, forward nfe 339876, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 210, Runtime 2.662422, Loss 1.561257, forward nfe 342034, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 211, Runtime 2.077564, Loss 1.611902, forward nfe 343760, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 212, Runtime 2.616428, Loss 1.569919, forward nfe 345714, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 213, Runtime 2.051324, Loss 1.562122, forward nfe 347440, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 214, Runtime 2.204881, Loss 1.509335, forward nfe 349262, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 215, Runtime 2.296712, Loss 1.588870, forward nfe 351162, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 216, Runtime 2.641830, Loss 1.518048, forward nfe 353320, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 217, Runtime 1.596121, Loss 1.573708, forward nfe 354704, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 218, Runtime 1.686727, Loss 1.569081, forward nfe 356154, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 219, Runtime 1.824980, Loss 1.545076, forward nfe 357682, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 220, Runtime 1.552462, Loss 1.604026, forward nfe 359024, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 221, Runtime 1.739998, Loss 1.617043, forward nfe 360516, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 222, Runtime 2.247171, Loss 1.594050, forward nfe 362308, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 223, Runtime 2.459341, Loss 1.533916, forward nfe 364322, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 224, Runtime 2.155582, Loss 1.493438, forward nfe 366102, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 225, Runtime 1.853863, Loss 1.676882, forward nfe 367636, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 226, Runtime 2.128686, Loss 1.493359, forward nfe 369392, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 227, Runtime 1.735075, Loss 1.580036, forward nfe 370872, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 228, Runtime 2.363981, Loss 1.575111, forward nfe 372802, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 229, Runtime 2.315587, Loss 1.564654, forward nfe 374624, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 230, Runtime 2.170320, Loss 1.593763, forward nfe 376422, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 231, Runtime 2.491640, Loss 1.669634, forward nfe 378466, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 232, Runtime 2.307397, Loss 1.557515, forward nfe 380372, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 233, Runtime 2.875758, Loss 1.582336, forward nfe 382536, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 234, Runtime 2.395554, Loss 1.606586, forward nfe 384466, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 235, Runtime 3.283726, Loss 1.574066, forward nfe 386990, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 236, Runtime 3.241070, Loss 1.510881, forward nfe 389580, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 237, Runtime 2.208701, Loss 1.583158, forward nfe 391384, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 238, Runtime 2.574504, Loss 1.495310, forward nfe 393320, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 239, Runtime 2.434633, Loss 1.581398, forward nfe 395334, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 240, Runtime 1.919502, Loss 1.574612, forward nfe 396934, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 241, Runtime 1.782058, Loss 1.548359, forward nfe 398450, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 242, Runtime 1.742834, Loss 1.559061, forward nfe 399858, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 243, Runtime 1.587453, Loss 1.536341, forward nfe 401236, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 244, Runtime 2.382438, Loss 1.623737, forward nfe 403184, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 245, Runtime 2.291873, Loss 1.511514, forward nfe 405048, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 246, Runtime 2.283209, Loss 1.482946, forward nfe 406912, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 247, Runtime 2.554810, Loss 1.573827, forward nfe 408968, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 248, Runtime 2.527425, Loss 1.611257, forward nfe 411018, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
Epoch: 249, Runtime 2.653269, Loss 1.531328, forward nfe 413116, backward nfe 0, Train: 0.7500, Val: 0.7536, Test: 0.6952, Best time: 128.0000
best val accuracy 0.753623 with test accuracy 0.695161 at epoch 159 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #2...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.048522, Loss 1.793318, forward nfe 356, backward nfe 0, Train: 0.2083, Val: 0.3109, Test: 0.2919, Best time: 0.0411
Epoch: 002, Runtime 1.007393, Loss 1.795589, forward nfe 1290, backward nfe 0, Train: 0.4083, Val: 0.4304, Test: 0.4290, Best time: 0.1812
Epoch: 003, Runtime 0.999997, Loss 1.782042, forward nfe 2224, backward nfe 0, Train: 0.5333, Val: 0.5986, Test: 0.5968, Best time: 2.1719
Epoch: 004, Runtime 1.055629, Loss 1.759037, forward nfe 3170, backward nfe 0, Train: 0.6000, Val: 0.6587, Test: 0.6565, Best time: 8.0672
Epoch: 005, Runtime 1.105342, Loss 1.726746, forward nfe 4176, backward nfe 0, Train: 0.6000, Val: 0.6587, Test: 0.6565, Best time: 128.0000
Epoch: 006, Runtime 1.153602, Loss 1.671349, forward nfe 5218, backward nfe 0, Train: 0.6000, Val: 0.6587, Test: 0.6565, Best time: 128.0000
Epoch: 007, Runtime 1.154836, Loss 1.638174, forward nfe 6266, backward nfe 0, Train: 0.6000, Val: 0.6587, Test: 0.6565, Best time: 128.0000
Epoch: 008, Runtime 1.136698, Loss 1.586504, forward nfe 7296, backward nfe 0, Train: 0.6667, Val: 0.7051, Test: 0.7065, Best time: 65.5684
Epoch: 009, Runtime 1.129913, Loss 1.591540, forward nfe 8326, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 80.0125
Epoch: 010, Runtime 1.143642, Loss 1.532986, forward nfe 9362, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 128.0000
Epoch: 011, Runtime 1.166460, Loss 1.519623, forward nfe 10416, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 128.0000
Epoch: 012, Runtime 1.169512, Loss 1.475376, forward nfe 11452, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 128.0000
Epoch: 013, Runtime 1.368406, Loss 1.516607, forward nfe 12638, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 128.0000
Epoch: 014, Runtime 1.222486, Loss 1.434087, forward nfe 13740, backward nfe 0, Train: 0.6750, Val: 0.7101, Test: 0.7242, Best time: 128.0000
Epoch: 015, Runtime 1.221794, Loss 1.444151, forward nfe 14836, backward nfe 0, Train: 0.7417, Val: 0.7217, Test: 0.7226, Best time: 57.1530
Epoch: 016, Runtime 1.264291, Loss 1.266150, forward nfe 15968, backward nfe 0, Train: 0.7250, Val: 0.7261, Test: 0.7226, Best time: 55.1577
Epoch: 017, Runtime 1.500278, Loss 1.345664, forward nfe 17100, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 59.6371
Epoch: 018, Runtime 1.459680, Loss 1.317474, forward nfe 18382, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 019, Runtime 1.274610, Loss 1.259072, forward nfe 19514, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 020, Runtime 1.330307, Loss 1.241465, forward nfe 20694, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 021, Runtime 1.583451, Loss 1.283056, forward nfe 21976, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 022, Runtime 1.444170, Loss 1.281995, forward nfe 23240, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 023, Runtime 1.603274, Loss 1.231340, forward nfe 24600, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 024, Runtime 1.439688, Loss 1.252975, forward nfe 25864, backward nfe 0, Train: 0.7333, Val: 0.7348, Test: 0.7290, Best time: 128.0000
Epoch: 025, Runtime 1.751203, Loss 1.196134, forward nfe 27326, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 88.2711
Epoch: 026, Runtime 1.413337, Loss 1.261011, forward nfe 28560, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 027, Runtime 1.516963, Loss 1.239612, forward nfe 29884, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 028, Runtime 1.733540, Loss 1.229054, forward nfe 31190, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 029, Runtime 1.663885, Loss 1.390654, forward nfe 32622, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 030, Runtime 1.427876, Loss 1.241549, forward nfe 33868, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 031, Runtime 1.509381, Loss 1.221503, forward nfe 35156, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 032, Runtime 1.764924, Loss 1.290712, forward nfe 36642, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 033, Runtime 2.304208, Loss 1.333954, forward nfe 38536, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 034, Runtime 1.857298, Loss 1.200555, forward nfe 40088, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 035, Runtime 1.772059, Loss 1.267206, forward nfe 41508, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 036, Runtime 1.705122, Loss 1.303201, forward nfe 42922, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 037, Runtime 1.984069, Loss 1.209327, forward nfe 44522, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 038, Runtime 1.683289, Loss 1.198291, forward nfe 45936, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 039, Runtime 1.692459, Loss 1.271462, forward nfe 47368, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 040, Runtime 1.716779, Loss 1.309977, forward nfe 48836, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 041, Runtime 1.572151, Loss 1.201074, forward nfe 50172, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 042, Runtime 1.743155, Loss 1.248693, forward nfe 51664, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 043, Runtime 1.738230, Loss 1.323435, forward nfe 53132, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 044, Runtime 1.860179, Loss 1.325608, forward nfe 54672, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 045, Runtime 1.872057, Loss 1.286613, forward nfe 56242, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 046, Runtime 1.552906, Loss 1.205139, forward nfe 57584, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 047, Runtime 1.818971, Loss 1.260229, forward nfe 59112, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 048, Runtime 1.720776, Loss 1.293569, forward nfe 60532, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 049, Runtime 2.078795, Loss 1.289656, forward nfe 62258, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 050, Runtime 1.739864, Loss 1.326583, forward nfe 63750, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 051, Runtime 1.817827, Loss 1.249494, forward nfe 65278, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 052, Runtime 1.566618, Loss 1.283026, forward nfe 66638, backward nfe 0, Train: 0.7417, Val: 0.7377, Test: 0.7403, Best time: 128.0000
Epoch: 053, Runtime 1.979107, Loss 1.261801, forward nfe 68112, backward nfe 0, Train: 0.7583, Val: 0.7399, Test: 0.7548, Best time: 48.5110
Epoch: 054, Runtime 2.116663, Loss 1.247401, forward nfe 69886, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 48.7231
Epoch: 055, Runtime 1.619416, Loss 1.022795, forward nfe 71282, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 056, Runtime 1.763943, Loss 1.288429, forward nfe 72774, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 057, Runtime 2.044542, Loss 1.316157, forward nfe 74398, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 058, Runtime 1.715301, Loss 1.187142, forward nfe 75854, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 059, Runtime 1.677981, Loss 1.300996, forward nfe 77292, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 060, Runtime 1.742330, Loss 1.333498, forward nfe 78760, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 061, Runtime 1.686025, Loss 1.259622, forward nfe 80204, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 062, Runtime 2.664619, Loss 1.393071, forward nfe 82212, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 063, Runtime 2.256669, Loss 1.145052, forward nfe 84070, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 064, Runtime 2.423518, Loss 1.242768, forward nfe 86054, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 065, Runtime 1.916286, Loss 1.177391, forward nfe 87654, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 066, Runtime 2.326056, Loss 1.283115, forward nfe 89512, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 067, Runtime 1.901315, Loss 1.205514, forward nfe 91112, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 068, Runtime 1.918322, Loss 1.213534, forward nfe 92712, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 069, Runtime 2.079792, Loss 1.232939, forward nfe 94432, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 070, Runtime 1.884123, Loss 1.323573, forward nfe 96008, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 071, Runtime 1.881061, Loss 1.496959, forward nfe 97602, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 072, Runtime 1.796078, Loss 1.237280, forward nfe 99136, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 073, Runtime 2.078548, Loss 1.313276, forward nfe 100856, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 074, Runtime 2.035740, Loss 1.402010, forward nfe 102504, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 075, Runtime 2.346023, Loss 1.431868, forward nfe 104422, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 076, Runtime 2.747252, Loss 1.630049, forward nfe 106652, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 077, Runtime 2.528765, Loss 1.398361, forward nfe 108696, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 078, Runtime 2.234659, Loss 1.328201, forward nfe 110512, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 079, Runtime 2.405236, Loss 1.449472, forward nfe 112478, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 080, Runtime 2.507721, Loss 1.430137, forward nfe 114408, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 081, Runtime 2.442334, Loss 1.340131, forward nfe 116404, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 082, Runtime 2.668737, Loss 1.481129, forward nfe 118412, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 083, Runtime 2.440552, Loss 1.405442, forward nfe 120402, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 084, Runtime 2.257034, Loss 1.467584, forward nfe 122254, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 085, Runtime 1.927346, Loss 1.345321, forward nfe 123890, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 086, Runtime 2.364756, Loss 1.361774, forward nfe 125838, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 087, Runtime 2.309867, Loss 1.420254, forward nfe 127678, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 088, Runtime 2.733808, Loss 1.501755, forward nfe 129704, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 089, Runtime 2.274974, Loss 1.452438, forward nfe 131580, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 090, Runtime 2.233526, Loss 1.413446, forward nfe 133426, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 091, Runtime 2.227460, Loss 1.429469, forward nfe 135272, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 092, Runtime 2.007833, Loss 1.355308, forward nfe 136944, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 093, Runtime 1.895686, Loss 1.220194, forward nfe 138532, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 094, Runtime 1.926971, Loss 1.392841, forward nfe 140120, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 095, Runtime 2.271013, Loss 1.372087, forward nfe 141996, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 096, Runtime 1.867729, Loss 1.315658, forward nfe 143554, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 097, Runtime 2.041219, Loss 1.432336, forward nfe 145256, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 098, Runtime 2.457306, Loss 1.393632, forward nfe 147264, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 099, Runtime 2.764817, Loss 1.248876, forward nfe 149512, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 100, Runtime 2.506656, Loss 1.367825, forward nfe 151556, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 101, Runtime 2.273540, Loss 1.293724, forward nfe 153378, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 102, Runtime 2.557474, Loss 1.372025, forward nfe 155464, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 103, Runtime 1.993010, Loss 1.332585, forward nfe 157118, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 104, Runtime 2.077758, Loss 1.430755, forward nfe 158844, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 105, Runtime 1.721865, Loss 1.391762, forward nfe 160324, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 106, Runtime 1.675870, Loss 1.456840, forward nfe 161756, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 107, Runtime 1.636172, Loss 1.276602, forward nfe 163170, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 108, Runtime 1.622681, Loss 1.423778, forward nfe 164578, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 109, Runtime 1.680121, Loss 1.416794, forward nfe 166028, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 110, Runtime 2.009718, Loss 1.335132, forward nfe 167544, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 111, Runtime 1.779956, Loss 1.300947, forward nfe 169060, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 112, Runtime 1.865374, Loss 1.402468, forward nfe 170552, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 113, Runtime 1.767009, Loss 1.320174, forward nfe 172038, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 114, Runtime 1.809963, Loss 1.340048, forward nfe 173560, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 115, Runtime 1.999332, Loss 1.339630, forward nfe 175214, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 116, Runtime 1.985458, Loss 1.433771, forward nfe 176868, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 117, Runtime 2.053798, Loss 1.371293, forward nfe 178570, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 118, Runtime 2.379267, Loss 1.364471, forward nfe 180350, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 119, Runtime 2.450241, Loss 1.328730, forward nfe 182286, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 120, Runtime 2.655430, Loss 1.229262, forward nfe 184390, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 121, Runtime 1.897584, Loss 1.253831, forward nfe 185972, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 122, Runtime 1.663461, Loss 1.348603, forward nfe 187398, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 123, Runtime 1.707286, Loss 1.315273, forward nfe 188860, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 124, Runtime 2.019850, Loss 1.233327, forward nfe 190544, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 125, Runtime 1.840115, Loss 1.390864, forward nfe 192120, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 126, Runtime 2.673084, Loss 1.291219, forward nfe 194284, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 127, Runtime 1.875214, Loss 1.256455, forward nfe 195848, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 128, Runtime 1.874464, Loss 1.292757, forward nfe 197424, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 129, Runtime 1.939750, Loss 1.240772, forward nfe 198988, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 130, Runtime 1.839104, Loss 1.389369, forward nfe 200528, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 131, Runtime 1.752439, Loss 1.361308, forward nfe 202026, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 132, Runtime 2.123368, Loss 1.352182, forward nfe 203782, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 133, Runtime 1.984670, Loss 1.366500, forward nfe 205436, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 134, Runtime 1.846039, Loss 1.405887, forward nfe 206988, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 135, Runtime 2.241180, Loss 1.179905, forward nfe 208840, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 136, Runtime 1.967014, Loss 1.314989, forward nfe 210476, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 137, Runtime 1.889412, Loss 1.511974, forward nfe 212064, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 138, Runtime 2.083033, Loss 1.295809, forward nfe 213784, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 139, Runtime 2.658398, Loss 1.346876, forward nfe 215894, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 140, Runtime 2.205390, Loss 1.297137, forward nfe 217524, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 141, Runtime 2.795435, Loss 1.385684, forward nfe 219772, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 142, Runtime 2.308069, Loss 1.177675, forward nfe 221666, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 143, Runtime 2.129314, Loss 1.346145, forward nfe 223416, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 144, Runtime 2.420157, Loss 1.352200, forward nfe 225394, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 145, Runtime 2.245631, Loss 1.381927, forward nfe 227234, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 146, Runtime 2.691795, Loss 1.417037, forward nfe 229170, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 147, Runtime 2.158069, Loss 1.316596, forward nfe 230950, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 148, Runtime 2.069751, Loss 1.283816, forward nfe 232658, backward nfe 0, Train: 0.7333, Val: 0.7420, Test: 0.7484, Best time: 128.0000
Epoch: 149, Runtime 2.060705, Loss 1.286805, forward nfe 234372, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 36.9821
Epoch: 150, Runtime 2.350669, Loss 1.444556, forward nfe 236302, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 151, Runtime 2.263430, Loss 1.294754, forward nfe 238154, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 152, Runtime 2.135469, Loss 1.463928, forward nfe 239916, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 153, Runtime 2.093895, Loss 1.271960, forward nfe 241600, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 154, Runtime 2.255443, Loss 1.338060, forward nfe 243452, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 155, Runtime 3.180911, Loss 1.328626, forward nfe 245994, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 156, Runtime 2.489483, Loss 1.379219, forward nfe 248014, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 157, Runtime 2.708887, Loss 1.311655, forward nfe 250202, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 158, Runtime 2.173558, Loss 1.326763, forward nfe 251976, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 159, Runtime 2.903970, Loss 1.296434, forward nfe 254266, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 160, Runtime 2.455105, Loss 1.310900, forward nfe 256262, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 161, Runtime 2.774559, Loss 1.148736, forward nfe 258516, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 162, Runtime 1.853407, Loss 1.335013, forward nfe 260080, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 163, Runtime 2.403985, Loss 1.400927, forward nfe 262046, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 164, Runtime 1.895154, Loss 1.334369, forward nfe 263646, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 165, Runtime 2.558355, Loss 1.403901, forward nfe 265498, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 166, Runtime 2.769209, Loss 1.476468, forward nfe 267716, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 167, Runtime 3.675416, Loss 1.246924, forward nfe 270558, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 168, Runtime 3.583300, Loss 1.378763, forward nfe 273412, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 169, Runtime 2.897499, Loss 1.363643, forward nfe 275756, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 170, Runtime 3.230513, Loss 1.341003, forward nfe 278100, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 171, Runtime 2.303807, Loss 1.482761, forward nfe 279982, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 172, Runtime 2.453787, Loss 1.389527, forward nfe 281984, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 173, Runtime 2.281617, Loss 1.358658, forward nfe 283860, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 174, Runtime 2.120755, Loss 1.401182, forward nfe 285616, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 175, Runtime 2.342557, Loss 1.375322, forward nfe 287540, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 176, Runtime 1.975925, Loss 1.279732, forward nfe 289188, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 177, Runtime 2.550687, Loss 1.378926, forward nfe 291208, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 178, Runtime 1.983228, Loss 1.394126, forward nfe 292862, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 179, Runtime 2.621485, Loss 1.317552, forward nfe 295002, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 180, Runtime 2.766051, Loss 1.287591, forward nfe 297238, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 181, Runtime 2.203317, Loss 1.330765, forward nfe 299048, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 182, Runtime 1.955478, Loss 1.241707, forward nfe 300678, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 183, Runtime 1.948537, Loss 1.327587, forward nfe 302296, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 184, Runtime 1.863790, Loss 1.233795, forward nfe 303800, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 185, Runtime 2.016651, Loss 1.326785, forward nfe 305472, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 186, Runtime 1.955425, Loss 1.292136, forward nfe 307108, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 187, Runtime 2.342793, Loss 1.288385, forward nfe 309044, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 188, Runtime 2.453109, Loss 1.191859, forward nfe 311052, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 189, Runtime 2.245678, Loss 1.244106, forward nfe 312898, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 190, Runtime 2.519058, Loss 1.266930, forward nfe 314792, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 191, Runtime 2.169850, Loss 1.208613, forward nfe 316572, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 192, Runtime 2.419978, Loss 1.372825, forward nfe 318502, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 193, Runtime 2.340252, Loss 1.309990, forward nfe 320426, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 194, Runtime 2.291061, Loss 1.294454, forward nfe 322302, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 195, Runtime 2.073688, Loss 1.354346, forward nfe 324010, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 196, Runtime 2.806721, Loss 1.268783, forward nfe 326108, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 197, Runtime 2.426729, Loss 1.286557, forward nfe 328080, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 198, Runtime 3.344861, Loss 1.402237, forward nfe 330694, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 199, Runtime 2.713735, Loss 1.369574, forward nfe 332888, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 200, Runtime 2.545379, Loss 1.321032, forward nfe 334950, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 201, Runtime 2.838084, Loss 1.354343, forward nfe 337240, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 202, Runtime 2.758734, Loss 1.336449, forward nfe 339464, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 203, Runtime 2.694129, Loss 1.441218, forward nfe 341658, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 204, Runtime 2.691830, Loss 1.274463, forward nfe 343774, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 205, Runtime 2.512438, Loss 1.325984, forward nfe 345812, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 206, Runtime 1.996823, Loss 1.284311, forward nfe 347472, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 207, Runtime 2.399725, Loss 1.304182, forward nfe 349408, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 208, Runtime 1.894842, Loss 1.324276, forward nfe 350996, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 209, Runtime 2.592264, Loss 1.332994, forward nfe 353112, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 210, Runtime 2.340123, Loss 1.472383, forward nfe 355030, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 211, Runtime 2.092246, Loss 1.377513, forward nfe 356642, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 212, Runtime 2.114929, Loss 1.361404, forward nfe 358398, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 213, Runtime 2.540191, Loss 1.305097, forward nfe 360484, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 214, Runtime 3.056401, Loss 1.361004, forward nfe 362774, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 215, Runtime 2.526500, Loss 1.267933, forward nfe 364830, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 216, Runtime 2.663173, Loss 1.347153, forward nfe 367000, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 217, Runtime 2.609237, Loss 1.363950, forward nfe 369086, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 218, Runtime 2.205503, Loss 1.331348, forward nfe 370908, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 219, Runtime 2.023900, Loss 1.235167, forward nfe 372592, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 220, Runtime 2.526835, Loss 1.283643, forward nfe 374510, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 221, Runtime 2.597988, Loss 1.344001, forward nfe 376638, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 222, Runtime 2.765821, Loss 1.309263, forward nfe 378874, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 223, Runtime 2.473087, Loss 1.436471, forward nfe 380852, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 224, Runtime 2.664325, Loss 1.321579, forward nfe 383028, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 225, Runtime 2.442183, Loss 1.359326, forward nfe 385036, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 226, Runtime 3.289809, Loss 1.326715, forward nfe 387698, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 227, Runtime 2.566304, Loss 1.429140, forward nfe 389814, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 228, Runtime 2.619749, Loss 1.390029, forward nfe 391888, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 229, Runtime 2.512694, Loss 1.358742, forward nfe 393944, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 230, Runtime 2.266719, Loss 1.363087, forward nfe 395820, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 231, Runtime 2.667248, Loss 1.193014, forward nfe 397990, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 232, Runtime 2.498373, Loss 1.252516, forward nfe 400040, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 233, Runtime 2.000443, Loss 1.104610, forward nfe 401706, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 234, Runtime 2.671175, Loss 1.340118, forward nfe 403882, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 235, Runtime 2.048435, Loss 1.323364, forward nfe 405536, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 236, Runtime 2.166824, Loss 1.360741, forward nfe 407358, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 237, Runtime 2.398184, Loss 1.446636, forward nfe 409354, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 238, Runtime 2.161664, Loss 1.412595, forward nfe 411170, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 239, Runtime 2.968604, Loss 1.696482, forward nfe 413436, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Epoch: 240, Runtime 2.769225, Loss 1.423259, forward nfe 415702, backward nfe 0, Train: 0.7583, Val: 0.7442, Test: 0.7355, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 40, in forward
    raise MaxNFEException
utils.MaxNFEException
best val accuracy 0.744203 with test accuracy 0.735484 at epoch 149 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #3...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.050430, Loss 1.793290, forward nfe 344, backward nfe 0, Train: 0.2750, Val: 0.2804, Test: 0.2903, Best time: 5.2715
Epoch: 002, Runtime 1.022674, Loss 1.796632, forward nfe 1290, backward nfe 0, Train: 0.2750, Val: 0.2804, Test: 0.2903, Best time: 128.0000
Epoch: 003, Runtime 0.991437, Loss 1.787984, forward nfe 2230, backward nfe 0, Train: 0.2667, Val: 0.2862, Test: 0.2919, Best time: 0.0376
Epoch: 004, Runtime 1.019826, Loss 1.764391, forward nfe 3188, backward nfe 0, Train: 0.3250, Val: 0.3167, Test: 0.3177, Best time: 0.0378
Epoch: 005, Runtime 1.303994, Loss 1.737934, forward nfe 4182, backward nfe 0, Train: 0.3667, Val: 0.3623, Test: 0.3742, Best time: 0.0386
Epoch: 006, Runtime 1.080311, Loss 1.741566, forward nfe 5188, backward nfe 0, Train: 0.4833, Val: 0.4638, Test: 0.4532, Best time: 0.3463
Epoch: 007, Runtime 1.086839, Loss 1.714378, forward nfe 6200, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 7.9339
Epoch: 008, Runtime 1.092162, Loss 1.655781, forward nfe 7218, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 128.0000
Epoch: 009, Runtime 1.150940, Loss 1.649169, forward nfe 8230, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 128.0000
Epoch: 010, Runtime 1.130501, Loss 1.644384, forward nfe 9260, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 128.0000
Epoch: 011, Runtime 1.123570, Loss 1.568956, forward nfe 10296, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 128.0000
Epoch: 012, Runtime 1.155201, Loss 1.560459, forward nfe 11344, backward nfe 0, Train: 0.6917, Val: 0.6623, Test: 0.6758, Best time: 128.0000
Epoch: 013, Runtime 1.191544, Loss 1.544001, forward nfe 12428, backward nfe 0, Train: 0.6917, Val: 0.6920, Test: 0.6887, Best time: 33.4252
Epoch: 014, Runtime 1.190571, Loss 1.527371, forward nfe 13512, backward nfe 0, Train: 0.6917, Val: 0.6920, Test: 0.6887, Best time: 128.0000
Epoch: 015, Runtime 1.187352, Loss 1.515504, forward nfe 14602, backward nfe 0, Train: 0.6917, Val: 0.6928, Test: 0.7032, Best time: 139.1315
Epoch: 016, Runtime 1.207916, Loss 1.469595, forward nfe 15698, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 118.6572
Epoch: 017, Runtime 1.242863, Loss 1.408191, forward nfe 16818, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 018, Runtime 1.234438, Loss 1.421656, forward nfe 17938, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 019, Runtime 1.311323, Loss 1.369300, forward nfe 19088, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 020, Runtime 1.416678, Loss 1.329319, forward nfe 20340, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 021, Runtime 1.416600, Loss 1.304065, forward nfe 21592, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 022, Runtime 1.430873, Loss 1.329643, forward nfe 22808, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 023, Runtime 1.636168, Loss 1.449109, forward nfe 24204, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 024, Runtime 1.414111, Loss 1.396328, forward nfe 25462, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 025, Runtime 1.385434, Loss 1.368634, forward nfe 26696, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 026, Runtime 1.673702, Loss 1.363465, forward nfe 28128, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 027, Runtime 1.575993, Loss 1.325722, forward nfe 29506, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 028, Runtime 1.678367, Loss 1.257313, forward nfe 30932, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 029, Runtime 1.554217, Loss 1.338484, forward nfe 32298, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 030, Runtime 1.706850, Loss 1.207741, forward nfe 33790, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 031, Runtime 1.698876, Loss 1.286273, forward nfe 35264, backward nfe 0, Train: 0.6750, Val: 0.7043, Test: 0.7016, Best time: 128.0000
Epoch: 032, Runtime 1.862294, Loss 1.267158, forward nfe 36846, backward nfe 0, Train: 0.7417, Val: 0.7065, Test: 0.7065, Best time: 49.2752
Epoch: 033, Runtime 1.725734, Loss 1.351419, forward nfe 38260, backward nfe 0, Train: 0.7333, Val: 0.7094, Test: 0.7097, Best time: 99.7202
Epoch: 034, Runtime 1.823857, Loss 1.394396, forward nfe 39806, backward nfe 0, Train: 0.7333, Val: 0.7094, Test: 0.7097, Best time: 128.0000
Epoch: 035, Runtime 1.716940, Loss 1.332245, forward nfe 41274, backward nfe 0, Train: 0.7333, Val: 0.7094, Test: 0.7097, Best time: 128.0000
Epoch: 036, Runtime 1.640557, Loss 1.414703, forward nfe 42712, backward nfe 0, Train: 0.7333, Val: 0.7094, Test: 0.7097, Best time: 128.0000
Epoch: 037, Runtime 1.592985, Loss 1.349104, forward nfe 44108, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 49.3500
Epoch: 038, Runtime 1.497083, Loss 1.314487, forward nfe 45426, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 039, Runtime 1.678112, Loss 1.347384, forward nfe 46786, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 040, Runtime 1.646959, Loss 1.387855, forward nfe 48212, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 041, Runtime 1.533859, Loss 1.342699, forward nfe 49566, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 042, Runtime 1.557895, Loss 1.335066, forward nfe 50932, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 043, Runtime 1.498021, Loss 1.315215, forward nfe 52250, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 044, Runtime 1.813302, Loss 1.317955, forward nfe 53790, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 045, Runtime 2.263157, Loss 1.336629, forward nfe 55624, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 046, Runtime 1.876711, Loss 1.272529, forward nfe 57188, backward nfe 0, Train: 0.7000, Val: 0.7123, Test: 0.7081, Best time: 128.0000
Epoch: 047, Runtime 1.753553, Loss 1.398640, forward nfe 58704, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 40.0918
Epoch: 048, Runtime 1.791924, Loss 1.312015, forward nfe 60232, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 128.0000
Epoch: 049, Runtime 1.867001, Loss 1.255084, forward nfe 61814, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 128.0000
Epoch: 050, Runtime 2.204757, Loss 1.311294, forward nfe 63660, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 128.0000
Epoch: 051, Runtime 1.843003, Loss 1.337921, forward nfe 65248, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 128.0000
Epoch: 052, Runtime 1.873726, Loss 1.265723, forward nfe 66866, backward nfe 0, Train: 0.6750, Val: 0.7203, Test: 0.7177, Best time: 128.0000
Epoch: 053, Runtime 1.748820, Loss 1.341249, forward nfe 68370, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 76.8950
Epoch: 054, Runtime 1.741465, Loss 1.355703, forward nfe 69880, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 055, Runtime 1.927227, Loss 1.302248, forward nfe 71468, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 056, Runtime 1.936487, Loss 1.228602, forward nfe 73098, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 057, Runtime 1.737442, Loss 1.338894, forward nfe 74614, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 058, Runtime 1.837274, Loss 1.237855, forward nfe 76148, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 059, Runtime 1.701343, Loss 1.329922, forward nfe 77622, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 060, Runtime 1.708033, Loss 1.486789, forward nfe 79072, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 061, Runtime 1.542060, Loss 1.299288, forward nfe 80426, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 062, Runtime 2.156761, Loss 1.460553, forward nfe 82224, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 063, Runtime 1.688075, Loss 1.360300, forward nfe 83692, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 064, Runtime 1.888847, Loss 1.412224, forward nfe 85226, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 065, Runtime 1.910158, Loss 1.394925, forward nfe 86832, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 066, Runtime 2.089552, Loss 1.311951, forward nfe 88588, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 067, Runtime 1.932146, Loss 1.321993, forward nfe 90176, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 068, Runtime 1.805451, Loss 1.297341, forward nfe 91734, backward nfe 0, Train: 0.6917, Val: 0.7254, Test: 0.7113, Best time: 128.0000
Epoch: 069, Runtime 1.799492, Loss 1.365720, forward nfe 93262, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 36.8674
Epoch: 070, Runtime 1.865196, Loss 1.281558, forward nfe 94838, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 071, Runtime 1.861768, Loss 1.437673, forward nfe 96414, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 072, Runtime 2.253184, Loss 1.466701, forward nfe 98200, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 073, Runtime 1.722178, Loss 1.277258, forward nfe 99668, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 074, Runtime 1.724852, Loss 1.550381, forward nfe 101136, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 075, Runtime 2.270484, Loss 1.373972, forward nfe 103030, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 076, Runtime 2.380818, Loss 1.428100, forward nfe 104996, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 077, Runtime 2.602444, Loss 1.280434, forward nfe 107088, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 078, Runtime 2.450364, Loss 1.482694, forward nfe 109108, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 079, Runtime 2.271364, Loss 1.423756, forward nfe 110984, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 080, Runtime 2.336996, Loss 1.368738, forward nfe 112878, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 081, Runtime 1.972946, Loss 1.565760, forward nfe 114568, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 082, Runtime 1.706267, Loss 1.473309, forward nfe 116054, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 083, Runtime 1.844918, Loss 1.358336, forward nfe 117648, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 084, Runtime 2.036657, Loss 1.504425, forward nfe 119368, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 085, Runtime 1.774033, Loss 1.400887, forward nfe 120896, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 086, Runtime 1.840179, Loss 1.411448, forward nfe 122460, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 087, Runtime 1.689476, Loss 1.255962, forward nfe 123928, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 088, Runtime 1.845090, Loss 1.328510, forward nfe 125444, backward nfe 0, Train: 0.7000, Val: 0.7362, Test: 0.7242, Best time: 128.0000
Epoch: 089, Runtime 1.771854, Loss 1.337850, forward nfe 126972, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 16.8580
Epoch: 090, Runtime 1.919198, Loss 1.382438, forward nfe 128620, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 091, Runtime 1.782057, Loss 1.467459, forward nfe 130160, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 092, Runtime 1.656198, Loss 1.347133, forward nfe 131586, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 093, Runtime 2.193940, Loss 1.365026, forward nfe 133402, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 094, Runtime 2.292353, Loss 1.471521, forward nfe 135302, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 095, Runtime 2.152771, Loss 1.411525, forward nfe 137112, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 096, Runtime 1.995020, Loss 1.272922, forward nfe 138820, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 097, Runtime 1.887373, Loss 1.371372, forward nfe 140348, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 098, Runtime 1.872410, Loss 1.392848, forward nfe 141942, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 099, Runtime 1.987904, Loss 1.389373, forward nfe 143632, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 100, Runtime 2.261891, Loss 1.640943, forward nfe 145520, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 101, Runtime 1.961350, Loss 1.420421, forward nfe 147180, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 102, Runtime 1.980909, Loss 1.367280, forward nfe 148774, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 103, Runtime 2.190802, Loss 1.615410, forward nfe 150596, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 104, Runtime 2.182792, Loss 1.360729, forward nfe 152418, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 105, Runtime 2.410896, Loss 1.511637, forward nfe 154408, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 106, Runtime 2.443613, Loss 1.498611, forward nfe 156368, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 107, Runtime 2.773976, Loss 1.399009, forward nfe 158568, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 108, Runtime 1.888024, Loss 1.334854, forward nfe 160090, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 109, Runtime 1.949787, Loss 1.452048, forward nfe 161750, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 110, Runtime 2.114235, Loss 1.404630, forward nfe 163548, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 111, Runtime 1.818228, Loss 1.386590, forward nfe 165082, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 112, Runtime 2.260048, Loss 1.392907, forward nfe 166970, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 113, Runtime 1.850855, Loss 1.403328, forward nfe 168552, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 114, Runtime 2.205524, Loss 1.312034, forward nfe 170338, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 115, Runtime 2.143173, Loss 1.278449, forward nfe 172136, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 116, Runtime 1.763328, Loss 1.434445, forward nfe 173658, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 117, Runtime 1.975378, Loss 1.372379, forward nfe 175264, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 118, Runtime 1.751403, Loss 1.404810, forward nfe 176780, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 119, Runtime 1.962221, Loss 1.326250, forward nfe 178434, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 120, Runtime 1.942849, Loss 1.300074, forward nfe 180076, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 121, Runtime 1.912076, Loss 1.412537, forward nfe 181718, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 122, Runtime 2.709121, Loss 1.375164, forward nfe 183906, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 123, Runtime 2.238757, Loss 1.448570, forward nfe 185764, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 124, Runtime 2.637547, Loss 1.424091, forward nfe 187928, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 125, Runtime 2.435318, Loss 1.422350, forward nfe 189948, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 126, Runtime 2.717961, Loss 1.633507, forward nfe 192190, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 127, Runtime 3.051449, Loss 1.428915, forward nfe 194600, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 128, Runtime 2.636434, Loss 1.554214, forward nfe 196776, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 129, Runtime 2.596075, Loss 1.441714, forward nfe 198838, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 130, Runtime 2.645158, Loss 1.458162, forward nfe 201008, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 131, Runtime 2.792054, Loss 1.570055, forward nfe 203292, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 132, Runtime 2.604620, Loss 1.442769, forward nfe 205426, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 133, Runtime 2.987492, Loss 1.451954, forward nfe 207716, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 134, Runtime 2.604278, Loss 1.441865, forward nfe 209850, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 135, Runtime 2.444342, Loss 1.627676, forward nfe 211870, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 136, Runtime 2.360555, Loss 1.390600, forward nfe 213830, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 137, Runtime 2.638326, Loss 1.594679, forward nfe 216006, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 138, Runtime 3.509769, Loss 1.453056, forward nfe 218788, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 139, Runtime 3.528209, Loss 1.483691, forward nfe 221648, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 140, Runtime 2.582441, Loss 1.445729, forward nfe 223776, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 141, Runtime 2.624777, Loss 1.472124, forward nfe 225940, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 142, Runtime 2.307650, Loss 1.447289, forward nfe 227858, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 143, Runtime 2.139411, Loss 1.496486, forward nfe 229656, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 144, Runtime 2.710208, Loss 1.400255, forward nfe 231880, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 145, Runtime 2.204312, Loss 1.490381, forward nfe 233660, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 146, Runtime 1.756790, Loss 1.451862, forward nfe 235188, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 147, Runtime 1.896879, Loss 1.497683, forward nfe 236812, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 148, Runtime 2.008035, Loss 1.409524, forward nfe 238484, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 149, Runtime 2.163115, Loss 1.386697, forward nfe 240210, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 150, Runtime 2.271924, Loss 1.366277, forward nfe 242020, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 151, Runtime 2.308055, Loss 1.312153, forward nfe 243938, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 152, Runtime 1.987573, Loss 1.437891, forward nfe 245610, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 153, Runtime 2.774703, Loss 1.322596, forward nfe 247624, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 154, Runtime 2.253396, Loss 1.312505, forward nfe 249434, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 155, Runtime 1.859213, Loss 1.379956, forward nfe 250980, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 156, Runtime 1.661705, Loss 1.499502, forward nfe 252400, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 157, Runtime 2.385743, Loss 1.536593, forward nfe 254378, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 158, Runtime 2.044002, Loss 1.370840, forward nfe 255972, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 159, Runtime 2.383238, Loss 1.474371, forward nfe 257944, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 160, Runtime 2.482899, Loss 1.461235, forward nfe 260000, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 161, Runtime 2.575383, Loss 1.458870, forward nfe 262116, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 162, Runtime 2.697123, Loss 1.345154, forward nfe 264274, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 163, Runtime 2.342817, Loss 1.472240, forward nfe 266228, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 164, Runtime 2.947927, Loss 1.359332, forward nfe 268632, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 165, Runtime 3.094534, Loss 1.506823, forward nfe 271156, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 166, Runtime 2.197576, Loss 1.442593, forward nfe 272978, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 167, Runtime 2.594813, Loss 1.473550, forward nfe 275070, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 168, Runtime 2.155238, Loss 1.449893, forward nfe 276868, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 169, Runtime 2.420069, Loss 1.471838, forward nfe 278882, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 170, Runtime 2.689113, Loss 1.436426, forward nfe 281088, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 171, Runtime 2.453781, Loss 1.549821, forward nfe 283108, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 172, Runtime 2.221463, Loss 1.426761, forward nfe 284966, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 173, Runtime 2.028505, Loss 1.359949, forward nfe 286680, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 174, Runtime 2.287616, Loss 1.428004, forward nfe 288544, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 175, Runtime 2.528892, Loss 1.442811, forward nfe 290642, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 176, Runtime 2.140516, Loss 1.507921, forward nfe 292446, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 177, Runtime 2.145457, Loss 1.332068, forward nfe 294256, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 178, Runtime 2.249025, Loss 1.283285, forward nfe 296120, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 179, Runtime 3.354385, Loss 1.304974, forward nfe 298680, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 180, Runtime 2.363344, Loss 1.228659, forward nfe 300574, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 181, Runtime 2.746091, Loss 1.368043, forward nfe 302822, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 182, Runtime 2.092237, Loss 1.552543, forward nfe 304572, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 183, Runtime 2.322497, Loss 1.425594, forward nfe 306346, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 184, Runtime 2.403411, Loss 1.482416, forward nfe 308348, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 185, Runtime 2.670112, Loss 1.430278, forward nfe 310548, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 186, Runtime 2.686577, Loss 1.373471, forward nfe 312688, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 187, Runtime 2.516076, Loss 1.353033, forward nfe 314768, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 188, Runtime 2.292422, Loss 1.503466, forward nfe 316692, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 189, Runtime 2.238555, Loss 1.436640, forward nfe 318562, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 190, Runtime 2.169005, Loss 1.363103, forward nfe 320300, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 191, Runtime 1.757955, Loss 1.391211, forward nfe 321828, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 192, Runtime 2.216326, Loss 1.367813, forward nfe 323680, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 193, Runtime 2.286740, Loss 1.464096, forward nfe 325532, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 194, Runtime 1.996899, Loss 1.546533, forward nfe 327222, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 195, Runtime 2.729972, Loss 1.359651, forward nfe 329464, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 196, Runtime 2.582180, Loss 1.245656, forward nfe 331592, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 197, Runtime 3.261738, Loss 1.274408, forward nfe 334242, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 198, Runtime 2.730683, Loss 1.273031, forward nfe 336472, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 199, Runtime 2.824017, Loss 1.344081, forward nfe 338732, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 200, Runtime 2.489883, Loss 1.452598, forward nfe 340794, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 201, Runtime 2.197218, Loss 1.361820, forward nfe 342634, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 202, Runtime 2.500705, Loss 1.372998, forward nfe 344696, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 203, Runtime 2.513398, Loss 1.425021, forward nfe 346602, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 204, Runtime 2.575312, Loss 1.360432, forward nfe 348724, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 205, Runtime 2.581663, Loss 1.373150, forward nfe 350786, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 206, Runtime 2.756296, Loss 1.477076, forward nfe 353052, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 207, Runtime 2.676764, Loss 1.260398, forward nfe 355060, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 208, Runtime 2.284901, Loss 1.484538, forward nfe 356966, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 209, Runtime 3.187611, Loss 1.368467, forward nfe 359556, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 210, Runtime 2.366144, Loss 1.618266, forward nfe 361516, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 211, Runtime 2.353939, Loss 1.467659, forward nfe 363392, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 212, Runtime 2.607625, Loss 1.513773, forward nfe 365550, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 213, Runtime 2.459727, Loss 1.449125, forward nfe 367582, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 214, Runtime 2.882346, Loss 1.399208, forward nfe 369932, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 215, Runtime 2.610208, Loss 1.494840, forward nfe 372078, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 216, Runtime 2.835395, Loss 1.631397, forward nfe 374332, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 217, Runtime 2.163814, Loss 1.335016, forward nfe 376136, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 218, Runtime 2.354652, Loss 1.456301, forward nfe 378090, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 219, Runtime 2.425797, Loss 1.425234, forward nfe 380098, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 220, Runtime 2.124664, Loss 1.469964, forward nfe 381884, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 221, Runtime 2.923573, Loss 1.358267, forward nfe 384276, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 222, Runtime 2.924714, Loss 1.504766, forward nfe 386614, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 223, Runtime 2.910505, Loss 1.476742, forward nfe 388988, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 224, Runtime 2.286101, Loss 1.491974, forward nfe 390888, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 225, Runtime 2.735013, Loss 1.371605, forward nfe 393136, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Epoch: 226, Runtime 2.654136, Loss 1.486736, forward nfe 395138, backward nfe 0, Train: 0.7167, Val: 0.7384, Test: 0.7290, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 40, in forward
    raise MaxNFEException
utils.MaxNFEException
best val accuracy 0.738406 with test accuracy 0.729032 at epoch 89 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #4...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.123542, Loss 1.792797, forward nfe 368, backward nfe 0, Train: 0.3917, Val: 0.4239, Test: 0.4210, Best time: 128.0000
Epoch: 002, Runtime 1.012733, Loss 1.770231, forward nfe 1296, backward nfe 0, Train: 0.4333, Val: 0.4725, Test: 0.4887, Best time: 8.1569
Epoch: 003, Runtime 1.046713, Loss 1.712115, forward nfe 2272, backward nfe 0, Train: 0.4500, Val: 0.4906, Test: 0.4661, Best time: 51.4820
Epoch: 004, Runtime 1.125216, Loss 1.681253, forward nfe 3308, backward nfe 0, Train: 0.4833, Val: 0.5471, Test: 0.5323, Best time: 53.6265
Epoch: 005, Runtime 1.152690, Loss 1.636008, forward nfe 4368, backward nfe 0, Train: 0.4833, Val: 0.5471, Test: 0.5323, Best time: 128.0000
Epoch: 006, Runtime 1.207991, Loss 1.587900, forward nfe 5470, backward nfe 0, Train: 0.5333, Val: 0.5812, Test: 0.5532, Best time: 31.1452
Epoch: 007, Runtime 1.479041, Loss 1.587196, forward nfe 6560, backward nfe 0, Train: 0.6917, Val: 0.7486, Test: 0.7339, Best time: 38.6109
Epoch: 008, Runtime 1.210135, Loss 1.579884, forward nfe 7656, backward nfe 0, Train: 0.6917, Val: 0.7486, Test: 0.7339, Best time: 128.0000
Epoch: 009, Runtime 1.367875, Loss 1.536888, forward nfe 8848, backward nfe 0, Train: 0.6917, Val: 0.7486, Test: 0.7339, Best time: 128.0000
Epoch: 010, Runtime 1.188083, Loss 1.541243, forward nfe 9932, backward nfe 0, Train: 0.6917, Val: 0.7486, Test: 0.7339, Best time: 128.0000
Epoch: 011, Runtime 1.214421, Loss 1.491134, forward nfe 11034, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 48.8838
Epoch: 012, Runtime 1.238091, Loss 1.497137, forward nfe 12160, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 013, Runtime 1.226556, Loss 1.538739, forward nfe 13274, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 014, Runtime 1.274914, Loss 1.522180, forward nfe 14424, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 015, Runtime 1.267903, Loss 1.517931, forward nfe 15580, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 016, Runtime 1.408175, Loss 1.439531, forward nfe 16832, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 017, Runtime 1.215587, Loss 1.409624, forward nfe 17940, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 018, Runtime 1.312515, Loss 1.412933, forward nfe 19120, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 019, Runtime 1.317101, Loss 1.400016, forward nfe 20300, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 020, Runtime 1.215912, Loss 1.330771, forward nfe 21408, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 021, Runtime 1.271820, Loss 1.281798, forward nfe 22558, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 022, Runtime 1.447180, Loss 1.205220, forward nfe 23840, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 023, Runtime 1.399843, Loss 1.359040, forward nfe 25092, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 024, Runtime 1.904454, Loss 1.407789, forward nfe 26626, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 025, Runtime 1.417365, Loss 1.271349, forward nfe 27890, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 026, Runtime 1.475305, Loss 1.347673, forward nfe 29196, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 027, Runtime 1.553924, Loss 1.413877, forward nfe 30562, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 028, Runtime 1.491486, Loss 1.382315, forward nfe 31880, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 029, Runtime 1.411115, Loss 1.286488, forward nfe 33132, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 030, Runtime 1.550351, Loss 1.225389, forward nfe 34492, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 031, Runtime 1.442484, Loss 1.400887, forward nfe 35774, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 032, Runtime 1.580456, Loss 1.377673, forward nfe 37134, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 033, Runtime 1.472018, Loss 1.292617, forward nfe 38440, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 034, Runtime 1.593516, Loss 1.287620, forward nfe 39812, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 035, Runtime 1.627749, Loss 1.402098, forward nfe 41232, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 036, Runtime 1.690794, Loss 1.320336, forward nfe 42700, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 037, Runtime 1.778773, Loss 1.328160, forward nfe 44222, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 038, Runtime 2.043476, Loss 1.250201, forward nfe 45882, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 039, Runtime 1.641361, Loss 1.316630, forward nfe 47320, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 040, Runtime 1.981158, Loss 1.299475, forward nfe 48992, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 041, Runtime 1.810794, Loss 1.256735, forward nfe 50376, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 042, Runtime 1.540154, Loss 1.529893, forward nfe 51706, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 043, Runtime 1.592885, Loss 1.334273, forward nfe 53102, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 044, Runtime 2.093869, Loss 1.338256, forward nfe 54858, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 045, Runtime 1.763285, Loss 1.449495, forward nfe 56368, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 046, Runtime 1.760145, Loss 1.420798, forward nfe 57872, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 047, Runtime 2.226028, Loss 1.404182, forward nfe 59514, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 048, Runtime 2.262589, Loss 1.456387, forward nfe 61384, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 049, Runtime 2.210385, Loss 1.496380, forward nfe 63230, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 050, Runtime 2.177329, Loss 1.365802, forward nfe 65046, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 051, Runtime 2.240133, Loss 1.516380, forward nfe 66904, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 052, Runtime 2.166669, Loss 1.374197, forward nfe 68708, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 053, Runtime 2.125193, Loss 1.406434, forward nfe 70476, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 054, Runtime 2.420743, Loss 1.459836, forward nfe 72400, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 055, Runtime 2.032912, Loss 1.368481, forward nfe 74108, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 056, Runtime 2.022735, Loss 1.419975, forward nfe 75804, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 057, Runtime 2.042153, Loss 1.391953, forward nfe 77518, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 058, Runtime 2.025156, Loss 1.311204, forward nfe 79220, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 059, Runtime 1.793865, Loss 1.416482, forward nfe 80748, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 060, Runtime 1.910969, Loss 1.359683, forward nfe 82360, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 061, Runtime 1.628350, Loss 1.298473, forward nfe 83786, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 062, Runtime 2.092163, Loss 1.353604, forward nfe 85548, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 063, Runtime 1.695440, Loss 1.338078, forward nfe 87028, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 064, Runtime 1.701982, Loss 1.478884, forward nfe 88454, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 065, Runtime 2.275697, Loss 1.516516, forward nfe 90240, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 066, Runtime 1.729751, Loss 1.477193, forward nfe 91738, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 067, Runtime 2.427195, Loss 1.471523, forward nfe 93752, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 068, Runtime 2.233895, Loss 1.581365, forward nfe 95622, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 069, Runtime 2.780160, Loss 1.525625, forward nfe 97894, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 070, Runtime 2.654340, Loss 1.459422, forward nfe 100070, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 071, Runtime 2.566275, Loss 1.430304, forward nfe 101952, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 072, Runtime 2.247852, Loss 1.437201, forward nfe 103822, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 073, Runtime 2.396110, Loss 1.444085, forward nfe 105806, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 074, Runtime 2.292567, Loss 1.302693, forward nfe 107538, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 075, Runtime 2.396605, Loss 1.565261, forward nfe 109534, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 076, Runtime 1.689332, Loss 1.400227, forward nfe 111002, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 077, Runtime 2.270162, Loss 1.579732, forward nfe 112884, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 078, Runtime 1.896630, Loss 1.586334, forward nfe 114442, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 079, Runtime 1.985213, Loss 1.430248, forward nfe 116102, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 080, Runtime 2.174489, Loss 1.430775, forward nfe 117930, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 081, Runtime 1.982362, Loss 1.508491, forward nfe 119590, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 082, Runtime 2.243854, Loss 1.507886, forward nfe 121454, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 083, Runtime 2.291745, Loss 1.445351, forward nfe 123366, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 084, Runtime 2.380264, Loss 1.611217, forward nfe 125338, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 085, Runtime 1.914654, Loss 1.439802, forward nfe 126890, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 086, Runtime 1.851997, Loss 1.479840, forward nfe 128490, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 087, Runtime 1.988997, Loss 1.457926, forward nfe 130168, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 088, Runtime 1.787750, Loss 1.478576, forward nfe 131720, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 089, Runtime 2.135341, Loss 1.476701, forward nfe 133512, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 090, Runtime 1.778413, Loss 1.447378, forward nfe 135058, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 091, Runtime 1.822001, Loss 1.575742, forward nfe 136610, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 092, Runtime 1.907727, Loss 1.629800, forward nfe 138228, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 093, Runtime 2.053808, Loss 1.487111, forward nfe 139954, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 094, Runtime 2.096761, Loss 1.573547, forward nfe 141728, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 095, Runtime 2.407136, Loss 1.558005, forward nfe 143676, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 096, Runtime 2.622848, Loss 1.491601, forward nfe 145840, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 097, Runtime 1.982984, Loss 1.502452, forward nfe 147518, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 098, Runtime 1.767887, Loss 1.476551, forward nfe 149058, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 099, Runtime 2.200876, Loss 1.471932, forward nfe 150712, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 100, Runtime 1.897316, Loss 1.364096, forward nfe 152324, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 101, Runtime 1.874652, Loss 1.534611, forward nfe 153912, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 102, Runtime 1.619858, Loss 1.479743, forward nfe 155272, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 103, Runtime 1.629838, Loss 1.539292, forward nfe 156698, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 104, Runtime 1.806085, Loss 1.407716, forward nfe 158082, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 105, Runtime 2.059329, Loss 1.442831, forward nfe 159814, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 106, Runtime 2.302640, Loss 1.432707, forward nfe 161744, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 107, Runtime 2.126885, Loss 1.433980, forward nfe 163524, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 108, Runtime 1.774677, Loss 1.378176, forward nfe 165070, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 109, Runtime 2.357440, Loss 1.440061, forward nfe 167036, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 110, Runtime 2.253135, Loss 1.477009, forward nfe 168918, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 111, Runtime 2.348451, Loss 1.424843, forward nfe 170818, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 112, Runtime 2.520314, Loss 1.441523, forward nfe 172874, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 113, Runtime 2.678947, Loss 1.372615, forward nfe 175044, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 114, Runtime 2.043761, Loss 1.492066, forward nfe 176764, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 115, Runtime 1.977693, Loss 1.439912, forward nfe 178436, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 116, Runtime 1.686108, Loss 1.414703, forward nfe 179910, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 117, Runtime 2.021661, Loss 1.371619, forward nfe 181618, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 118, Runtime 1.910912, Loss 1.454266, forward nfe 183164, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 119, Runtime 1.768556, Loss 1.413876, forward nfe 184674, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 120, Runtime 1.748808, Loss 1.394740, forward nfe 186166, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 121, Runtime 1.948392, Loss 1.486939, forward nfe 187814, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 122, Runtime 1.789216, Loss 1.357088, forward nfe 189342, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 123, Runtime 1.903201, Loss 1.423155, forward nfe 190954, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 124, Runtime 1.890240, Loss 1.441742, forward nfe 192560, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 125, Runtime 2.058509, Loss 1.465050, forward nfe 194298, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 126, Runtime 2.115463, Loss 1.446024, forward nfe 196012, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 127, Runtime 2.624833, Loss 1.483483, forward nfe 198176, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 128, Runtime 1.819607, Loss 1.398946, forward nfe 199746, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 129, Runtime 2.547254, Loss 1.422838, forward nfe 201670, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 130, Runtime 2.218385, Loss 1.449705, forward nfe 203516, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 131, Runtime 2.596708, Loss 1.414095, forward nfe 205644, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 132, Runtime 2.574378, Loss 1.343502, forward nfe 207574, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 133, Runtime 2.615943, Loss 1.353621, forward nfe 209684, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 134, Runtime 2.014216, Loss 1.371910, forward nfe 211374, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 135, Runtime 2.218066, Loss 1.457700, forward nfe 213226, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 136, Runtime 2.188968, Loss 1.393689, forward nfe 215060, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 137, Runtime 2.020862, Loss 1.532317, forward nfe 216762, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 138, Runtime 1.689737, Loss 1.423139, forward nfe 218230, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 139, Runtime 2.050296, Loss 1.506943, forward nfe 219950, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 140, Runtime 2.403343, Loss 1.442317, forward nfe 221934, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 141, Runtime 2.290501, Loss 1.391371, forward nfe 223780, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 142, Runtime 2.492776, Loss 1.566437, forward nfe 225800, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 143, Runtime 2.058884, Loss 1.421803, forward nfe 227520, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 144, Runtime 2.264456, Loss 1.442006, forward nfe 229390, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 145, Runtime 2.301771, Loss 1.438329, forward nfe 231296, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 146, Runtime 2.753584, Loss 1.562056, forward nfe 233556, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 147, Runtime 3.410498, Loss 1.431855, forward nfe 236200, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 148, Runtime 2.739293, Loss 1.419494, forward nfe 238424, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 149, Runtime 3.346489, Loss 1.405899, forward nfe 241110, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 150, Runtime 2.296281, Loss 1.405778, forward nfe 243034, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 151, Runtime 2.703732, Loss 1.451724, forward nfe 245246, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 152, Runtime 2.912072, Loss 1.479784, forward nfe 247548, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 153, Runtime 2.601224, Loss 1.372110, forward nfe 249616, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 154, Runtime 2.989711, Loss 1.569467, forward nfe 251882, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 155, Runtime 1.992737, Loss 1.440350, forward nfe 253554, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 156, Runtime 2.744281, Loss 1.498250, forward nfe 255808, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 157, Runtime 2.677202, Loss 1.484367, forward nfe 257780, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 158, Runtime 2.541845, Loss 1.473146, forward nfe 259878, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 159, Runtime 2.251436, Loss 1.517944, forward nfe 261754, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 160, Runtime 2.632042, Loss 1.582735, forward nfe 263924, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 161, Runtime 2.138497, Loss 1.569890, forward nfe 265716, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 162, Runtime 2.112749, Loss 1.346062, forward nfe 267496, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 163, Runtime 2.235430, Loss 1.516340, forward nfe 269354, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 164, Runtime 2.413347, Loss 1.387079, forward nfe 271308, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 165, Runtime 2.109658, Loss 1.411127, forward nfe 273070, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 166, Runtime 2.234618, Loss 1.429128, forward nfe 274940, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 167, Runtime 2.065895, Loss 1.501969, forward nfe 276684, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 168, Runtime 2.593704, Loss 1.701299, forward nfe 278830, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 169, Runtime 2.793394, Loss 1.633010, forward nfe 281096, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 170, Runtime 3.159619, Loss 1.535529, forward nfe 283608, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 171, Runtime 2.744236, Loss 1.506291, forward nfe 285850, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 172, Runtime 3.040201, Loss 1.515597, forward nfe 288320, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 173, Runtime 2.728603, Loss 1.593408, forward nfe 290550, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 174, Runtime 2.469994, Loss 1.428288, forward nfe 292588, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 175, Runtime 2.490237, Loss 1.394620, forward nfe 294656, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 176, Runtime 2.204418, Loss 1.534824, forward nfe 296418, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 177, Runtime 2.042145, Loss 1.371156, forward nfe 298132, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 178, Runtime 2.377425, Loss 1.578798, forward nfe 299924, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 179, Runtime 2.298699, Loss 1.502671, forward nfe 301842, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 180, Runtime 1.678266, Loss 1.395538, forward nfe 303304, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 181, Runtime 1.915604, Loss 1.529808, forward nfe 304922, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 182, Runtime 2.360718, Loss 1.531293, forward nfe 306696, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 183, Runtime 2.034950, Loss 1.530413, forward nfe 308416, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 184, Runtime 1.987808, Loss 1.519143, forward nfe 310022, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 185, Runtime 1.840019, Loss 1.505737, forward nfe 311586, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 186, Runtime 1.933677, Loss 1.416366, forward nfe 313216, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 187, Runtime 2.274541, Loss 1.483371, forward nfe 315104, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 188, Runtime 2.040411, Loss 1.431010, forward nfe 316812, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 189, Runtime 1.713969, Loss 1.439143, forward nfe 318298, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 190, Runtime 1.928818, Loss 1.423236, forward nfe 319928, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 191, Runtime 1.819825, Loss 1.461985, forward nfe 321456, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 192, Runtime 2.206757, Loss 1.337493, forward nfe 323296, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 193, Runtime 2.583704, Loss 1.533136, forward nfe 325274, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 194, Runtime 1.962987, Loss 1.540314, forward nfe 326928, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 195, Runtime 1.917976, Loss 1.583551, forward nfe 328570, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 196, Runtime 1.893708, Loss 1.541972, forward nfe 330200, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 197, Runtime 2.109117, Loss 1.702807, forward nfe 331968, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 198, Runtime 2.344130, Loss 1.549101, forward nfe 333910, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 199, Runtime 2.695845, Loss 1.543220, forward nfe 336104, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 200, Runtime 2.740536, Loss 1.669944, forward nfe 338292, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 201, Runtime 2.356293, Loss 1.544643, forward nfe 340246, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 202, Runtime 2.913823, Loss 1.488457, forward nfe 342620, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 203, Runtime 3.144658, Loss 1.524024, forward nfe 345192, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 204, Runtime 3.079827, Loss 1.391063, forward nfe 347692, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 205, Runtime 3.749778, Loss 1.494991, forward nfe 350456, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 206, Runtime 2.537914, Loss 1.487811, forward nfe 352554, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 207, Runtime 3.106703, Loss 1.491141, forward nfe 354886, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 208, Runtime 2.518514, Loss 1.458894, forward nfe 356960, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 209, Runtime 2.065219, Loss 1.630437, forward nfe 358686, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 210, Runtime 2.350505, Loss 1.524237, forward nfe 360586, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 211, Runtime 3.032284, Loss 1.442766, forward nfe 363050, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 212, Runtime 2.446027, Loss 1.444237, forward nfe 365064, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 213, Runtime 2.386659, Loss 1.441938, forward nfe 367042, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 214, Runtime 2.524022, Loss 1.519984, forward nfe 369122, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 215, Runtime 2.436025, Loss 1.446270, forward nfe 371118, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 216, Runtime 2.178290, Loss 1.438037, forward nfe 372880, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 217, Runtime 2.134505, Loss 1.399164, forward nfe 374660, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 218, Runtime 2.218744, Loss 1.395996, forward nfe 376512, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 219, Runtime 2.125170, Loss 1.505160, forward nfe 378286, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 220, Runtime 2.208743, Loss 1.433656, forward nfe 380132, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 221, Runtime 1.886312, Loss 1.404618, forward nfe 381726, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 222, Runtime 2.317537, Loss 1.516897, forward nfe 383656, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 223, Runtime 2.794161, Loss 1.617413, forward nfe 385898, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 224, Runtime 2.192313, Loss 1.522195, forward nfe 387726, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 225, Runtime 2.770356, Loss 1.410853, forward nfe 389980, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 226, Runtime 2.560863, Loss 1.439316, forward nfe 392084, backward nfe 0, Train: 0.6583, Val: 0.7500, Test: 0.7403, Best time: 128.0000
Epoch: 227, Runtime 2.048442, Loss 1.471969, forward nfe 393804, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 65.9692
Epoch: 228, Runtime 2.541921, Loss 1.429806, forward nfe 395896, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 229, Runtime 2.069090, Loss 1.479071, forward nfe 397562, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 230, Runtime 2.936382, Loss 1.554084, forward nfe 399732, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 231, Runtime 2.278141, Loss 1.384765, forward nfe 401632, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 232, Runtime 1.897366, Loss 1.500226, forward nfe 403232, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 233, Runtime 2.314531, Loss 1.520701, forward nfe 405000, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 234, Runtime 1.785134, Loss 1.436020, forward nfe 406540, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 235, Runtime 2.020713, Loss 1.559461, forward nfe 408242, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 236, Runtime 1.945601, Loss 1.550034, forward nfe 409884, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 237, Runtime 2.788674, Loss 1.576814, forward nfe 412120, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 238, Runtime 3.675160, Loss 1.549120, forward nfe 415082, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 239, Runtime 2.510211, Loss 1.466893, forward nfe 417156, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 240, Runtime 2.852116, Loss 1.722565, forward nfe 419500, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 241, Runtime 2.511466, Loss 1.586090, forward nfe 421562, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 242, Runtime 2.805586, Loss 1.574580, forward nfe 423792, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 243, Runtime 2.625292, Loss 1.641547, forward nfe 425944, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 244, Runtime 2.551499, Loss 1.540203, forward nfe 428054, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 245, Runtime 2.353285, Loss 1.534643, forward nfe 429996, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 246, Runtime 2.297758, Loss 1.419825, forward nfe 431908, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 247, Runtime 2.022057, Loss 1.421475, forward nfe 433628, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 248, Runtime 1.852242, Loss 1.460345, forward nfe 435174, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
Epoch: 249, Runtime 1.851324, Loss 1.573867, forward nfe 436768, backward nfe 0, Train: 0.7083, Val: 0.7543, Test: 0.7452, Best time: 128.0000
best val accuracy 0.754348 with test accuracy 0.745161 at epoch 227 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #5...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.059635, Loss 1.793714, forward nfe 350, backward nfe 0, Train: 0.3000, Val: 0.3022, Test: 0.2710, Best time: 0.0399
Epoch: 002, Runtime 1.016059, Loss 1.794172, forward nfe 1302, backward nfe 0, Train: 0.3417, Val: 0.3565, Test: 0.3113, Best time: 0.0378
Epoch: 003, Runtime 0.993337, Loss 1.789890, forward nfe 2242, backward nfe 0, Train: 0.4583, Val: 0.4283, Test: 0.3968, Best time: 0.0374
Epoch: 004, Runtime 1.007080, Loss 1.751508, forward nfe 3188, backward nfe 0, Train: 0.5833, Val: 0.6384, Test: 0.5968, Best time: 187.5407
Epoch: 005, Runtime 1.063261, Loss 1.741006, forward nfe 4182, backward nfe 0, Train: 0.6250, Val: 0.7123, Test: 0.6758, Best time: 115.7560
Epoch: 006, Runtime 1.106559, Loss 1.666805, forward nfe 5206, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 99.0522
Epoch: 007, Runtime 1.103698, Loss 1.676700, forward nfe 6218, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 008, Runtime 1.104637, Loss 1.638646, forward nfe 7236, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 009, Runtime 1.108962, Loss 1.604586, forward nfe 8266, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 010, Runtime 1.147542, Loss 1.602912, forward nfe 9314, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 011, Runtime 1.145580, Loss 1.573106, forward nfe 10362, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 012, Runtime 1.366755, Loss 1.581354, forward nfe 11398, backward nfe 0, Train: 0.6417, Val: 0.7232, Test: 0.6952, Best time: 128.0000
Epoch: 013, Runtime 1.166561, Loss 1.545749, forward nfe 12458, backward nfe 0, Train: 0.6667, Val: 0.7290, Test: 0.7048, Best time: 77.0409
Epoch: 014, Runtime 1.165565, Loss 1.576660, forward nfe 13518, backward nfe 0, Train: 0.6667, Val: 0.7290, Test: 0.7048, Best time: 128.0000
Epoch: 015, Runtime 1.181006, Loss 1.539012, forward nfe 14596, backward nfe 0, Train: 0.6917, Val: 0.7420, Test: 0.6919, Best time: 30.9166
Epoch: 016, Runtime 1.163032, Loss 1.466717, forward nfe 15656, backward nfe 0, Train: 0.6917, Val: 0.7420, Test: 0.6919, Best time: 128.0000
Epoch: 017, Runtime 1.365611, Loss 1.505337, forward nfe 16878, backward nfe 0, Train: 0.6917, Val: 0.7420, Test: 0.6919, Best time: 128.0000
Epoch: 018, Runtime 1.521118, Loss 1.437057, forward nfe 18034, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 41.0381
Epoch: 019, Runtime 1.228187, Loss 1.415878, forward nfe 19148, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 020, Runtime 1.318229, Loss 1.363598, forward nfe 20334, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 021, Runtime 1.643166, Loss 1.369721, forward nfe 21748, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 022, Runtime 1.463083, Loss 1.465994, forward nfe 23042, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 023, Runtime 1.474785, Loss 1.432562, forward nfe 24348, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 024, Runtime 1.440910, Loss 1.398856, forward nfe 25546, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 025, Runtime 1.707142, Loss 1.332920, forward nfe 27014, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 026, Runtime 1.580081, Loss 1.404086, forward nfe 28398, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 027, Runtime 1.754184, Loss 1.389772, forward nfe 29896, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 028, Runtime 1.592587, Loss 1.487263, forward nfe 31286, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 029, Runtime 1.515308, Loss 1.429301, forward nfe 32622, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 030, Runtime 1.564991, Loss 1.498800, forward nfe 33988, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 031, Runtime 1.625756, Loss 1.481979, forward nfe 35384, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 032, Runtime 1.728671, Loss 1.406553, forward nfe 36852, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 033, Runtime 1.853420, Loss 1.504552, forward nfe 38416, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 034, Runtime 2.036234, Loss 1.552853, forward nfe 40100, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 035, Runtime 1.887917, Loss 1.517728, forward nfe 41652, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 036, Runtime 1.673517, Loss 1.396782, forward nfe 43108, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 037, Runtime 1.962023, Loss 1.443355, forward nfe 44768, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 038, Runtime 1.473385, Loss 1.441572, forward nfe 46068, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 039, Runtime 1.595965, Loss 1.406691, forward nfe 47464, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 040, Runtime 1.599856, Loss 1.458259, forward nfe 48860, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 041, Runtime 1.705609, Loss 1.509113, forward nfe 50310, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 042, Runtime 1.624852, Loss 1.459928, forward nfe 51730, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 043, Runtime 1.836898, Loss 1.460699, forward nfe 53294, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 044, Runtime 2.153343, Loss 1.464756, forward nfe 55092, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 045, Runtime 1.803564, Loss 1.562922, forward nfe 56572, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 046, Runtime 1.643036, Loss 1.549405, forward nfe 58004, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 047, Runtime 1.496267, Loss 1.433530, forward nfe 59310, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 048, Runtime 1.667461, Loss 1.447376, forward nfe 60730, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 049, Runtime 1.489057, Loss 1.529537, forward nfe 62036, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 050, Runtime 2.038266, Loss 1.428669, forward nfe 63540, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 051, Runtime 1.543610, Loss 1.363614, forward nfe 64882, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 052, Runtime 1.631502, Loss 1.507639, forward nfe 66266, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 053, Runtime 1.663521, Loss 1.343135, forward nfe 67698, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 054, Runtime 1.918695, Loss 1.488682, forward nfe 69160, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 055, Runtime 1.719436, Loss 1.403520, forward nfe 70622, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 056, Runtime 1.858322, Loss 1.441880, forward nfe 72186, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 057, Runtime 1.777065, Loss 1.412526, forward nfe 73678, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 058, Runtime 2.389260, Loss 1.416178, forward nfe 75572, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 059, Runtime 1.673664, Loss 1.453181, forward nfe 77004, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 060, Runtime 1.594494, Loss 1.418967, forward nfe 78400, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 061, Runtime 1.627903, Loss 1.454612, forward nfe 79796, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 062, Runtime 1.701149, Loss 1.466816, forward nfe 81270, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 063, Runtime 2.385093, Loss 1.476408, forward nfe 83242, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 064, Runtime 2.045592, Loss 1.448148, forward nfe 84950, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 065, Runtime 1.958532, Loss 1.626261, forward nfe 86598, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 066, Runtime 1.891062, Loss 1.626203, forward nfe 88192, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 067, Runtime 2.200386, Loss 1.688607, forward nfe 90020, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 068, Runtime 2.673411, Loss 1.548283, forward nfe 92208, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 069, Runtime 2.484891, Loss 1.589171, forward nfe 94204, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 070, Runtime 2.749198, Loss 1.472456, forward nfe 96452, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 071, Runtime 2.334476, Loss 1.570826, forward nfe 98382, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 072, Runtime 2.411227, Loss 1.575034, forward nfe 100378, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 073, Runtime 2.146992, Loss 1.511928, forward nfe 102164, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 074, Runtime 2.549732, Loss 1.559045, forward nfe 104256, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 075, Runtime 1.736451, Loss 1.510418, forward nfe 105700, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 076, Runtime 1.959706, Loss 1.488865, forward nfe 107348, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 077, Runtime 1.599019, Loss 1.547803, forward nfe 108750, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 078, Runtime 1.373633, Loss 1.475064, forward nfe 109972, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 079, Runtime 1.477848, Loss 1.604393, forward nfe 111278, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 080, Runtime 1.693529, Loss 1.410294, forward nfe 112722, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 081, Runtime 1.996941, Loss 1.527684, forward nfe 114124, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 082, Runtime 1.530385, Loss 1.589784, forward nfe 115454, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 083, Runtime 1.642851, Loss 1.523777, forward nfe 116886, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 084, Runtime 2.407611, Loss 1.527319, forward nfe 118684, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 085, Runtime 1.757111, Loss 1.350403, forward nfe 120146, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 086, Runtime 1.918297, Loss 1.782097, forward nfe 121752, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 087, Runtime 1.877962, Loss 1.496679, forward nfe 123334, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 088, Runtime 1.681659, Loss 1.413611, forward nfe 124766, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 089, Runtime 1.737836, Loss 1.464232, forward nfe 126234, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 090, Runtime 2.239738, Loss 1.537841, forward nfe 128092, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 091, Runtime 2.157510, Loss 1.458917, forward nfe 129896, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 092, Runtime 1.982216, Loss 1.458985, forward nfe 131556, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 093, Runtime 2.519788, Loss 1.495699, forward nfe 133564, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 094, Runtime 2.180479, Loss 1.466972, forward nfe 135380, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 095, Runtime 2.459545, Loss 1.522742, forward nfe 137406, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 096, Runtime 2.429325, Loss 1.538454, forward nfe 139402, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 097, Runtime 1.711680, Loss 1.555353, forward nfe 140876, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 098, Runtime 1.892846, Loss 1.436702, forward nfe 142470, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 099, Runtime 1.684691, Loss 1.395212, forward nfe 143920, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 100, Runtime 2.103118, Loss 1.584400, forward nfe 145676, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 101, Runtime 2.147273, Loss 1.451213, forward nfe 147474, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 102, Runtime 2.154244, Loss 1.526899, forward nfe 149218, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 103, Runtime 1.691476, Loss 1.460768, forward nfe 150686, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 104, Runtime 1.835808, Loss 1.515795, forward nfe 152238, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 105, Runtime 1.844980, Loss 1.393688, forward nfe 153790, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 106, Runtime 2.130431, Loss 1.469579, forward nfe 155570, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 107, Runtime 1.907007, Loss 1.407606, forward nfe 157176, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 108, Runtime 1.764845, Loss 1.459895, forward nfe 158704, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 109, Runtime 1.724905, Loss 1.473361, forward nfe 160166, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 110, Runtime 1.969689, Loss 1.509113, forward nfe 161820, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 111, Runtime 2.196643, Loss 1.360298, forward nfe 163588, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 112, Runtime 1.933231, Loss 1.500649, forward nfe 165050, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 113, Runtime 2.210677, Loss 1.444450, forward nfe 166914, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 114, Runtime 2.160907, Loss 1.630022, forward nfe 168520, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 115, Runtime 2.446900, Loss 1.447087, forward nfe 170534, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 116, Runtime 1.966303, Loss 1.558904, forward nfe 172176, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 117, Runtime 1.849515, Loss 1.617602, forward nfe 173746, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 118, Runtime 1.835988, Loss 1.518348, forward nfe 175328, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 119, Runtime 2.147540, Loss 1.454300, forward nfe 177054, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 120, Runtime 1.756725, Loss 1.540939, forward nfe 178570, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 121, Runtime 1.914674, Loss 1.506674, forward nfe 180194, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 122, Runtime 2.457752, Loss 1.572350, forward nfe 182214, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 123, Runtime 2.291682, Loss 1.478943, forward nfe 184114, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 124, Runtime 2.094441, Loss 1.436914, forward nfe 185858, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 125, Runtime 2.104265, Loss 1.491988, forward nfe 187620, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 126, Runtime 1.903853, Loss 1.426641, forward nfe 189232, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 127, Runtime 1.826468, Loss 1.499262, forward nfe 190706, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 128, Runtime 2.435491, Loss 1.539342, forward nfe 192720, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 129, Runtime 1.671480, Loss 1.622938, forward nfe 194176, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 130, Runtime 2.148345, Loss 1.467149, forward nfe 195974, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 131, Runtime 2.191062, Loss 1.491178, forward nfe 197688, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 132, Runtime 2.312721, Loss 1.525360, forward nfe 199642, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 133, Runtime 1.870499, Loss 1.472206, forward nfe 201278, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 134, Runtime 1.976917, Loss 1.494426, forward nfe 202986, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 135, Runtime 2.020127, Loss 1.543663, forward nfe 204676, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 136, Runtime 1.910780, Loss 1.462255, forward nfe 206324, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 137, Runtime 1.949449, Loss 1.399782, forward nfe 208008, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 138, Runtime 1.845244, Loss 1.346051, forward nfe 209602, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 139, Runtime 1.986000, Loss 1.487080, forward nfe 211304, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 140, Runtime 1.955009, Loss 1.442099, forward nfe 212988, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 141, Runtime 2.151866, Loss 1.600851, forward nfe 214630, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 142, Runtime 2.129548, Loss 1.426720, forward nfe 216440, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 143, Runtime 2.804082, Loss 1.444622, forward nfe 218532, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 144, Runtime 1.897249, Loss 1.509665, forward nfe 220192, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 145, Runtime 2.048078, Loss 1.446390, forward nfe 221954, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 146, Runtime 2.639475, Loss 1.475067, forward nfe 224172, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 147, Runtime 1.906482, Loss 1.524675, forward nfe 225796, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 148, Runtime 1.770180, Loss 1.471094, forward nfe 227354, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 149, Runtime 1.566843, Loss 1.543376, forward nfe 228756, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 150, Runtime 1.658607, Loss 1.511029, forward nfe 230236, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 151, Runtime 1.807626, Loss 1.562860, forward nfe 231734, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 152, Runtime 2.077023, Loss 1.572786, forward nfe 233520, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 153, Runtime 2.006110, Loss 1.568318, forward nfe 235240, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 154, Runtime 2.388674, Loss 1.509892, forward nfe 237260, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 155, Runtime 2.620489, Loss 1.547613, forward nfe 239460, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 156, Runtime 2.487952, Loss 1.523882, forward nfe 241546, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 157, Runtime 2.414346, Loss 1.442614, forward nfe 243572, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 158, Runtime 2.793183, Loss 1.456353, forward nfe 245844, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 159, Runtime 2.508907, Loss 1.712897, forward nfe 247954, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 160, Runtime 2.015905, Loss 1.580466, forward nfe 249674, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 161, Runtime 2.706793, Loss 1.492855, forward nfe 251946, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 162, Runtime 2.498523, Loss 1.584334, forward nfe 254050, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 163, Runtime 2.443799, Loss 1.711482, forward nfe 256052, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 164, Runtime 2.488216, Loss 1.558861, forward nfe 258150, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 165, Runtime 2.498699, Loss 1.530762, forward nfe 260242, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 166, Runtime 2.271535, Loss 1.632378, forward nfe 262172, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 167, Runtime 2.531454, Loss 1.588520, forward nfe 264288, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 168, Runtime 3.001757, Loss 1.616726, forward nfe 266578, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 169, Runtime 2.399840, Loss 1.607257, forward nfe 268364, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 170, Runtime 2.265904, Loss 1.548872, forward nfe 270282, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 171, Runtime 2.054434, Loss 1.642416, forward nfe 272044, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 172, Runtime 2.432020, Loss 1.802930, forward nfe 274094, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 173, Runtime 2.214495, Loss 1.609746, forward nfe 275976, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 174, Runtime 2.240076, Loss 1.590953, forward nfe 277882, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 175, Runtime 2.150958, Loss 1.520447, forward nfe 279722, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 176, Runtime 2.227880, Loss 1.531242, forward nfe 281544, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 177, Runtime 2.071625, Loss 1.575650, forward nfe 283318, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 178, Runtime 2.430051, Loss 1.584026, forward nfe 285314, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 179, Runtime 2.251547, Loss 1.621282, forward nfe 287220, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 180, Runtime 1.705924, Loss 1.508967, forward nfe 288724, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 181, Runtime 1.787886, Loss 1.440421, forward nfe 290276, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 182, Runtime 1.904767, Loss 1.385029, forward nfe 291906, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 183, Runtime 1.952100, Loss 1.488862, forward nfe 293584, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 184, Runtime 2.322513, Loss 1.533281, forward nfe 295478, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 185, Runtime 2.472733, Loss 1.604109, forward nfe 297552, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 186, Runtime 2.377655, Loss 1.581797, forward nfe 299560, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 187, Runtime 2.349775, Loss 1.544091, forward nfe 301538, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 188, Runtime 2.692217, Loss 1.481106, forward nfe 303786, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 189, Runtime 2.353985, Loss 1.547078, forward nfe 305758, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 190, Runtime 2.078917, Loss 1.523552, forward nfe 307532, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 191, Runtime 2.568984, Loss 1.483551, forward nfe 309636, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 192, Runtime 2.196989, Loss 1.533396, forward nfe 311506, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 193, Runtime 1.941156, Loss 1.528349, forward nfe 313190, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 194, Runtime 2.059781, Loss 1.474680, forward nfe 314946, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 195, Runtime 1.886683, Loss 1.378580, forward nfe 316384, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 196, Runtime 1.960854, Loss 1.463002, forward nfe 318062, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 197, Runtime 1.920793, Loss 1.442474, forward nfe 319530, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 198, Runtime 1.594970, Loss 1.418369, forward nfe 320950, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 199, Runtime 1.570600, Loss 1.479311, forward nfe 322322, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 200, Runtime 2.070106, Loss 1.481712, forward nfe 324030, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 201, Runtime 2.039482, Loss 1.540642, forward nfe 325804, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 202, Runtime 2.074571, Loss 1.464537, forward nfe 327572, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 203, Runtime 2.302803, Loss 1.384317, forward nfe 329526, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 204, Runtime 1.999923, Loss 1.484347, forward nfe 331246, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 205, Runtime 1.864856, Loss 1.505776, forward nfe 332846, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 206, Runtime 1.760245, Loss 1.465937, forward nfe 334380, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 207, Runtime 2.257571, Loss 1.551306, forward nfe 336280, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 208, Runtime 1.911077, Loss 1.516085, forward nfe 337850, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 209, Runtime 2.283356, Loss 1.537924, forward nfe 339774, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 210, Runtime 2.244261, Loss 1.435599, forward nfe 341668, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 211, Runtime 1.914320, Loss 1.713016, forward nfe 343334, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 212, Runtime 2.187918, Loss 1.820190, forward nfe 345186, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 213, Runtime 2.253217, Loss 1.440468, forward nfe 347086, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 214, Runtime 2.464852, Loss 1.617135, forward nfe 349160, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 215, Runtime 2.435353, Loss 1.547748, forward nfe 351138, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 216, Runtime 2.969954, Loss 1.418430, forward nfe 353596, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 217, Runtime 2.256842, Loss 1.632408, forward nfe 355502, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 218, Runtime 2.375086, Loss 1.453679, forward nfe 357510, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 219, Runtime 2.630012, Loss 1.539900, forward nfe 359560, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 220, Runtime 2.169666, Loss 1.488631, forward nfe 361418, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 221, Runtime 2.156527, Loss 1.673349, forward nfe 363192, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 222, Runtime 2.033616, Loss 1.401115, forward nfe 364930, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 223, Runtime 2.357486, Loss 1.457519, forward nfe 366746, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 224, Runtime 1.966952, Loss 1.754904, forward nfe 368424, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 225, Runtime 2.040392, Loss 1.510806, forward nfe 369976, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 226, Runtime 2.585222, Loss 1.562579, forward nfe 372140, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 227, Runtime 2.011704, Loss 1.494255, forward nfe 373854, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 228, Runtime 2.112226, Loss 1.605397, forward nfe 375664, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 229, Runtime 2.146781, Loss 1.590494, forward nfe 377438, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 230, Runtime 1.927188, Loss 1.507904, forward nfe 379092, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 231, Runtime 1.996461, Loss 1.496274, forward nfe 380812, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 232, Runtime 1.816090, Loss 1.529732, forward nfe 382382, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 233, Runtime 2.181868, Loss 1.438506, forward nfe 384228, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 234, Runtime 1.996066, Loss 1.450670, forward nfe 385930, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 235, Runtime 1.876184, Loss 1.507763, forward nfe 387542, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 236, Runtime 2.308330, Loss 1.493468, forward nfe 389484, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 237, Runtime 2.450721, Loss 1.540164, forward nfe 391486, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 238, Runtime 2.422676, Loss 1.556692, forward nfe 393500, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 239, Runtime 2.673674, Loss 1.538020, forward nfe 395718, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 240, Runtime 2.605413, Loss 1.392108, forward nfe 397894, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 241, Runtime 2.491369, Loss 1.420535, forward nfe 399974, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 242, Runtime 2.399942, Loss 1.424687, forward nfe 401922, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 243, Runtime 2.438834, Loss 1.473045, forward nfe 403966, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 244, Runtime 2.325310, Loss 1.678607, forward nfe 405932, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 245, Runtime 2.009673, Loss 1.450695, forward nfe 407652, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 246, Runtime 2.919419, Loss 1.526477, forward nfe 410080, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 247, Runtime 2.627949, Loss 1.519227, forward nfe 412262, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 248, Runtime 3.452519, Loss 1.441438, forward nfe 415044, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
Epoch: 249, Runtime 2.867482, Loss 1.677099, forward nfe 417256, backward nfe 0, Train: 0.7083, Val: 0.7623, Test: 0.7435, Best time: 128.0000
best val accuracy 0.762319 with test accuracy 0.743548 at epoch 18 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #6...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.008289, Loss 1.795444, forward nfe 356, backward nfe 0, Train: 0.1667, Val: 0.0486, Test: 0.0710, Best time: 3.5476
Epoch: 002, Runtime 1.235681, Loss 1.795117, forward nfe 1296, backward nfe 0, Train: 0.1667, Val: 0.0565, Test: 0.0790, Best time: 5.1614
Epoch: 003, Runtime 1.015295, Loss 1.790845, forward nfe 2236, backward nfe 0, Train: 0.2167, Val: 0.0964, Test: 0.1177, Best time: 3.4815
Epoch: 004, Runtime 0.986246, Loss 1.775194, forward nfe 3182, backward nfe 0, Train: 0.3833, Val: 0.2783, Test: 0.3113, Best time: 128.0000
Epoch: 005, Runtime 1.024040, Loss 1.758880, forward nfe 4158, backward nfe 0, Train: 0.3833, Val: 0.2783, Test: 0.3113, Best time: 128.0000
Epoch: 006, Runtime 1.054901, Loss 1.711210, forward nfe 5158, backward nfe 0, Train: 0.3833, Val: 0.2783, Test: 0.3113, Best time: 128.0000
Epoch: 007, Runtime 1.072748, Loss 1.691645, forward nfe 6170, backward nfe 0, Train: 0.3833, Val: 0.2783, Test: 0.3113, Best time: 128.0000
Epoch: 008, Runtime 1.073716, Loss 1.672661, forward nfe 7188, backward nfe 0, Train: 0.4167, Val: 0.3942, Test: 0.3742, Best time: 119.4298
Epoch: 009, Runtime 1.052796, Loss 1.640959, forward nfe 8188, backward nfe 0, Train: 0.5250, Val: 0.4761, Test: 0.4661, Best time: 96.9370
Epoch: 010, Runtime 1.049272, Loss 1.610513, forward nfe 9188, backward nfe 0, Train: 0.6750, Val: 0.5833, Test: 0.5903, Best time: 10.1817
Epoch: 011, Runtime 1.094540, Loss 1.562912, forward nfe 10212, backward nfe 0, Train: 0.6667, Val: 0.6565, Test: 0.6581, Best time: 5.6822
Epoch: 012, Runtime 1.081514, Loss 1.593089, forward nfe 11236, backward nfe 0, Train: 0.7333, Val: 0.6819, Test: 0.6887, Best time: 11.5997
Epoch: 013, Runtime 1.153957, Loss 1.542229, forward nfe 12314, backward nfe 0, Train: 0.7333, Val: 0.6819, Test: 0.6887, Best time: 128.0000
Epoch: 014, Runtime 1.124994, Loss 1.547052, forward nfe 13374, backward nfe 0, Train: 0.7333, Val: 0.7022, Test: 0.6952, Best time: 23.7725
Epoch: 015, Runtime 1.132035, Loss 1.510992, forward nfe 14440, backward nfe 0, Train: 0.6917, Val: 0.7399, Test: 0.7145, Best time: 34.8061
Epoch: 016, Runtime 1.158185, Loss 1.385899, forward nfe 15524, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 38.9101
Epoch: 017, Runtime 1.303496, Loss 1.461206, forward nfe 16632, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 018, Runtime 1.197631, Loss 1.433464, forward nfe 17740, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 019, Runtime 1.190302, Loss 1.331079, forward nfe 18848, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 020, Runtime 1.471676, Loss 1.393759, forward nfe 20142, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 021, Runtime 1.255208, Loss 1.403144, forward nfe 21304, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 022, Runtime 1.519739, Loss 1.510410, forward nfe 22622, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 023, Runtime 1.258606, Loss 1.400709, forward nfe 23784, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 024, Runtime 1.386297, Loss 1.365162, forward nfe 25042, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 025, Runtime 1.362603, Loss 1.268499, forward nfe 26282, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 026, Runtime 1.479742, Loss 1.330253, forward nfe 27612, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 027, Runtime 1.642818, Loss 1.405379, forward nfe 29080, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 028, Runtime 1.536859, Loss 1.515770, forward nfe 30452, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 029, Runtime 1.466008, Loss 1.321139, forward nfe 31776, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 030, Runtime 1.591619, Loss 1.370918, forward nfe 33178, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 031, Runtime 1.494288, Loss 1.359635, forward nfe 34520, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 032, Runtime 1.615299, Loss 1.337398, forward nfe 35940, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 033, Runtime 1.553331, Loss 1.348651, forward nfe 37324, backward nfe 0, Train: 0.7167, Val: 0.7428, Test: 0.7113, Best time: 128.0000
Epoch: 034, Runtime 1.588327, Loss 1.344864, forward nfe 38672, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 30.6993
Epoch: 035, Runtime 1.450197, Loss 1.384476, forward nfe 39990, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 036, Runtime 1.552559, Loss 1.348365, forward nfe 41380, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 037, Runtime 1.528514, Loss 1.478054, forward nfe 42752, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 038, Runtime 1.690138, Loss 1.387847, forward nfe 44232, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 039, Runtime 1.557533, Loss 1.296855, forward nfe 45628, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 040, Runtime 1.466934, Loss 1.441287, forward nfe 46946, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 041, Runtime 1.638685, Loss 1.341117, forward nfe 48402, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 042, Runtime 1.620208, Loss 1.425206, forward nfe 49822, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 043, Runtime 1.634019, Loss 1.508052, forward nfe 51278, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 044, Runtime 1.943276, Loss 1.430731, forward nfe 52770, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 045, Runtime 1.688443, Loss 1.417132, forward nfe 54238, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 046, Runtime 2.334702, Loss 1.435511, forward nfe 55994, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 047, Runtime 1.849870, Loss 1.405681, forward nfe 57594, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 048, Runtime 1.960101, Loss 1.278593, forward nfe 59278, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 049, Runtime 2.296099, Loss 1.395831, forward nfe 61178, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 050, Runtime 1.943338, Loss 1.428375, forward nfe 62844, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 051, Runtime 1.751035, Loss 1.339874, forward nfe 64372, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 052, Runtime 1.792433, Loss 1.321953, forward nfe 65930, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 053, Runtime 1.608372, Loss 1.321521, forward nfe 67362, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 054, Runtime 1.547266, Loss 1.341933, forward nfe 68734, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 055, Runtime 1.920558, Loss 1.467066, forward nfe 70388, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 056, Runtime 1.663013, Loss 1.367432, forward nfe 71868, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 057, Runtime 1.638995, Loss 1.470362, forward nfe 73330, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 058, Runtime 2.141213, Loss 1.505608, forward nfe 75164, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 059, Runtime 1.756656, Loss 1.481335, forward nfe 76632, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 060, Runtime 2.163364, Loss 1.388106, forward nfe 78472, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 061, Runtime 2.071925, Loss 1.446874, forward nfe 80246, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 062, Runtime 2.225739, Loss 1.505164, forward nfe 82140, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 063, Runtime 2.243532, Loss 1.499488, forward nfe 84028, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 064, Runtime 1.947756, Loss 1.487342, forward nfe 85706, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 065, Runtime 2.150185, Loss 1.453462, forward nfe 87528, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 066, Runtime 2.371730, Loss 1.477017, forward nfe 89470, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 067, Runtime 1.929116, Loss 1.432497, forward nfe 91136, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 068, Runtime 1.851409, Loss 1.543816, forward nfe 92742, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 069, Runtime 2.082922, Loss 1.469438, forward nfe 94516, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 070, Runtime 1.578049, Loss 1.430657, forward nfe 95930, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 071, Runtime 1.736718, Loss 1.474933, forward nfe 97428, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 072, Runtime 1.846203, Loss 1.366574, forward nfe 98944, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 073, Runtime 1.819474, Loss 1.367783, forward nfe 100520, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 074, Runtime 1.757374, Loss 1.369325, forward nfe 102066, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 075, Runtime 1.522756, Loss 1.333225, forward nfe 103426, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 076, Runtime 2.498385, Loss 1.319143, forward nfe 105278, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 077, Runtime 1.837944, Loss 1.355421, forward nfe 106698, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 078, Runtime 2.246526, Loss 1.378788, forward nfe 108610, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 079, Runtime 1.826085, Loss 1.531142, forward nfe 110216, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 080, Runtime 1.809620, Loss 1.502981, forward nfe 111774, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 081, Runtime 1.605464, Loss 1.398443, forward nfe 113206, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 082, Runtime 2.001910, Loss 1.440718, forward nfe 114926, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 083, Runtime 2.313669, Loss 1.341431, forward nfe 116892, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 084, Runtime 1.725599, Loss 1.348261, forward nfe 118420, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 085, Runtime 1.945931, Loss 1.374238, forward nfe 120044, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 086, Runtime 1.682297, Loss 1.380856, forward nfe 121536, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 087, Runtime 2.131065, Loss 1.386133, forward nfe 123346, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 088, Runtime 2.149901, Loss 1.478783, forward nfe 125180, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 089, Runtime 1.581505, Loss 1.411126, forward nfe 126588, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 090, Runtime 2.077399, Loss 1.413291, forward nfe 128362, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 091, Runtime 1.965098, Loss 1.386546, forward nfe 130046, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 092, Runtime 1.698371, Loss 1.466148, forward nfe 131550, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 093, Runtime 1.817169, Loss 1.450441, forward nfe 133072, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 094, Runtime 1.800732, Loss 1.463411, forward nfe 134636, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 095, Runtime 2.377148, Loss 1.488741, forward nfe 136644, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 096, Runtime 2.165839, Loss 1.517757, forward nfe 138496, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 097, Runtime 1.896753, Loss 1.406191, forward nfe 140120, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 098, Runtime 2.023415, Loss 1.589395, forward nfe 141852, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 099, Runtime 2.137503, Loss 1.554655, forward nfe 143668, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 100, Runtime 1.697854, Loss 1.486065, forward nfe 145172, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 101, Runtime 1.785613, Loss 1.470416, forward nfe 146724, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 102, Runtime 1.967888, Loss 1.410695, forward nfe 148342, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 103, Runtime 2.156984, Loss 1.459489, forward nfe 150164, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 104, Runtime 1.781107, Loss 1.377678, forward nfe 151710, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 105, Runtime 1.831468, Loss 1.459678, forward nfe 153286, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 106, Runtime 2.671081, Loss 1.532525, forward nfe 155324, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 107, Runtime 2.360118, Loss 1.566651, forward nfe 157122, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 108, Runtime 2.039824, Loss 1.504364, forward nfe 158866, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 109, Runtime 2.231130, Loss 1.419522, forward nfe 160760, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 110, Runtime 2.325525, Loss 1.423948, forward nfe 162672, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 111, Runtime 2.735801, Loss 1.462300, forward nfe 164950, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 112, Runtime 2.281910, Loss 1.446384, forward nfe 166886, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 113, Runtime 2.199000, Loss 1.443827, forward nfe 168780, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 114, Runtime 2.218144, Loss 1.444446, forward nfe 170644, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 115, Runtime 2.088324, Loss 1.415830, forward nfe 172424, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 116, Runtime 2.044052, Loss 1.489608, forward nfe 174114, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 117, Runtime 1.800142, Loss 1.466820, forward nfe 175672, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 118, Runtime 2.498463, Loss 1.442945, forward nfe 177764, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 119, Runtime 1.827766, Loss 1.513162, forward nfe 179340, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 120, Runtime 2.336790, Loss 1.487243, forward nfe 181306, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 121, Runtime 1.934780, Loss 1.486299, forward nfe 182966, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 122, Runtime 2.414024, Loss 1.456361, forward nfe 185004, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 123, Runtime 2.374986, Loss 1.400302, forward nfe 186952, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 124, Runtime 2.314283, Loss 1.416086, forward nfe 188906, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 125, Runtime 1.822584, Loss 1.485031, forward nfe 190512, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 126, Runtime 1.885267, Loss 1.406857, forward nfe 192136, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 127, Runtime 2.221720, Loss 1.629811, forward nfe 194030, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 128, Runtime 2.137212, Loss 1.368705, forward nfe 195660, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 129, Runtime 1.888512, Loss 1.389297, forward nfe 197284, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 130, Runtime 1.708051, Loss 1.483257, forward nfe 198794, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 131, Runtime 2.303921, Loss 1.482395, forward nfe 200682, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 132, Runtime 2.368637, Loss 1.560125, forward nfe 202576, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 133, Runtime 1.709898, Loss 1.661891, forward nfe 204086, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 134, Runtime 1.855132, Loss 1.499036, forward nfe 205686, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 135, Runtime 2.672561, Loss 1.508871, forward nfe 207682, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 136, Runtime 2.563957, Loss 1.422194, forward nfe 209654, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 137, Runtime 2.299147, Loss 1.410949, forward nfe 211596, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 138, Runtime 2.358418, Loss 1.519014, forward nfe 213538, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 139, Runtime 2.988201, Loss 1.386792, forward nfe 215942, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 140, Runtime 2.250667, Loss 1.580899, forward nfe 217854, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 141, Runtime 1.741367, Loss 1.435566, forward nfe 219400, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 142, Runtime 2.336419, Loss 1.432321, forward nfe 221372, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 143, Runtime 1.691032, Loss 1.516442, forward nfe 222870, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 144, Runtime 1.977680, Loss 1.580847, forward nfe 224548, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 145, Runtime 2.616300, Loss 1.385135, forward nfe 226736, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 146, Runtime 2.794829, Loss 1.523039, forward nfe 229008, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 147, Runtime 2.669910, Loss 1.489934, forward nfe 231250, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Epoch: 148, Runtime 2.628001, Loss 1.335224, forward nfe 233450, backward nfe 0, Train: 0.7000, Val: 0.7442, Test: 0.7145, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 87, in train
    loss.backward()
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
best val accuracy 0.744203 with test accuracy 0.714516 at epoch 34 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #7...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.034618, Loss 1.796364, forward nfe 362, backward nfe 0, Train: 0.4583, Val: 0.4348, Test: 0.4242, Best time: 1.4043
Epoch: 002, Runtime 0.967154, Loss 1.797328, forward nfe 1296, backward nfe 0, Train: 0.4583, Val: 0.4348, Test: 0.4242, Best time: 128.0000
Epoch: 003, Runtime 0.988993, Loss 1.791694, forward nfe 2248, backward nfe 0, Train: 0.4667, Val: 0.4746, Test: 0.4516, Best time: 0.9968
Epoch: 004, Runtime 0.984905, Loss 1.772180, forward nfe 3200, backward nfe 0, Train: 0.5167, Val: 0.5101, Test: 0.5032, Best time: 1.0135
Epoch: 005, Runtime 1.028268, Loss 1.765666, forward nfe 4182, backward nfe 0, Train: 0.5250, Val: 0.5428, Test: 0.5532, Best time: 128.0000
Epoch: 006, Runtime 1.035773, Loss 1.734487, forward nfe 5170, backward nfe 0, Train: 0.5833, Val: 0.5710, Test: 0.5532, Best time: 1.0352
Epoch: 007, Runtime 1.048671, Loss 1.703837, forward nfe 6164, backward nfe 0, Train: 0.5833, Val: 0.5710, Test: 0.5532, Best time: 128.0000
Epoch: 008, Runtime 1.040604, Loss 1.690468, forward nfe 7158, backward nfe 0, Train: 0.6250, Val: 0.6507, Test: 0.6581, Best time: 68.6551
Epoch: 009, Runtime 1.072415, Loss 1.691861, forward nfe 8176, backward nfe 0, Train: 0.6583, Val: 0.7159, Test: 0.6935, Best time: 86.0308
Epoch: 010, Runtime 1.120769, Loss 1.638787, forward nfe 9176, backward nfe 0, Train: 0.6833, Val: 0.7413, Test: 0.7210, Best time: 98.1344
Epoch: 011, Runtime 1.078573, Loss 1.640954, forward nfe 10194, backward nfe 0, Train: 0.6917, Val: 0.7464, Test: 0.7032, Best time: 67.9448
Epoch: 012, Runtime 1.098730, Loss 1.654112, forward nfe 11230, backward nfe 0, Train: 0.6917, Val: 0.7464, Test: 0.7032, Best time: 128.0000
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 77, in _runge_kutta_step
    k = _UncheckedAssign.apply(k, f, (..., i + 1))
KeyboardInterrupt
best val accuracy 0.746377 with test accuracy 0.703226 at epoch 11 and best time 128.000000
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #8...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.084726, Loss 1.793784, forward nfe 374, backward nfe 0, Train: 0.2500, Val: 0.2536, Test: 0.2226, Best time: 0.1096
Epoch: 002, Runtime 0.977065, Loss 1.795906, forward nfe 1320, backward nfe 0, Train: 0.3250, Val: 0.3333, Test: 0.3290, Best time: 4.2249
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 42, in forward
    ax = self.sparse_multiply(x)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 31, in sparse_multiply
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 24, in spmm
    out = matrix.index_select(-2, col)
KeyboardInterrupt
best val accuracy 0.333333 with test accuracy 0.329032 at epoch 2 and best time 4.224917
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #9...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.053144, Loss 1.789628, forward nfe 350, backward nfe 0, Train: 0.2333, Val: 0.2761, Test: 0.2613, Best time: 242.9662
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 253, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 68, in train
    out = model(feat, pos_encoding)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 41, in forward
    self.nfe += 1
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1233, in __setattr__
    object.__setattr__(self, name, value)
KeyboardInterrupt
best val accuracy 0.276087 with test accuracy 0.261290 at epoch 1 and best time 242.966191
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #10...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 125, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 404, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 225, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 230, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
Epoch: 001, Runtime 1.093532, Loss 1.796290, forward nfe 356, backward nfe 0, Train: 0.1667, Val: 0.2616, Test: 0.2468, Best time: 1.8966
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 229, in _adaptive_step
    assert torch.isfinite(y0).all(), 'non-finite values in state `y`: {}'.format(y0)
KeyboardInterrupt
best val accuracy 0.261594 with test accuracy 0.246774 at epoch 1 and best time 1.896606
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #11...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.018185, Loss 1.791365, forward nfe 356, backward nfe 0, Train: 0.4250, Val: 0.4717, Test: 0.5065, Best time: 95.0766
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 41, in forward
    self.nfe += 1
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1211, in __setattr__
    modules = self.__dict__.get('_modules')
KeyboardInterrupt
best val accuracy 0.471739 with test accuracy 0.506452 at epoch 1 and best time 95.076594
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #12...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.062157, Loss 1.795131, forward nfe 350, backward nfe 0, Train: 0.2167, Val: 0.2681, Test: 0.2355, Best time: 0.2361
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 42, in forward
    ax = self.sparse_multiply(x)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 31, in sparse_multiply
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 24, in spmm
    out = matrix.index_select(-2, col)
KeyboardInterrupt
best val accuracy 0.268116 with test accuracy 0.235484 at epoch 1 and best time 0.236087
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #13...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
Epoch: 001, Runtime 1.073831, Loss 1.794980, forward nfe 344, backward nfe 0, Train: 0.1667, Val: 0.1732, Test: 0.2113, Best time: 1.8510
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 42, in forward
    ax = self.sparse_multiply(x)
  File "/home/administrator/hieu/graph-neural-pde/src/function_laplacian_diffusion.py", line 31, in sparse_multiply
    ax = torch_sparse.spmm(self.edge_index, mean_attention, x.shape[0], x.shape[0], x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_sparse/spmm.py", line 26, in spmm
    out = scatter_add(out, row, dim=-2, dim_size=m)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_scatter/scatter.py", line 29, in scatter_add
    return scatter_sum(src, index, dim, out, dim_size)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_scatter/scatter.py", line 11, in scatter_sum
    index = broadcast(index, src, dim)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch_scatter/utils.py", line 12, in broadcast
    src = src.expand(other.size())
KeyboardInterrupt
best val accuracy 0.173188 with test accuracy 0.211290 at epoch 1 and best time 1.850997
[INFO] Loggin into 2_tbl_1_lr_grand_citeseer.json for seed #14...
[INFO] Experiment mode is :  ON
[INFO] ODE function :  laplacian
[INFO] Block type :  attention
[INFO] T value :  128.0
[INFO] L1 regularization on :  False
[INFO] L1 reg coefficient :  0.001
GNNEarly
m1.module.weight
torch.Size([80, 3703])
m1.module.bias
torch.Size([80])
m2.module.weight
torch.Size([6, 80])
m2.module.bias
torch.Size([6])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([80, 80])
odeblock.odefunc.d
torch.Size([80])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([80, 80])
odeblock.reg_odefunc.odefunc.d
torch.Size([80])
odeblock.multihead_att_layer.output_var
torch.Size([1])
odeblock.multihead_att_layer.lengthscale
torch.Size([1])
odeblock.multihead_att_layer.Q.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.Q.bias
torch.Size([32])
odeblock.multihead_att_layer.V.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.V.bias
torch.Size([32])
odeblock.multihead_att_layer.K.weight
torch.Size([32, 80])
odeblock.multihead_att_layer.K.bias
torch.Size([32])
odeblock.multihead_att_layer.Wout.weight
torch.Size([80, 4])
odeblock.multihead_att_layer.Wout.bias
torch.Size([80])
/home/administrator/hieu/graph-neural-pde/src/run_GNN.py:288: RuntimeWarning: Mean of empty slice.
  mean_fw_nfe = np.array(fw_nfe_ls).mean()
/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/administrator/hieu/graph-neural-pde/src/run_GNN.py:289: RuntimeWarning: Mean of empty slice.
  mean_run_time = np.array(run_time_ls).mean()
Traceback (most recent call last):
  File "test_multiple_planetoid_splits.py", line 387, in <module>
    fw_nfes, losses, train_accs, val_accs, test_accs = main(opt)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 290, in main
    min_run_time = min(run_time_ls)
ValueError: min() arg is an empty sequence
Traceback (most recent call last):
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 254, in main
    tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, pos_encoding, opt)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/run_GNN.py", line 138, in test
    logits, accs = model(feat, pos_encoding), []
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/GNN_early.py", line 84, in forward
    z = self.odeblock(x)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/administrator/hieu/graph-neural-pde/src/block_transformer_attention.py", line 59, in forward
    state_dt = integrator(
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 301, in __call__
    t, solution = self.solver.integrate(t)
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 65, in integrate
    new_t, y = self.advance(t[i])
  File "/home/administrator/hieu/graph-neural-pde/src/early_stop_solver.py", line 77, in advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 75, in _runge_kutta_step
    yi = y0 + k[..., :i + 1].matmul(beta_i * dt).view_as(f0)
KeyboardInterrupt
best val accuracy 0.000000 with test accuracy 0.000000 at epoch 0 and best time 0.000000
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 114, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 394, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 225, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/home/administrator/anaconda3/envs/deepgrand/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 230, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown