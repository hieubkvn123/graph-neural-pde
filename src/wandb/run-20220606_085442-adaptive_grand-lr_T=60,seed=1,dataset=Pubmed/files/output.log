/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: AdaptiveHeunSolver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
****************** Adaptive GRAND laplacian function ******************
****************** Adaptive GRAND laplacian function ******************
GNNEarly
m1.module.weight
torch.Size([128, 500])
m1.module.bias
torch.Size([128])
m2.module.weight
torch.Size([3, 128])
m2.module.bias
torch.Size([3])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([128, 128])
odeblock.odefunc.d
torch.Size([128])
odeblock.odefunc.k_d
torch.Size([128])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([128, 128])
odeblock.reg_odefunc.odefunc.d
torch.Size([128])
odeblock.reg_odefunc.odefunc.k_d
torch.Size([128])
odeblock.multihead_att_layer.Q.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.Q.bias
torch.Size([16])
odeblock.multihead_att_layer.V.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.V.bias
torch.Size([16])
odeblock.multihead_att_layer.K.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.K.bias
torch.Size([16])
odeblock.multihead_att_layer.Wout.weight
torch.Size([128, 16])
odeblock.multihead_att_layer.Wout.bias
torch.Size([128])
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
Epoch: 001/100, Runtime 1325.833593, Loss 1.100500, forward nfe 128, backward nfe 153596, Train: 0.4667, Val: 0.3194, Test: 0.3366, Best time: 2.6367
Epoch: 002/100, Runtime 6103.291082, Loss 1.099566, forward nfe 846, backward nfe 893317, Train: 0.7000, Val: 0.6569, Test: 0.6571, Best time: 2.7537
Epoch: 003/100, Runtime 2363.453296, Loss 1.096516, forward nfe 1564, backward nfe 1173467, Train: 0.7000, Val: 0.6569, Test: 0.6571, Best time: 60.0000
Epoch: 004/100, Runtime 3558.568919, Loss 1.090349, forward nfe 2282, backward nfe 1604438, Train: 0.7000, Val: 0.6569, Test: 0.6571, Best time: 60.0000
Epoch: 005/100, Runtime 2248.776018, Loss 1.077451, forward nfe 3000, backward nfe 1873080, Train: 0.7333, Val: 0.6611, Test: 0.6616, Best time: 2.6292
Epoch: 006/100, Runtime 2552.909563, Loss 1.060243, forward nfe 3724, backward nfe 2178391, Train: 0.7833, Val: 0.7069, Test: 0.7069, Best time: 5.8051
Epoch: 007/100, Runtime 2222.662938, Loss 1.037072, forward nfe 4448, backward nfe 2442429, Train: 0.7833, Val: 0.7181, Test: 0.7178, Best time: 9.8988
Epoch: 008/100, Runtime 2690.798524, Loss 1.009392, forward nfe 5172, backward nfe 2764245, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 23.7534
Epoch: 009/100, Runtime 2737.995426, Loss 0.972510, forward nfe 5890, backward nfe 3085651, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 010/100, Runtime 3282.062877, Loss 0.923718, forward nfe 6614, backward nfe 3476984, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 011/100, Runtime 3303.778546, Loss 0.881297, forward nfe 7338, backward nfe 3871032, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 012/100, Runtime 3026.050402, Loss 0.850946, forward nfe 8062, backward nfe 4229613, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 013/100, Runtime 2747.492256, Loss 0.776431, forward nfe 8792, backward nfe 4557893, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 014/100, Runtime 3116.156552, Loss 0.753622, forward nfe 9522, backward nfe 4932176, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 015/100, Runtime 2445.902985, Loss 0.714875, forward nfe 10252, backward nfe 5224411, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 016/100, Runtime 2746.106608, Loss 0.666535, forward nfe 10982, backward nfe 5551271, Train: 0.7667, Val: 0.7194, Test: 0.7185, Best time: 60.0000
Epoch: 017/100, Runtime 2580.141578, Loss 0.633267, forward nfe 11712, backward nfe 5856542, Train: 0.7500, Val: 0.7278, Test: 0.7170, Best time: 5.3035
Epoch: 018/100, Runtime 2977.333154, Loss 0.557916, forward nfe 12442, backward nfe 6210077, Train: 0.7833, Val: 0.7354, Test: 0.7290, Best time: 9.2208
Epoch: 019/100, Runtime 2927.247829, Loss 0.542455, forward nfe 13172, backward nfe 6558227, Train: 0.8500, Val: 0.7431, Test: 0.7426, Best time: 9.2403
Epoch: 020/100, Runtime 3429.278355, Loss 0.515323, forward nfe 13902, backward nfe 6964271, Train: 0.8500, Val: 0.7556, Test: 0.7536, Best time: 14.5572
Traceback (most recent call last):
  File "run_GNN.py", line 259, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "run_GNN.py", line 89, in train
    loss.backward()
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 95, in augmented_dynamics
    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 275, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.20 GiB total capacity; 1.46 GiB already allocated; 55.06 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.755556 with test accuracy 0.753582 at epoch 20 and best time 14.557162