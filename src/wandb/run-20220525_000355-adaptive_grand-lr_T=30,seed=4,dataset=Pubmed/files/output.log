/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: AdaptiveHeunSolver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
****************** Adaptive GRAND laplacian function ******************
****************** Adaptive GRAND laplacian function ******************
GNNEarly
m1.module.weight
torch.Size([128, 500])
m1.module.bias
torch.Size([128])
m2.module.weight
torch.Size([3, 128])
m2.module.bias
torch.Size([3])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([128, 128])
odeblock.odefunc.d
torch.Size([128])
odeblock.odefunc.k_d
torch.Size([128])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([128, 128])
odeblock.reg_odefunc.odefunc.d
torch.Size([128])
odeblock.reg_odefunc.odefunc.k_d
torch.Size([128])
odeblock.multihead_att_layer.Q.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.Q.bias
torch.Size([16])
odeblock.multihead_att_layer.V.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.V.bias
torch.Size([16])
odeblock.multihead_att_layer.K.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.K.bias
torch.Size([16])
odeblock.multihead_att_layer.Wout.weight
torch.Size([128, 16])
odeblock.multihead_att_layer.Wout.bias
torch.Size([128])
Epoch: 001/100, Runtime 15.308188, Loss 1.099149, forward nfe 68, backward nfe 1458, Train: 0.5500, Val: 0.5875, Test: 0.5582, Best time: 32.4961
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
Epoch: 002/100, Runtime 17.440006, Loss 1.090245, forward nfe 432, backward nfe 3148, Train: 0.7833, Val: 0.6910, Test: 0.6613, Best time: 6.2272
Epoch: 003/100, Runtime 15.682654, Loss 1.074256, forward nfe 784, backward nfe 4660, Train: 0.7667, Val: 0.6924, Test: 0.6649, Best time: 6.1266
Epoch: 004/100, Runtime 15.651006, Loss 1.049996, forward nfe 1136, backward nfe 6181, Train: 0.7667, Val: 0.7042, Test: 0.6830, Best time: 5.9380
Epoch: 005/100, Runtime 16.248521, Loss 1.020148, forward nfe 1500, backward nfe 7756, Train: 0.7500, Val: 0.7188, Test: 0.7025, Best time: 2.4954
Epoch: 006/100, Runtime 16.608668, Loss 0.981713, forward nfe 1864, backward nfe 9343, Train: 0.8000, Val: 0.7312, Test: 0.7083, Best time: 5.6717
Epoch: 007/100, Runtime 17.758219, Loss 0.933608, forward nfe 2222, backward nfe 11079, Train: 0.8000, Val: 0.7375, Test: 0.7099, Best time: 30.0000
Epoch: 008/100, Runtime 16.607568, Loss 0.873489, forward nfe 2586, backward nfe 12696, Train: 0.8000, Val: 0.7375, Test: 0.7099, Best time: 30.0000
Epoch: 009/100, Runtime 16.364290, Loss 0.813014, forward nfe 2944, backward nfe 14266, Train: 0.8000, Val: 0.7375, Test: 0.7099, Best time: 30.0000
Epoch: 010/100, Runtime 15.333734, Loss 0.751252, forward nfe 3302, backward nfe 15751, Train: 0.8000, Val: 0.7424, Test: 0.7166, Best time: 5.4909
Epoch: 011/100, Runtime 14.632144, Loss 0.695050, forward nfe 3660, backward nfe 17156, Train: 0.8167, Val: 0.7493, Test: 0.7286, Best time: 5.4921
Epoch: 012/100, Runtime 13.661175, Loss 0.623127, forward nfe 4018, backward nfe 18475, Train: 0.8333, Val: 0.7535, Test: 0.7390, Best time: 5.5080
Epoch: 013/100, Runtime 13.053005, Loss 0.573981, forward nfe 4376, backward nfe 19707, Train: 0.8333, Val: 0.7639, Test: 0.7531, Best time: 2.1039
Epoch: 014/100, Runtime 12.332950, Loss 0.518569, forward nfe 4734, backward nfe 20887, Train: 0.8333, Val: 0.7743, Test: 0.7621, Best time: 2.0736
Epoch: 015/100, Runtime 11.337732, Loss 0.486527, forward nfe 5092, backward nfe 21953, Train: 0.8333, Val: 0.7743, Test: 0.7621, Best time: 30.0000
Epoch: 016/100, Runtime 10.816106, Loss 0.488034, forward nfe 5444, backward nfe 22963, Train: 0.8500, Val: 0.7812, Test: 0.7745, Best time: 2.0265
Epoch: 017/100, Runtime 10.243384, Loss 0.459798, forward nfe 5796, backward nfe 23937, Train: 0.8833, Val: 0.7854, Test: 0.7787, Best time: 2.0081
Epoch: 018/100, Runtime 9.389127, Loss 0.408145, forward nfe 6142, backward nfe 24815, Train: 0.8000, Val: 0.7951, Test: 0.7698, Best time: 108.6174
Epoch: 019/100, Runtime 8.209390, Loss 0.402440, forward nfe 6494, backward nfe 25557, Train: 0.8000, Val: 0.8069, Test: 0.7838, Best time: 130.1130
Epoch: 020/100, Runtime 9.043651, Loss 0.434963, forward nfe 6840, backward nfe 26392, Train: 0.8000, Val: 0.8069, Test: 0.7838, Best time: 30.0000
Epoch: 021/100, Runtime 8.204941, Loss 0.382476, forward nfe 7180, backward nfe 27145, Train: 0.8000, Val: 0.8069, Test: 0.7838, Best time: 30.0000
Epoch: 022/100, Runtime 8.903502, Loss 0.395439, forward nfe 7526, backward nfe 27958, Train: 0.8000, Val: 0.8069, Test: 0.7838, Best time: 30.0000
Epoch: 023/100, Runtime 9.200250, Loss 0.370718, forward nfe 7872, backward nfe 28805, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 46.7807
Epoch: 024/100, Runtime 9.325522, Loss 0.366413, forward nfe 8200, backward nfe 29664, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 025/100, Runtime 9.098862, Loss 0.371670, forward nfe 8528, backward nfe 30502, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 026/100, Runtime 8.730571, Loss 0.336931, forward nfe 8856, backward nfe 31303, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 027/100, Runtime 8.401916, Loss 0.316154, forward nfe 9184, backward nfe 32065, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 028/100, Runtime 7.973072, Loss 0.312192, forward nfe 9512, backward nfe 32797, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 029/100, Runtime 7.655680, Loss 0.265031, forward nfe 9828, backward nfe 33516, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 030/100, Runtime 7.545549, Loss 0.323368, forward nfe 10144, backward nfe 34210, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 031/100, Runtime 6.649361, Loss 0.298672, forward nfe 10466, backward nfe 34811, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 032/100, Runtime 6.380054, Loss 0.257009, forward nfe 10788, backward nfe 35385, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 033/100, Runtime 6.342134, Loss 0.310940, forward nfe 11110, backward nfe 35954, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 034/100, Runtime 5.616671, Loss 0.273148, forward nfe 11432, backward nfe 36445, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 035/100, Runtime 5.818058, Loss 0.227501, forward nfe 11754, backward nfe 36952, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 036/100, Runtime 6.166147, Loss 0.247575, forward nfe 12076, backward nfe 37488, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 037/100, Runtime 6.226850, Loss 0.222280, forward nfe 12398, backward nfe 38030, Train: 0.8500, Val: 0.8076, Test: 0.7933, Best time: 30.0000
Epoch: 038/100, Runtime 5.748197, Loss 0.276511, forward nfe 12720, backward nfe 38527, Train: 0.8667, Val: 0.8097, Test: 0.7862, Best time: 30.0000
Epoch: 039/100, Runtime 5.659686, Loss 0.233621, forward nfe 13036, backward nfe 39017, Train: 0.8667, Val: 0.8097, Test: 0.7862, Best time: 30.0000
Epoch: 040/100, Runtime 6.201360, Loss 0.235029, forward nfe 13346, backward nfe 39560, Train: 0.8667, Val: 0.8097, Test: 0.7862, Best time: 30.0000
Epoch: 041/100, Runtime 5.007965, Loss 0.185331, forward nfe 13656, backward nfe 39983, Train: 0.8667, Val: 0.8097, Test: 0.7862, Best time: 30.0000
Epoch: 042/100, Runtime 4.731299, Loss 0.168371, forward nfe 13966, backward nfe 40379, Train: 0.8667, Val: 0.8097, Test: 0.7862, Best time: 30.0000
Epoch: 043/100, Runtime 5.643929, Loss 0.235888, forward nfe 14282, backward nfe 40864, Train: 0.9000, Val: 0.8111, Test: 0.7920, Best time: 140.7595
Epoch: 044/100, Runtime 5.307939, Loss 0.173212, forward nfe 14586, backward nfe 41319, Train: 0.9000, Val: 0.8111, Test: 0.7920, Best time: 30.0000
Epoch: 045/100, Runtime 5.148737, Loss 0.187593, forward nfe 14890, backward nfe 41757, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 046/100, Runtime 5.124763, Loss 0.199750, forward nfe 15194, backward nfe 42193, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 047/100, Runtime 5.440513, Loss 0.209245, forward nfe 15498, backward nfe 42661, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 048/100, Runtime 4.212051, Loss 0.239281, forward nfe 15802, backward nfe 43008, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 049/100, Runtime 4.753369, Loss 0.234701, forward nfe 16106, backward nfe 43409, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 050/100, Runtime 5.484100, Loss 0.147516, forward nfe 16404, backward nfe 43887, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 051/100, Runtime 5.788491, Loss 0.226825, forward nfe 16696, backward nfe 44394, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 052/100, Runtime 5.049993, Loss 0.181708, forward nfe 16994, backward nfe 44827, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 053/100, Runtime 5.589953, Loss 0.140280, forward nfe 17286, backward nfe 45313, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 054/100, Runtime 5.130577, Loss 0.181749, forward nfe 17578, backward nfe 45756, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 055/100, Runtime 5.398989, Loss 0.143700, forward nfe 17870, backward nfe 46225, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 056/100, Runtime 5.359613, Loss 0.104579, forward nfe 18162, backward nfe 46687, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 057/100, Runtime 5.359231, Loss 0.141946, forward nfe 18454, backward nfe 47151, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 058/100, Runtime 5.318778, Loss 0.155759, forward nfe 18734, backward nfe 47614, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 059/100, Runtime 2.996351, Loss 0.214059, forward nfe 19014, backward nfe 47845, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 060/100, Runtime 3.774431, Loss 0.149265, forward nfe 19294, backward nfe 48153, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 061/100, Runtime 3.842673, Loss 0.177078, forward nfe 19574, backward nfe 48469, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 062/100, Runtime 3.351565, Loss 0.114106, forward nfe 19854, backward nfe 48736, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 063/100, Runtime 4.225895, Loss 0.153431, forward nfe 20134, backward nfe 49090, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 064/100, Runtime 4.322505, Loss 0.108706, forward nfe 20414, backward nfe 49451, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 065/100, Runtime 2.616134, Loss 0.126814, forward nfe 20694, backward nfe 49644, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 066/100, Runtime 4.968778, Loss 0.163691, forward nfe 20974, backward nfe 50071, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 067/100, Runtime 4.705886, Loss 0.103928, forward nfe 21254, backward nfe 50471, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 068/100, Runtime 4.823308, Loss 0.089380, forward nfe 21534, backward nfe 50881, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 069/100, Runtime 3.541944, Loss 0.151017, forward nfe 21814, backward nfe 51166, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 070/100, Runtime 3.062504, Loss 0.101831, forward nfe 22094, backward nfe 51403, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 071/100, Runtime 3.840909, Loss 0.092747, forward nfe 22374, backward nfe 51718, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 072/100, Runtime 3.852085, Loss 0.114208, forward nfe 22654, backward nfe 52035, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 073/100, Runtime 4.023272, Loss 0.151246, forward nfe 22934, backward nfe 52371, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 074/100, Runtime 3.909163, Loss 0.092174, forward nfe 23208, backward nfe 52695, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 075/100, Runtime 3.767240, Loss 0.051952, forward nfe 23482, backward nfe 53006, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 076/100, Runtime 3.820769, Loss 0.061879, forward nfe 23756, backward nfe 53322, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 077/100, Runtime 3.167620, Loss 0.089748, forward nfe 24030, backward nfe 53571, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 078/100, Runtime 3.412463, Loss 0.094554, forward nfe 24304, backward nfe 53846, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 079/100, Runtime 2.886076, Loss 0.066249, forward nfe 24578, backward nfe 54069, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 080/100, Runtime 3.446971, Loss 0.123298, forward nfe 24852, backward nfe 54346, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 081/100, Runtime 4.411608, Loss 0.118147, forward nfe 25126, backward nfe 54721, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Epoch: 082/100, Runtime 4.680928, Loss 0.119859, forward nfe 25400, backward nfe 55121, Train: 0.8833, Val: 0.8118, Test: 0.7845, Best time: 30.0000
Traceback (most recent call last):
  File "run_GNN.py", line 259, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "run_GNN.py", line 89, in train
    loss.backward()
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 95, in augmented_dynamics
    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 275, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.20 GiB total capacity; 2.79 GiB already allocated; 20.06 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.811806 with test accuracy 0.784487 at epoch 45 and best time 30.000000