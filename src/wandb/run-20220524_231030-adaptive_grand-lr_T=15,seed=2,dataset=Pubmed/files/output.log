****************** Adaptive GRAND laplacian function ******************
****************** Adaptive GRAND laplacian function ******************
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Dopri5Solver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: AdaptiveHeunSolver: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:11: UserWarning: EarlyStopDopri5: Unexpected arguments {'step_size': 1}
  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))
GNNEarly
m1.module.weight
torch.Size([128, 500])
m1.module.bias
torch.Size([128])
m2.module.weight
torch.Size([3, 128])
m2.module.bias
torch.Size([3])
odeblock.odefunc.alpha_train
torch.Size([])
odeblock.odefunc.beta_train
torch.Size([])
odeblock.odefunc.alpha_sc
torch.Size([1])
odeblock.odefunc.beta_sc
torch.Size([1])
odeblock.odefunc.w
torch.Size([128, 128])
odeblock.odefunc.d
torch.Size([128])
odeblock.odefunc.k_d
torch.Size([128])
odeblock.reg_odefunc.odefunc.alpha_train
torch.Size([])
odeblock.reg_odefunc.odefunc.beta_train
torch.Size([])
odeblock.reg_odefunc.odefunc.alpha_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.beta_sc
torch.Size([1])
odeblock.reg_odefunc.odefunc.w
torch.Size([128, 128])
odeblock.reg_odefunc.odefunc.d
torch.Size([128])
odeblock.reg_odefunc.odefunc.k_d
torch.Size([128])
odeblock.multihead_att_layer.Q.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.Q.bias
torch.Size([16])
odeblock.multihead_att_layer.V.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.V.bias
torch.Size([16])
odeblock.multihead_att_layer.K.weight
torch.Size([16, 128])
odeblock.multihead_att_layer.K.bias
torch.Size([16])
odeblock.multihead_att_layer.Wout.weight
torch.Size([128, 16])
odeblock.multihead_att_layer.Wout.bias
torch.Size([128])
Epoch: 001/100, Runtime 1.262467, Loss 1.101895, forward nfe 44, backward nfe 83, Train: 0.6500, Val: 0.6375, Test: 0.6168, Best time: 0.2347
Epoch: 002/100, Runtime 0.797965, Loss 1.089769, forward nfe 234, backward nfe 118, Train: 0.8000, Val: 0.7174, Test: 0.6985, Best time: 10.2772
Epoch: 003/100, Runtime 1.107538, Loss 1.074669, forward nfe 424, backward nfe 187, Train: 0.8000, Val: 0.7174, Test: 0.6985, Best time: 15.0000
Epoch: 004/100, Runtime 1.040571, Loss 1.053063, forward nfe 608, backward nfe 248, Train: 0.8000, Val: 0.7174, Test: 0.6985, Best time: 15.0000
Epoch: 005/100, Runtime 0.896015, Loss 1.026358, forward nfe 798, backward nfe 293, Train: 0.8667, Val: 0.7201, Test: 0.6939, Best time: 9.7730
Epoch: 006/100, Runtime 1.238590, Loss 0.986302, forward nfe 982, backward nfe 373, Train: 0.8667, Val: 0.7250, Test: 0.6992, Best time: 9.6442
Epoch: 007/100, Runtime 1.302565, Loss 0.944096, forward nfe 1166, backward nfe 459, Train: 0.9000, Val: 0.7285, Test: 0.7051, Best time: 5.5556
Epoch: 008/100, Runtime 1.100974, Loss 0.889302, forward nfe 1350, backward nfe 526, Train: 0.8833, Val: 0.7326, Test: 0.7078, Best time: 9.5063
Epoch: 009/100, Runtime 1.134166, Loss 0.837299, forward nfe 1528, backward nfe 597, Train: 0.8333, Val: 0.7347, Test: 0.7157, Best time: 15.0000
Epoch: 010/100, Runtime 1.065172, Loss 0.771608, forward nfe 1706, backward nfe 661, Train: 0.8667, Val: 0.7361, Test: 0.7096, Best time: 30.5842
Epoch: 011/100, Runtime 0.929209, Loss 0.691065, forward nfe 1878, backward nfe 712, Train: 0.9000, Val: 0.7396, Test: 0.7166, Best time: 5.5062
Epoch: 012/100, Runtime 0.898117, Loss 0.619179, forward nfe 2044, backward nfe 761, Train: 0.9000, Val: 0.7396, Test: 0.7166, Best time: 15.0000
Epoch: 013/100, Runtime 0.781599, Loss 0.548912, forward nfe 2204, backward nfe 798, Train: 0.9167, Val: 0.7451, Test: 0.7259, Best time: 5.5493
Epoch: 014/100, Runtime 0.734884, Loss 0.489960, forward nfe 2364, backward nfe 829, Train: 0.9333, Val: 0.7500, Test: 0.7334, Best time: 5.5894
Epoch: 015/100, Runtime 0.670784, Loss 0.432114, forward nfe 2524, backward nfe 854, Train: 0.9500, Val: 0.7618, Test: 0.7439, Best time: 5.6443
Epoch: 016/100, Runtime 0.689869, Loss 0.374454, forward nfe 2678, backward nfe 882, Train: 0.9500, Val: 0.7625, Test: 0.7423, Best time: 9.9359
Epoch: 017/100, Runtime 0.696561, Loss 0.333599, forward nfe 2832, backward nfe 911, Train: 0.9500, Val: 0.7625, Test: 0.7423, Best time: 15.0000
Epoch: 018/100, Runtime 0.680863, Loss 0.285913, forward nfe 2980, backward nfe 941, Train: 0.9500, Val: 0.7625, Test: 0.7423, Best time: 15.0000
Epoch: 019/100, Runtime 0.675066, Loss 0.250170, forward nfe 3128, backward nfe 971, Train: 0.9500, Val: 0.7708, Test: 0.7451, Best time: 25.1057
Epoch: 020/100, Runtime 0.654102, Loss 0.227639, forward nfe 3270, backward nfe 999, Train: 0.9500, Val: 0.7819, Test: 0.7636, Best time: 25.7630
Epoch: 021/100, Runtime 0.581715, Loss 0.225208, forward nfe 3412, backward nfe 1019, Train: 0.9500, Val: 0.7924, Test: 0.7732, Best time: 31.7633
Epoch: 022/100, Runtime 0.544500, Loss 0.208658, forward nfe 3554, backward nfe 1036, Train: 0.9500, Val: 0.7986, Test: 0.7768, Best time: 32.5246
Epoch: 023/100, Runtime 0.557889, Loss 0.193531, forward nfe 3690, backward nfe 1054, Train: 0.9500, Val: 0.7986, Test: 0.7768, Best time: 15.0000
Epoch: 024/100, Runtime 0.554595, Loss 0.192147, forward nfe 3826, backward nfe 1071, Train: 0.9500, Val: 0.7986, Test: 0.7768, Best time: 15.0000
Epoch: 025/100, Runtime 0.557467, Loss 0.195088, forward nfe 3968, backward nfe 1088, Train: 0.9500, Val: 0.7986, Test: 0.7768, Best time: 15.0000
Epoch: 026/100, Runtime 0.518825, Loss 0.191509, forward nfe 4110, backward nfe 1104, Train: 0.9500, Val: 0.7986, Test: 0.7768, Best time: 15.0000
Epoch: 027/100, Runtime 0.520640, Loss 0.146948, forward nfe 4240, backward nfe 1121, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 13.2581
Epoch: 028/100, Runtime 0.525985, Loss 0.183614, forward nfe 4370, backward nfe 1138, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 029/100, Runtime 0.509041, Loss 0.161466, forward nfe 4500, backward nfe 1153, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 030/100, Runtime 0.496341, Loss 0.150505, forward nfe 4630, backward nfe 1168, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 031/100, Runtime 0.493545, Loss 0.150122, forward nfe 4754, backward nfe 1184, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 032/100, Runtime 0.481204, Loss 0.160735, forward nfe 4878, backward nfe 1198, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 033/100, Runtime 0.485476, Loss 0.174192, forward nfe 5002, backward nfe 1213, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 034/100, Runtime 0.422235, Loss 0.137894, forward nfe 5120, backward nfe 1226, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 035/100, Runtime 0.466968, Loss 0.128185, forward nfe 5226, backward nfe 1244, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 036/100, Runtime 0.440136, Loss 0.171340, forward nfe 5332, backward nfe 1258, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 037/100, Runtime 0.454250, Loss 0.175855, forward nfe 5438, backward nfe 1275, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 038/100, Runtime 0.417744, Loss 0.136043, forward nfe 5544, backward nfe 1287, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 039/100, Runtime 0.442096, Loss 0.137885, forward nfe 5650, backward nfe 1302, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 040/100, Runtime 0.446866, Loss 0.115556, forward nfe 5756, backward nfe 1318, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 041/100, Runtime 0.413163, Loss 0.125810, forward nfe 5862, backward nfe 1332, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 042/100, Runtime 0.419926, Loss 0.104059, forward nfe 5962, backward nfe 1348, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 043/100, Runtime 0.435605, Loss 0.117590, forward nfe 6062, backward nfe 1365, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 044/100, Runtime 0.454777, Loss 0.158004, forward nfe 6162, backward nfe 1383, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 045/100, Runtime 0.404366, Loss 0.094394, forward nfe 6262, backward nfe 1396, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 046/100, Runtime 0.399298, Loss 0.090862, forward nfe 6362, backward nfe 1408, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 047/100, Runtime 0.417404, Loss 0.105910, forward nfe 6462, backward nfe 1422, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 048/100, Runtime 0.386889, Loss 0.098998, forward nfe 6562, backward nfe 1434, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 049/100, Runtime 0.388674, Loss 0.111386, forward nfe 6662, backward nfe 1446, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 050/100, Runtime 0.386659, Loss 0.079662, forward nfe 6762, backward nfe 1457, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 051/100, Runtime 0.376141, Loss 0.129044, forward nfe 6862, backward nfe 1467, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 052/100, Runtime 0.378917, Loss 0.073661, forward nfe 6962, backward nfe 1479, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 053/100, Runtime 0.373796, Loss 0.073728, forward nfe 7056, backward nfe 1490, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 054/100, Runtime 0.393843, Loss 0.075183, forward nfe 7150, backward nfe 1503, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 055/100, Runtime 0.384245, Loss 0.063393, forward nfe 7244, backward nfe 1516, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 056/100, Runtime 0.407493, Loss 0.081432, forward nfe 7338, backward nfe 1531, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 057/100, Runtime 0.347453, Loss 0.061282, forward nfe 7432, backward nfe 1540, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 058/100, Runtime 0.363920, Loss 0.054675, forward nfe 7526, backward nfe 1550, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 059/100, Runtime 0.366175, Loss 0.136555, forward nfe 7620, backward nfe 1561, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 060/100, Runtime 0.366689, Loss 0.103106, forward nfe 7714, backward nfe 1573, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 061/100, Runtime 0.355874, Loss 0.071515, forward nfe 7802, backward nfe 1585, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 062/100, Runtime 0.366425, Loss 0.082205, forward nfe 7890, backward nfe 1598, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 063/100, Runtime 0.379282, Loss 0.121849, forward nfe 7978, backward nfe 1611, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 064/100, Runtime 0.358988, Loss 0.077883, forward nfe 8066, backward nfe 1623, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 065/100, Runtime 0.355643, Loss 0.058754, forward nfe 8154, backward nfe 1635, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 066/100, Runtime 0.342799, Loss 0.096914, forward nfe 8242, backward nfe 1645, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 067/100, Runtime 0.361625, Loss 0.075593, forward nfe 8330, backward nfe 1657, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 068/100, Runtime 0.340142, Loss 0.082841, forward nfe 8418, backward nfe 1667, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 069/100, Runtime 0.350015, Loss 0.056890, forward nfe 8506, backward nfe 1678, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 070/100, Runtime 0.365188, Loss 0.099027, forward nfe 8594, backward nfe 1690, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 071/100, Runtime 0.341383, Loss 0.069856, forward nfe 8682, backward nfe 1700, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 072/100, Runtime 0.363754, Loss 0.091683, forward nfe 8770, backward nfe 1712, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 073/100, Runtime 0.372291, Loss 0.085241, forward nfe 8858, backward nfe 1724, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 074/100, Runtime 0.352452, Loss 0.037667, forward nfe 8946, backward nfe 1736, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 075/100, Runtime 0.336583, Loss 0.073784, forward nfe 9034, backward nfe 1746, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 076/100, Runtime 0.359997, Loss 0.070698, forward nfe 9122, backward nfe 1758, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 077/100, Runtime 0.351331, Loss 0.052772, forward nfe 9210, backward nfe 1769, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 078/100, Runtime 0.330395, Loss 0.088627, forward nfe 9298, backward nfe 1778, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 079/100, Runtime 0.344609, Loss 0.072497, forward nfe 9386, backward nfe 1788, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 080/100, Runtime 0.347677, Loss 0.048742, forward nfe 9474, backward nfe 1799, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 081/100, Runtime 0.348731, Loss 0.087088, forward nfe 9562, backward nfe 1810, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 082/100, Runtime 0.342824, Loss 0.044960, forward nfe 9650, backward nfe 1820, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 083/100, Runtime 0.341634, Loss 0.061712, forward nfe 9738, backward nfe 1830, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 084/100, Runtime 0.356498, Loss 0.073809, forward nfe 9826, backward nfe 1841, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Epoch: 085/100, Runtime 0.351699, Loss 0.055191, forward nfe 9914, backward nfe 1853, Train: 0.9833, Val: 0.8118, Test: 0.7929, Best time: 15.0000
Traceback (most recent call last):
  File "run_GNN.py", line 259, in main
    loss = train(model, optimizer, data, pos_encoding)
  File "run_GNN.py", line 89, in train
    loss.backward()
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 126, in backward
    aug_state = odeint(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py", line 30, in integrate
    solution[i] = self._advance(t[i])
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 194, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 255, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py", line 76, in _runge_kutta_step
    f = func(ti, yi, perturb=perturb)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 159, in forward
    return self.mul * self.base_func(-t, y)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py", line 95, in augmented_dynamics
    vjp_t, vjp_y, *vjp_params = torch.autograd.grad(
  File "/home/hieunm60/anaconda3/envs/deepgrand/lib/python3.8/site-packages/torch/autograd/__init__.py", line 275, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.20 GiB total capacity; 2.90 GiB already allocated; 62.06 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
best val accuracy 0.811806 with test accuracy 0.792886 at epoch 27 and best time 15.000000